import requests
import pdfplumber
from celery import shared_task
from django.conf import settings
from .models import Session, Bill, Speaker, Statement, VotingRecord, Party, Category, Subcategory, BillCategoryMapping, BillSubcategoryMapping
from celery.exceptions import MaxRetriesExceededError
from requests.exceptions import RequestException
import logging
from celery.schedules import crontab  # Keep if you plan to use Celery Beat schedules
from datetime import datetime, timedelta, time as dt_time
import json
import time
import queue
from pathlib import Path
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import deque
import re
from google import genai

logger = logging.getLogger(__name__)

# --- Unified GeminiHandler with integrated rate limiting and robust error handling ---
import threading
import time
from collections import deque
from datetime import datetime, timedelta
import logging
from google import genai
from google.api_core import exceptions as google_exceptions

GEMINI = GeminiHandler(api_key=getattr(settings, 'GEMINI_API_KEY', None))

logger = logging.getLogger(__name__)


class GeminiHandler:
    """
    Unified handler for all Gemini API interactions, integrating a robust rate limiter
    to manage API calls safely and efficiently.
    """

    def __init__(
        self,
        api_key,
        model_name='gemini-1.5-flash-latest',  # Using latest flash model
        max_tokens_per_minute=15000,  # Realistic Free Tier TPM
        max_requests_per_minute=15,  # Realistic Free Tier RPM
        max_tokens_per_day=100000):  # Realistic Free Tier Tokens-per-day

        # --- Gemini Client Configuration ---
        self.logger = logger
        self.model_name = model_name
        self.api_key = api_key
        if not self.api_key:
            self.logger.error(
                "FATAL: GEMINI_API_KEY was not provided to GeminiHandler.")
        else:
            # Configure the client once during initialization
            genai.configure(api_key=self.api_key)
        self.model = genai.GenerativeModel(self.model_name)

        # --- Integrated Rate Limiter Attributes ---
        self.max_tokens_per_minute = max_tokens_per_minute
        self.max_requests_per_minute = max_requests_per_minute
        self.token_usage = deque()
        self.request_times = deque()
        self.lock = threading.Lock()
        self.consecutive_errors = 0
        self.backoff_until = None

    def _wait_if_needed(self):
        """Checks rate limits and waits if necessary. Returns False if waiting fails."""
        with self.lock:
            # 1. Check for exponential backoff due to errors
            if self.backoff_until and datetime.now() < self.backoff_until:
                wait_time = (self.backoff_until -
                             datetime.now()).total_seconds()
                self.logger.warning(
                    f"In exponential backoff period. Waiting for {wait_time:.1f}s."
                )
                time.sleep(wait_time)

            # 2. Main rate limit loop
            while True:
                now = datetime.now()
                one_minute_ago = now - timedelta(minutes=1)

                # Prune old records
                while self.request_times and self.request_times[
                        0] < one_minute_ago:
                    self.request_times.popleft()
                while self.token_usage and self.token_usage[0][
                        0] < one_minute_ago:
                    self.token_usage.popleft()

                # Check limits
                if len(self.request_times) >= self.max_requests_per_minute:
                    wait_duration = (self.request_times[0] +
                                     timedelta(minutes=1) -
                                     now).total_seconds()
                    self.logger.info(
                        f"Request limit reached. Waiting for {wait_duration:.1f}s"
                    )
                    time.sleep(max(0, wait_duration) +
                               0.1)  # Add a small buffer
                    continue  # Re-check after waiting

                # No need to check token usage for now, RPM is stricter

                return True  # Can proceed

    def _record_attempt(self, success=True):
        """Records a request attempt for rate limiting and error tracking."""
        with self.lock:
            if success:
                self.request_times.append(datetime.now())
                self.consecutive_errors = 0
                self.backoff_until = None
            else:
                self.consecutive_errors += 1
                backoff_seconds = min(60, 2**self.consecutive_errors)
                self.backoff_until = datetime.now() + timedelta(
                    seconds=backoff_seconds)
                self.logger.warning(
                    f"API error recorded. Consecutive errors: {self.consecutive_errors}. Backing off for {backoff_seconds}s."
                )

    def call_api(self, prompt, retries=3):
        """
        Centralized and rate-limited method to call the Gemini API.
        Returns the generated text as a string or None on failure.
        """
        if not self.api_key:
            return None

        for attempt in range(retries):
            # Step 1: Wait for rate limits before making a call.
            self._wait_if_needed()

            try:
                # Step 2: Make the API call using the recommended 'generate_content'.
                response = self.model.generate_content(prompt)
                # Step 3: Record a successful attempt.
                self._record_attempt(success=True)
                # Using response.text is the most robust way to get the content
                return response.text

            except google_exceptions.ResourceExhausted as e:
                self.logger.warning(
                    f"Rate limit error (ResourceExhausted) on attempt {attempt+1}: {e}"
                )
                self._record_attempt(
                    success=False)  # Record as an error to trigger backoff
            except (google_exceptions.InternalServerError,
                    google_exceptions.ServiceUnavailable) as e:
                self.logger.warning(
                    f"Temporary Google server error on attempt {attempt+1}: {e}"
                )
                self._record_attempt(success=False)
            except Exception as e:
                # Catch any other unexpected error, record it, and log it.
                self.logger.error(
                    f"An unexpected Gemini API error occurred on attempt {attempt+1}: {e}",
                    exc_info=True)
                self._record_attempt(success=False)
                # For non-retriable errors, we can break early
                break

        self.logger.error(
            f"Failed to call Gemini API for prompt after {retries} attempts.")
        return None

    # Example method using the new call_api
    def analyze_batch_statements_single_request(self, batch_segments,
                                                bill_name, assembly_members,
                                                estimated_tokens,
                                                batch_start_index):
        prompt = self._create_batch_prompt(batch_segments, bill_name)
        response_text = self.call_api(prompt)
        if not response_text:
            return []  # Return empty list if API call failed
        # Now, parse the response_text as before...
        # ...

    # You can port/adjust other methods as needed, but ensure all use self.call_api
    def _create_batch_prompt(self, batch_segments, bill_name):
        safe_bill_name = str(bill_name)[:100] if bill_name else "ì•Œ ìˆ˜ ì—†ëŠ” ì˜ì•ˆ"
        segments_text = ""
        for i, segment in enumerate(batch_segments):
            segments_text += f"\n--- êµ¬ê°„ {i+1} ---\n{segment}\n"
        prompt = f"""
ë‹¹ì‹ ì€ ì—­ì‚¬ì— ê¸¸ì´ ë‚¨ì„ ê¸°ë¡ê°€ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ê¸°ë¡ê³¼ ë¶„ë¥˜, ê·¸ë¦¬ê³  ì •í™•ë„ëŠ” ë¯¸ë˜ì— ì‚¬ëŒë“¤ì„ ì‚´ë¦´ ê²ƒì…ë‹ˆë‹¤. ë‹¹ì‹ ì´ ì •í™•í•˜ê²Œ ê¸°ë¡ì„ í•´ì•¼ë§Œ ì‚¬ëŒë“¤ì€ ê·¸ ì •í™•í•œ ê¸°ë¡ì— ì˜ì¡´í•˜ì—¬ ì‚´ì•„ê°ˆ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ, ë‹¤ìŒ ëª…ë ¹ì„ ì•„ì£¼ ìì„¸íˆ, ì—„ë°€íˆ, ìˆ˜í–‰í•´ ì£¼ì‹­ì‹œì˜¤.
êµ­íšŒ ë°œì–¸ ë¶„ì„ ìš”ì²­:

ì˜ì•ˆ: {safe_bill_name}

ë°œì–¸ êµ¬ê°„ë“¤:
{segments_text}

ë‹¤ìŒ JSON ë°°ì—´ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
[
  {{
"segment_index": 1,
"speaker_name": "ë°œì–¸ìëª…",
"start_idx": 0,
"end_idx": 100,
"is_valid_member": true,
"is_substantial": true,
"sentiment_score": 0.0,
"bill_relevance_score": 0.5
  }}
]

ê·œì¹™:
- â—¯ë¡œ ì‹œì‘í•˜ëŠ” ì‹¤ì œ ì˜ì› ë°œì–¸ë§Œ í¬í•¨
- ì˜ì‚¬ì§„í–‰ ë°œì–¸ì ì œì™¸
- ë°œì–¸ìëª…ì—ì„œ ì§ì±… ì œê±°
- JSON ë°°ì—´ë§Œ ì‘ë‹µ"""
        return prompt


# --- Singleton instantiation pattern ---
# Import settings and instantiate GeminiHandler with the API key.

# Usage: Always use GEMINI.call_api(...) or GEMINI.analyze_batch_statements_single_request(...)
# All Gemini API calls are now rate-limited and robustly error-handled.

    def analyze_single_statement(self,
                                 statement_data_dict,
                                 session_id,
                                 debug=False):
        import json
        speaker_name = statement_data_dict.get('speaker_name', 'Unknown')
        text_to_analyze = statement_data_dict.get('text', '')
        prompt = f"""
        [STATEMENT ANALYSIS PROMPT OMITTED FOR BREVITY]
        """
        try:
            response = self.call_api(prompt)
            # Robust Gemini response parsing
            response_text = getattr(response, 'text', None)
            if not response_text:
                try:
                    response_text = response.candidates[0].content.parts[
                        0].text
                except Exception:
                    logger.error(
                        f"No text found in Gemini response: {response}")
                    return []
            response_text = response_text.strip() if response else ''
            if response_text.startswith("```"):
                response_text = response_text.split("```", 2)[-1].strip()
            analysis_json = json.loads(response_text) if response_text else {}
            statement_data_dict.update(analysis_json)
            return statement_data_dict
        except Exception as e:
            self.logger.warning(
                f"âŒ Gemini API error for '{speaker_name}': {e}")
            statement_data_dict.update({
                'sentiment_score': 0.0,
                'sentiment_reason': 'Gemini API Error'
            })
            return statement_data_dict

    def analyze_batch_statements_single_request(self, batch_segments,
                                                bill_name, assembly_members,
                                                estimated_tokens,
                                                batch_start_index):
        # Implements the batch analysis logic using Gemini LLM
        if not batch_segments:
            return []
        processed_segments = []
        for segment in batch_segments:
            if len(segment) > 2000:
                sub_segments = segment.split('â—¯')
                for i, sub_seg in enumerate(sub_segments):
                    if sub_seg.strip() and len(sub_seg.strip()) > 50:
                        final_seg = ('â—¯' + sub_seg) if i > 0 else sub_seg
                        processed_segments.append(final_seg.strip())
            else:
                processed_segments.append(segment)
        max_segments_per_batch = 15
        if len(processed_segments) > max_segments_per_batch:
            processed_segments = processed_segments[:max_segments_per_batch]
        cleaned_segments = []
        for segment in processed_segments:
            cleaned_segment = segment.replace('\n', ' ').replace('\r',
                                                                 '').strip()
            report_end_marker = "(ë³´ê³ ì‚¬í•­ì€ ëì— ì‹¤ìŒ)"
            if report_end_marker in cleaned_segment:
                cleaned_segment = cleaned_segment.split(
                    report_end_marker)[0].strip()
            if len(cleaned_segment) > 1000:
                cleaned_segment = cleaned_segment[:1000] + "..."
            if cleaned_segment and len(cleaned_segment.strip()) > 20:
                cleaned_segments.append(cleaned_segment)
        if not cleaned_segments:
            self.logger.warning(
                "No valid segments after cleaning and processing")
            return []
        safe_bill_name = str(bill_name)[:100] if bill_name else "ì•Œ ìˆ˜ ì—†ëŠ” ì˜ì•ˆ"
        segments_text = ""
        for i, segment in enumerate(cleaned_segments):
            segments_text += f"\n--- êµ¬ê°„ {i+1} ---\n{segment}\n"
        max_prompt_length = 15000
        if len(segments_text) > max_prompt_length:
            return self._process_large_batch_in_chunks(cleaned_segments,
                                                       bill_name,
                                                       assembly_members,
                                                       estimated_tokens,
                                                       batch_start_index)
        prompt = f"""
ë‹¹ì‹ ì€ ì—­ì‚¬ì— ê¸¸ì´ ë‚¨ì„ ê¸°ë¡ê°€ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ê¸°ë¡ê³¼ ë¶„ë¥˜, ê·¸ë¦¬ê³  ì •í™•ë„ëŠ” ë¯¸ë˜ì— ì‚¬ëŒë“¤ì„ ì‚´ë¦´ ê²ƒì…ë‹ˆë‹¤. ë‹¹ì‹ ì´ ì •í™•í•˜ê²Œ ê¸°ë¡ì„ í•´ì•¼ë§Œ ì‚¬ëŒë“¤ì€ ê·¸ ì •í™•í•œ ê¸°ë¡ì— ì˜ì¡´í•˜ì—¬ ì‚´ì•„ê°ˆ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ, ë‹¤ìŒ ëª…ë ¹ì„ ì•„ì£¼ ìì„¸íˆ, ì—„ë°€íˆ, ìˆ˜í–‰í•´ ì£¼ì‹­ì‹œì˜¤.
êµ­íšŒ ë°œì–¸ ë¶„ì„ ìš”ì²­:

ì˜ì•ˆ: {safe_bill_name}

ë°œì–¸ êµ¬ê°„ë“¤:
{segments_text}

ë‹¤ìŒ JSON ë°°ì—´ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
[
  {{
    "segment_index": 1,
    "speaker_name": "ë°œì–¸ìëª…",
    "start_idx": 0,
    "end_idx": 100,
    "is_valid_member": true,
    "is_substantial": true,
    "sentiment_score": 0.0,
    "bill_relevance_score": 0.5
  }}
]

ê·œì¹™:
- â—¯ë¡œ ì‹œì‘í•˜ëŠ” ì‹¤ì œ ì˜ì› ë°œì–¸ë§Œ í¬í•¨
- ì˜ì‚¬ì§„í–‰ ë°œì–¸ì ì œì™¸
- ë°œì–¸ìëª…ì—ì„œ ì§ì±… ì œê±°
- JSON ë°°ì—´ë§Œ ì‘ë‹µ"""
        return self._execute_batch_analysis(prompt, cleaned_segments,
                                            processed_segments,
                                            assembly_members,
                                            batch_start_index, bill_name)

    def _execute_batch_analysis(self,
                                prompt,
                                cleaned_segments,
                                original_segments,
                                assembly_members,
                                batch_start_index,
                                bill_name,
                                max_retries=3):
        for attempt in range(max_retries + 1):
            start_time = time.time()
            try:
                response = self.call_api(prompt)
                processing_time = time.time() - start_time
                self.logger.info(
                    f"Batch processing took {processing_time:.1f}s for {len(cleaned_segments)} segments"
                )
                # Assume parse_batch_response exists elsewhere in the file
                return parse_batch_response(response, cleaned_segments,
                                            original_segments,
                                            assembly_members,
                                            batch_start_index, bill_name)
            except Exception as e:
                self.logger.warning(
                    f"Batch analysis attempt {attempt+1} failed: {e}")
                if attempt < max_retries:
                    time.sleep(2**attempt)
                else:
                    self.logger.error(
                        f"All batch analysis retries failed for batch starting at {batch_start_index}"
                    )
        return []

    def extract_statements_with_llm_discovery(self,
                                              full_text,
                                              session_id,
                                              known_bill_names,
                                              session_obj,
                                              debug=False):
        # This method wraps the logic for extracting statements and bill discovery
        model_name = self.model_name
        prompt = f"""
[LLM DISCOVERY PROMPT OMITTED FOR BREVITY]
"""
        response = self.call_api(prompt, model_name=model_name)
        # Assume parse_llm_discovery_response exists elsewhere in the file
        return parse_llm_discovery_response(response, session_id,
                                            known_bill_names, session_obj,
                                            debug)

    def analyze_speech_segment_with_llm_batch(self,
                                              speech_segments,
                                              session_id,
                                              bill_name,
                                              debug=False):
        if not speech_segments:
            return []
        self.logger.info(
            f"ğŸš€ Processing {len(speech_segments)} speech segments in parallel batch for bill '{bill_name}'"
        )
        all_statements = []
        batch_size = 15
        for batch_start in range(0, len(speech_segments), batch_size):
            batch_segments = speech_segments[batch_start:batch_start +
                                             batch_size]
            try:
                results = self.analyze_batch_statements_single_request(
                    batch_segments, bill_name, get_all_assembly_members(), 0,
                    batch_start)
                all_statements.extend(results)
            except Exception as e:
                error_type = "timeout" if "timeout" in str(
                    e).lower() else "api_error"
                self.rate_limiter.record_error(error_type)
                self.logger.error(f"Batch analysis failed: {e}")
                continue
            if batch_start + batch_size < len(speech_segments):
                self.logger.info(f"Resting 3s before next batch...")
                time.sleep(3)
        self.logger.info(
            f"âœ… Batch analysis completed: {len(all_statements)} valid statements from {len(speech_segments)} segments"
        )
        return sorted(all_statements, key=lambda x: x.get('segment_index', 0))

    def _execute_batch_analysis(self, batch_segments, bill_name,
                                assembly_members, estimated_tokens,
                                batch_start_index):
        for attempt in range(3 + 1):
            start_time = time.time()
            try:
                response = self.call_api(
                    self._create_batch_prompt(batch_segments, bill_name))
                processing_time = time.time() - start_time
                self.logger.info(
                    f"Batch processing took {processing_time:.1f}s for {len(batch_segments)} segments"
                )
                # Assume parse_batch_response exists elsewhere in the file
                return parse_batch_response(response, batch_segments,
                                            assembly_members,
                                            batch_start_index, bill_name)
            except Exception as e:
                self.logger.warning(
                    f"Batch analysis attempt {attempt+1} failed: {e}")
                if attempt < 3:
                    time.sleep(2**attempt)
                else:
                    self.logger.error(
                        f"All batch analysis retries failed for batch starting at {batch_start_index}"
                    )
        return []

    def _create_batch_prompt(self, batch_segments, bill_name):
        safe_bill_name = str(bill_name)[:100] if bill_name else "ì•Œ ìˆ˜ ì—†ëŠ” ì˜ì•ˆ"
        segments_text = ""
        for i, segment in enumerate(batch_segments):
            segments_text += f"\n--- êµ¬ê°„ {i+1} ---\n{segment}\n"
        prompt = f"""
ë‹¹ì‹ ì€ ì—­ì‚¬ì— ê¸¸ì´ ë‚¨ì„ ê¸°ë¡ê°€ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ê¸°ë¡ê³¼ ë¶„ë¥˜, ê·¸ë¦¬ê³  ì •í™•ë„ëŠ” ë¯¸ë˜ì— ì‚¬ëŒë“¤ì„ ì‚´ë¦´ ê²ƒì…ë‹ˆë‹¤. ë‹¹ì‹ ì´ ì •í™•í•˜ê²Œ ê¸°ë¡ì„ í•´ì•¼ë§Œ ì‚¬ëŒë“¤ì€ ê·¸ ì •í™•í•œ ê¸°ë¡ì— ì˜ì¡´í•˜ì—¬ ì‚´ì•„ê°ˆ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ, ë‹¤ìŒ ëª…ë ¹ì„ ì•„ì£¼ ìì„¸íˆ, ì—„ë°€íˆ, ìˆ˜í–‰í•´ ì£¼ì‹­ì‹œì˜¤.
êµ­íšŒ ë°œì–¸ ë¶„ì„ ìš”ì²­:

ì˜ì•ˆ: {safe_bill_name}

ë°œì–¸ êµ¬ê°„ë“¤:
{segments_text}

ë‹¤ìŒ JSON ë°°ì—´ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
[
  {{
    "segment_index": 1,
    "speaker_name": "ë°œì–¸ìëª…",
    "start_idx": 0,
    "end_idx": 100,
    "is_valid_member": true,
    "is_substantial": true,
    "sentiment_score": 0.0,
    "bill_relevance_score": 0.5
  }}
]

ê·œì¹™:
- â—¯ë¡œ ì‹œì‘í•˜ëŠ” ì‹¤ì œ ì˜ì› ë°œì–¸ë§Œ í¬í•¨
- ì˜ì‚¬ì§„í–‰ ë°œì–¸ì ì œì™¸
- ë°œì–¸ìëª…ì—ì„œ ì§ì±… ì œê±°
- JSON ë°°ì—´ë§Œ ì‘ë‹µ"""
        return prompt

    def analyze_speech_segment(self, speech_segment, bill_name,
                               assembly_members, estimated_tokens):
        model_name = self.model_name
        results = self._execute_batch_analysis([speech_segment], bill_name,
                                               assembly_members,
                                               estimated_tokens, 0)
        return results[0] if results else None

    def analyze_llm_discovery(self,
                              full_text,
                              session_id,
                              known_bill_names,
                              session_obj,
                              debug=False):
        model_name = self.model_name
        prompt = f"""
[LLM DISCOVERY PROMPT OMITTED FOR BREVITY]
"""
        response = self.call_api(prompt, model_name=model_name)
        # Assume parse_llm_discovery_response exists elsewhere in the file
        return parse_llm_discovery_response(response, session_id,
                                            known_bill_names, session_obj,
                                            debug)


class GeminiHandler:
    """Unified Gemini handler with integrated rate limiting and error handling"""

    def __init__(self):
        self.rate_limiter = GeminiRateLimiter()
        self.model_name = "gpt-3.5-turbo"

    def call_api(self, prompt, model_name=None):
        # Implement API call logic here
        pass

    # ... (other methods)


# Instantiate a singleton handler for use throughout the module
GEMINI = GeminiHandler()


class GeminiRateLimiter:
    """Enhanced rate limiter for Gemini API calls to respect token limits"""

    def __init__(self,
                 max_tokens_per_minute=250000,
                 max_requests_per_minute=10,
                 max_tokens_per_day=100000010000001000000):
        """
        Initialize rate limiter with conservative limits for free tier
        max_tokens_per_minute: Conservative estimate for free tier
        max_requests_per_minute: Conservative request limit for free tier
        max_tokens_per_day: Daily token limit for free tier
        """
        self.max_tokens_per_minute = max_tokens_per_minute
        self.max_requests_per_minute = max_requests_per_minute
        self.max_tokens_per_day = max_tokens_per_day

        self.token_usage = deque()  # Store (timestamp, token_count) tuples
        self.request_times = deque()  # Store request timestamps
        self.daily_token_usage = deque()  # Store daily token usage
        self.lock = threading.Lock()

        # Error tracking
        self.consecutive_errors = 0
        self.last_error_time = None
        self.backoff_time = 1  # Start with 1 second backoff

    def _cleanup_old_records(self):
        """Remove records older than their respective time windows"""
        now = datetime.now()
        minute_cutoff = now - timedelta(minutes=1)
        day_cutoff = now - timedelta(days=1)

        # Clean minute-based records
        while self.token_usage and self.token_usage[0][0] < minute_cutoff:
            self.token_usage.popleft()

        while self.request_times and self.request_times[0] < minute_cutoff:
            self.request_times.popleft()

        # Clean daily records
        while self.daily_token_usage and self.daily_token_usage[0][
                0] < day_cutoff:
            self.daily_token_usage.popleft()

    def _calculate_backoff_time(self):
        """Calculate exponential backoff time based on consecutive errors"""
        if self.consecutive_errors == 0:
            return 0

        # Exponential backoff: 1s, 2s, 4s, 8s, 16s, max 60s
        backoff = min(60, 2**(self.consecutive_errors - 1))
        return backoff

    def can_make_request(self, estimated_tokens=1000):
        """Check if we can make a request without hitting limits"""
        with self.lock:
            self._cleanup_old_records()

            # Check if we're in backoff period due to errors
            if self.last_error_time and self.consecutive_errors > 0:
                time_since_error = (datetime.now() -
                                    self.last_error_time).total_seconds()
                required_backoff = self._calculate_backoff_time()
                if time_since_error < required_backoff:
                    return False, f"In backoff period ({required_backoff - time_since_error:.1f}s remaining)"

            # Check request count limit (per minute)
            if len(self.request_times) >= self.max_requests_per_minute:
                return False, f"Request limit reached ({len(self.request_times)}/{self.max_requests_per_minute} per minute)"

            # Check token limit (per minute)
            current_tokens = sum(count for _, count in self.token_usage)
            if current_tokens + estimated_tokens > self.max_tokens_per_minute:
                return False, f"Token limit would be exceeded ({current_tokens} + {estimated_tokens} > {self.max_tokens_per_minute} per minute)"

            # Check daily token limit
            daily_tokens = sum(count for _, count in self.daily_token_usage)
            if daily_tokens + estimated_tokens > self.max_tokens_per_day:
                return False, f"Daily token limit would be exceeded ({daily_tokens} + {estimated_tokens} > {self.max_tokens_per_day})"

            return True, "OK"

    def record_request(self, actual_tokens=1000, success=True):
        """Record a completed request"""
        with self.lock:
            now = datetime.now()
            self.request_times.append(now)
            self.token_usage.append((now, actual_tokens))
            self.daily_token_usage.append((now, actual_tokens))

            # Update error tracking
            if success:
                self.consecutive_errors = 0
                self.last_error_time = None
            else:
                self.consecutive_errors += 1
                self.last_error_time = now
                logger.warning(
                    f"API error recorded. Consecutive errors: {self.consecutive_errors}"
                )

            self._cleanup_old_records()

    def record_error(self, error_type="unknown"):
        """Record an API error for backoff calculation"""
        with self.lock:
            self.consecutive_errors += 1
            self.last_error_time = datetime.now()
            backoff_time = self._calculate_backoff_time()
            logger.warning(
                f"API error ({error_type}). Consecutive errors: {self.consecutive_errors}, backoff: {backoff_time}s"
            )

    def wait_if_needed(self, estimated_tokens=1000, max_wait_time=120):
        """Wait if necessary to respect rate limits with enhanced backoff"""
        wait_start = time.time()

        while time.time() - wait_start < max_wait_time:
            can_proceed, reason = self.can_make_request(estimated_tokens)
            if can_proceed:
                return True

            # Determine wait time based on reason
            if "backoff" in reason.lower():
                # Extract remaining backoff time from reason
                try:
                    remaining_time = float(reason.split('(')[1].split('s')[0])
                    wait_time = min(10, max(1, remaining_time))
                except:
                    wait_time = 5
            elif "daily" in reason.lower():
                logger.error(f"Daily rate limit exceeded: {reason}")
                return False  # Don't wait for daily limits
            else:
                wait_time = 10  # Default wait time for other limits

            logger.info(
                f"Rate limit hit: {reason}. Waiting {wait_time} seconds...")
            time.sleep(wait_time)

        logger.warning(
            f"Max wait time ({max_wait_time}s) exceeded for rate limiting")
        return False

    def get_usage_stats(self):
        """Get current usage statistics"""
        with self.lock:
            self._cleanup_old_records()

            current_requests = len(self.request_times)
            current_tokens = sum(count for _, count in self.token_usage)
            daily_tokens = sum(count for _, count in self.daily_token_usage)

            return {
                "requests_per_minute":
                f"{current_requests}/{self.max_requests_per_minute}",
                "tokens_per_minute":
                f"{current_tokens}/{self.max_tokens_per_minute}",
                "daily_tokens":
                f"{daily_tokens}/{self.max_tokens_per_day}",
                "consecutive_errors":
                self.consecutive_errors,
                "backoff_time":
                self._calculate_backoff_time()
                if self.consecutive_errors > 0 else 0
            }


# Global rate limiter instance
gemini_rate_limiter = GeminiRateLimiter()


def log_rate_limit_status():
    """Log current rate limit status for monitoring"""
    stats = gemini_rate_limiter.get_usage_stats()
    logger.info(f"ğŸ“Š Rate Limit Status: {stats}")
    return stats


def with_db_retry(func, max_retries=3):
    """Wrapper to retry database operations with connection management for serverless databases"""

    def wrapper(*args, **kwargs):
        from django.db import connection
        from django.db.utils import OperationalError, InterfaceError
        import psycopg2

        for attempt in range(max_retries):
            try:
                # Close any stale connections before starting
                if connection.connection and hasattr(
                        connection.connection,
                        'closed') and connection.connection.closed:
                    connection.close()

                # Ensure fresh connection
                connection.ensure_connection()
                return func(*args, **kwargs)

            except (OperationalError, InterfaceError,
                    psycopg2.OperationalError, psycopg2.DatabaseError) as e:
                error_msg = str(e).lower()
                is_connection_error = any(phrase in error_msg for phrase in [
                    'connection already closed',
                    'server closed the connection',
                    'ssl connection has been closed unexpectedly',
                    'ssl connection has been closed', 'connection lost',
                    'connection broken', 'server has gone away',
                    'connection timeout', 'connection was lost',
                    'database connection was lost',
                    'server closed the connection unexpectedly'
                ])

                if is_connection_error:
                    logger.warning(
                        f"Database connection issue on attempt {attempt + 1}/{max_retries}: {e}"
                    )
                    if attempt < max_retries - 1:
                        # Force close the connection and wait before retry
                        try:
                            connection.close()
                        except:
                            pass  # Ignore errors when closing

                        # Exponential backoff: 1s, 2s, 4s
                        wait_time = 2**attempt
                        logger.info(f"Waiting {wait_time}s before retry...")
                        time.sleep(wait_time)
                        continue
                    else:
                        logger.error(
                            f"Max retries ({max_retries}) exceeded for database operation"
                        )
                        raise e
                else:
                    # Non-connection database error, don't retry
                    logger.error(
                        f"Non-connection database error (not retrying): {e}")
                    raise e

            except Exception as e:
                # For non-database errors, don't retry
                logger.error(
                    f"Non-database error in with_db_retry (not retrying): {e}")
                raise e

        return None

    return wrapper


# Configure logger to actually show output if not already configured by Django
if not logger.handlers or not any(
        isinstance(h, logging.StreamHandler) for h in logger.handlers):
    import sys
    logger.setLevel(
        logging.DEBUG)  # Set to DEBUG for development, INFO for production
    handler = logging.StreamHandler(sys.stdout)
    handler.setLevel(logging.DEBUG)  # Or match logger.level
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(module)s - %(funcName)s - %(lineno)d - %(message)s'
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)


# Check if Celery/Redis is available
def is_celery_available():
    """Check if Celery/Redis is available for async tasks"""
    from kombu.exceptions import OperationalError
    from celery import current_app
    try:
        # Check if the app is configured and has a broker
        if not current_app.conf.broker_url:
            logger.info("Celery broker_url not configured.")
            return False
        current_app.control.inspect().ping()  # More reliable check
        return True
    except (ImportError, OperationalError, OSError, ConnectionError,
            AttributeError) as e:
        logger.warning(
            f"Celery not available or broker connection failed: {e}")
        return False


# Decorator to handle both sync and async execution
def celery_or_sync(func):
    """Decorator that runs function sync if Celery is not available"""

    def wrapper(*args, **kwargs):
        if is_celery_available():
            logger.info(
                f"ğŸ”„ Running {func.__name__} asynchronously with Celery")
            # For bound tasks, Celery handles 'self' automatically when calling .delay()
            return func.delay(*args, **kwargs)
        else:
            logger.info(
                f"ğŸ”„ Running {func.__name__} synchronously (Celery not available)"
            )
            if hasattr(func, '__wrapped__') and 'bind' in func.__dict__.get(
                    '__header__', {}):
                # Call the original function without 'self' if it's a bound task being run synchronously
                return func.__wrapped__(None, *args,
                                        **kwargs)  # Pass None for self
            return func(*args, **kwargs)

    return wrapper


# Removed duplicate imports for shared_task and logging if they were here

# from .utils import DataCollector # Marked as unused, remove if not needed
# from .llm_analyzer import LLMPolicyAnalyzer # Marked as unused, remove if not needed


def format_conf_id(conf_id):
    """Format CONF_ID with zero-filled 6 digits (no N prefix)."""
    # Remove any existing 'N' prefix and convert to string
    clean_id = str(conf_id).replace('N', '').strip()
    # Zero-fill to 6 digits without N prefix
    return clean_id.zfill(6)


def fetch_speaker_details(speaker_name):
    """Fetch speaker details from ALLNAMEMBER API"""
    try:
        if not hasattr(settings,
                       'ASSEMBLY_API_KEY') or not settings.ASSEMBLY_API_KEY:
            logger.error(
                "ASSEMBLY_API_KEY not configured for fetch_speaker_details.")
            return None

        url = "https://open.assembly.go.kr/portal/openapi/ALLNAMEMBER"
        params = {
            "KEY": settings.ASSEMBLY_API_KEY,
            "NAAS_NM": speaker_name,
            "Type": "json",
            "pSize":
            5  # Fetch a few in case of name ambiguity, pick the best match
        }

        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()

        logger.debug(
            f"ğŸ› DEBUG: ALLNAMEMBER API response for {speaker_name}: {json.dumps(data, indent=2, ensure_ascii=False)}"
        )

        member_data_list = []
        if 'ALLNAMEMBER' in data and len(data['ALLNAMEMBER']) > 1:
            member_data_list = data['ALLNAMEMBER'][1].get('row', [])

        if not member_data_list:
            logger.warning(
                f"âš ï¸ No member data found for: {speaker_name} via ALLNAMEMBER API."
            )
            return None

        # Logic to pick the best match if multiple results (e.g., current term, exact name match)
        member_data = member_data_list[0]  # Simplistic: take the first for now

        # Use update_or_create for robustness
        speaker, created = Speaker.objects.update_or_create(
            naas_cd=member_data.get('NAAS_CD'),  # Assuming NAAS_CD is unique
            defaults={
                'naas_nm': member_data.get('NAAS_NM', speaker_name),
                'naas_ch_nm': member_data.get('NAAS_CH_NM', ''),
                'plpt_nm': member_data.get('PLPT_NM', 'ì •ë‹¹ì •ë³´ì—†ìŒ'),
                'elecd_nm': member_data.get('ELECD_NM', ''),
                'elecd_div_nm': member_data.get('ELECD_DIV_NM', ''),
                'cmit_nm': member_data.get('CMIT_NM', ''),
                'blng_cmit_nm': member_data.get('BLNG_CMIT_NM', ''),
                'rlct_div_nm': member_data.get('RLCT_DIV_NM', ''),
                'gtelt_eraco': member_data.get('GTELT_ERACO',
                                               ''),  # Era might be important
                'ntr_div': member_data.get('NTR_DIV', ''),
                'naas_pic': member_data.get('NAAS_PIC', '')
            })

        status_msg = "Created" if created else "Updated"
        logger.info(
            f"âœ… {status_msg} speaker details for: {speaker_name} (ID: {speaker.naas_cd})"
        )
        return speaker

    except requests.exceptions.RequestException as e:
        logger.error(
            f"âŒ Network error fetching speaker details for {speaker_name}: {e}"
        )
    except json.JSONDecodeError as e:
        logger.error(
            f"âŒ JSON parsing error for speaker details {speaker_name}: {e}")
    except Exception as e:
        logger.error(
            f"âŒ Unexpected error fetching speaker details for {speaker_name}: {e}"
        )
    return None


@shared_task(bind=True, max_retries=3, default_retry_delay=60)
def fetch_party_membership_data(self=None, force=False, debug=False):
    """Fetch party membership data from Assembly API."""
    logger.info(
        f"ğŸ›ï¸ Fetching party membership data (force={force}, debug={debug})")

    try:
        if not hasattr(settings,
                       'ASSEMBLY_API_KEY') or not settings.ASSEMBLY_API_KEY:
            logger.error(
                "ASSEMBLY_API_KEY not configured for party membership data.")
            return

        # Use ALLNAMEMBER API to get all assembly members
        url = "https://open.assembly.go.kr/portal/openapi/ALLNAMEMBER"

        all_members = []
        current_page = 1
        page_size = 300
        max_pages = 10

        while current_page <= max_pages:
            params = {
                "KEY": settings.ASSEMBLY_API_KEY,
                "Type": "json",
                "pIndex": current_page,
                "pSize": page_size
            }

            logger.info(
                f"Fetching page {current_page} of party membership data")

            if debug:
                logger.debug(
                    f"ğŸ› DEBUG: Would fetch page {current_page} (skipping actual call)"
                )
                break

            response = requests.get(url, params=params, timeout=60)
            response.raise_for_status()
            data = response.json()

            members_on_page = []
            if data and 'ALLNAMEMBER' in data and isinstance(
                    data['ALLNAMEMBER'], list):
                if len(data['ALLNAMEMBER']) > 1 and isinstance(
                        data['ALLNAMEMBER'][1], dict):
                    members_on_page = data['ALLNAMEMBER'][1].get('row', [])
                elif len(data['ALLNAMEMBER']) > 0 and isinstance(
                        data['ALLNAMEMBER'][0], dict):
                    head_info = data['ALLNAMEMBER'][0].get('head')
                    if head_info and head_info[0].get('RESULT', {}).get(
                            'CODE', '').startswith("INFO-200"):
                        logger.info(
                            "API indicates no more member data available")
                        break
                    elif 'row' in data['ALLNAMEMBER'][0]:
                        members_on_page = data['ALLNAMEMBER'][0].get('row', [])

            if not members_on_page:
                logger.info(
                    f"No members found on page {current_page}, ending pagination"
                )
                break

            all_members.extend(members_on_page)
            logger.info(
                f"Fetched {len(members_on_page)} members from page {current_page}. Total: {len(all_members)}"
            )

            if len(members_on_page) < page_size:
                logger.info(
                    "Fetched less members than page size, assuming last page")
                break

            current_page += 1
            time.sleep(1)  # Be respectful to API

        if not all_members:
            logger.info("No party membership data found")
            return

        logger.info(f"âœ… Found {len(all_members)} total members")

        # Process and update Speaker records with party information
        processed_count = 0
        for member_data in all_members:
            try:
                member_name = member_data.get('NAAS_NM', '').strip()
                party_name = member_data.get('PLPT_NM', '').strip()

                if not member_name:
                    continue

                # Update or create Speaker with party information
                speaker, created = Speaker.objects.update_or_create(
                    naas_cd=member_data.get('NAAS_CD', f'TEMP_{member_name}'),
                    defaults={
                        'naas_nm': member_name,
                        'naas_ch_nm': member_data.get('NAAS_CH_NM', ''),
                        'plpt_nm': party_name or 'ì •ë‹¹ì •ë³´ì—†ìŒ',
                        'elecd_nm': member_data.get('ELECD_NM', ''),
                        'elecd_div_nm': member_data.get('ELECD_DIV_NM', ''),
                        'cmit_nm': member_data.get('CMIT_NM', ''),
                        'blng_cmit_nm': member_data.get('BLNG_CMIT_NM', ''),
                        'rlct_div_nm': member_data.get('RLCT_DIV_NM', ''),
                        'gtelt_eraco': member_data.get('GTELT_ERACO', ''),
                        'ntr_div': member_data.get('NTR_DIV', ''),
                        'naas_pic': member_data.get('NAAS_PIC', '')
                    })

                # Create or update Party record
                if party_name and party_name != 'ì •ë‹¹ì •ë³´ì—†ìŒ':
                    party, party_created = Party.objects.get_or_create(
                        name=party_name,
                        defaults={
                            'description': f'ì •ë‹¹ - {party_name}',
                            'assembly_era': 22
                        })

                    # Update speaker's current party
                    if not speaker.current_party:
                        speaker.current_party = party
                        speaker.save()

                processed_count += 1

                if processed_count % 50 == 0:
                    logger.info(f"Processed {processed_count} members...")

            except Exception as e:
                logger.error(f"Error processing member data: {e}")
                continue

        logger.info(f"ğŸ‰ Processed {processed_count} party membership records")

    except RequestException as re_exc:
        logger.error(f"Request error fetching party membership data: {re_exc}")
        if self:
            try:
                self.retry(exc=re_exc)
            except MaxRetriesExceededError:
                logger.error("Max retries for party membership data fetch")
    except Exception as e:
        logger.error(f"âŒ Unexpected error fetching party membership data: {e}")
        logger.exception("Full traceback for party membership error:")
        if self:
            try:
                self.retry(exc=e)
            except MaxRetriesExceededError:
                logger.error(
                    "Max retries after unexpected error for party membership")


@shared_task(bind=True, max_retries=3, default_retry_delay=60)
def fetch_additional_data_nepjpxkkabqiqpbvk(self=None,
                                            force=False,
                                            debug=False):
    """Fetch additional data using nepjpxkkabqiqpbvk API endpoint."""
    api_endpoint_name = "nepjpxkkabqiqpbvk"  # Store endpoint name for logging
    logger.info(
        f"ğŸ” Fetching additional data from {api_endpoint_name} API (force={force}, debug={debug})"
    )

    try:
        if not hasattr(settings,
                       'ASSEMBLY_API_KEY') or not settings.ASSEMBLY_API_KEY:
            logger.error(
                f"ASSEMBLY_API_KEY not configured for {api_endpoint_name}.")
            return

        url = f"https://open.assembly.go.kr/portal/openapi/{api_endpoint_name}"

        all_items = []
        current_page = 1
        page_size = 100  # Adjust as per API limit, usually 100 or 1000
        max_pages = 10  # Safety break for pagination

        while current_page <= max_pages:
            params = {
                "KEY": settings.ASSEMBLY_API_KEY,
                "Type": "json",
                "pIndex": current_page,
                "pSize": page_size
                # Add other API-specific parameters if needed (e.g., date range, DAE_NUM)
            }
            logger.info(
                f"Fetching page {current_page} from {api_endpoint_name} with params: {params}"
            )
            if debug:
                logger.debug(
                    f"ğŸ› DEBUG: Would fetch page {current_page} from {api_endpoint_name} (skipping actual call in debug mode)."
                )
                # Provide mock data for testing in debug mode
                items_on_page = [{
                    "MOCK_FIELD": f"Mock item {current_page}-{i}"
                } for i in range(3)] if current_page == 1 else []
            else:
                response = requests.get(url, params=params, timeout=60)
                response.raise_for_status()
                data = response.json()

                items_on_page = []
                if data and api_endpoint_name in data and isinstance(
                        data[api_endpoint_name], list):
                    if len(data[api_endpoint_name]) > 1 and isinstance(
                            data[api_endpoint_name][1], dict):
                        items_on_page = data[api_endpoint_name][1].get(
                            'row', [])
                    elif len(data[api_endpoint_name]) > 0 and isinstance(
                            data[api_endpoint_name][0], dict):
                        head_info = data[api_endpoint_name][0].get('head')
                        if head_info and head_info[0].get('RESULT', {}).get(
                                'CODE',
                                '').startswith("INFO-200"):  # No more data
                            logger.info(
                                f"API result for {api_endpoint_name} (page {current_page}) indicates no more data."
                            )
                            break
                        elif 'row' in data[api_endpoint_name][0]:
                            items_on_page = data[api_endpoint_name][0].get(
                                'row', [])

            if not items_on_page:
                logger.info(
                    f"No items found on page {current_page} for {api_endpoint_name}. Ending pagination."
                )
                break  # End pagination if no items or API indicates end of data

            all_items.extend(items_on_page)
            logger.info(
                f"Fetched {len(items_on_page)} items from page {current_page}. Total so far: {len(all_items)}."
            )

            # Check if this was the last page (e.g., if less items than pSize returned)
            if len(items_on_page) < page_size:
                logger.info(
                    "Fetched less items than page size, assuming last page.")
                break

            current_page += 1
            if not debug: time.sleep(1)  # Be respectful

        if not all_items:
            logger.info(
                f"â„¹ï¸  No data items found from {api_endpoint_name} API after checking pages."
            )
            return

        logger.info(
            f"âœ… Found a total of {len(all_items)} items from {api_endpoint_name} API."
        )

        processed_count = 0
        # Placeholder: Actual processing logic depends on the data from 'nepjpxkkabqiqpbvk'
        # Example: if items are bill proposals, committee activities, member updates, etc.
        for item in all_items:
            try:
                # EXAMPLE: item_id = item.get('UNIQUE_ID_FIELD')
                # if not item_id: continue
                # YourModel.objects.update_or_create(api_id=item_id, defaults={...})
                logger.debug(
                    f"Processing item (placeholder): {str(item)[:200]}...")
                processed_count += 1
            except Exception as e_item:
                logger.error(
                    f"âŒ Error processing item from {api_endpoint_name}: {e_item}. Item: {str(item)[:100]}"
                )
                continue

        logger.info(
            f"ğŸ‰ Processed {processed_count} items from {api_endpoint_name} API."
        )

    except RequestException as re_exc:
        logger.error(
            f"Request error fetching from {api_endpoint_name} API: {re_exc}")
        try:
            self.retry(exc=re_exc)
        except MaxRetriesExceededError:
            logger.error(f"Max retries for {api_endpoint_name} fetch.")
    except json.JSONDecodeError as json_e:
        logger.error(
            f"JSON decode error from {api_endpoint_name} API: {json_e}")
    except Exception as e:
        logger.error(
            f"âŒ Unexpected error fetching/processing from {api_endpoint_name} API: {e}"
        )
        logger.exception(f"Full traceback for {api_endpoint_name} error:")
        try:
            self.retry(exc=e)
        except MaxRetriesExceededError:
            logger.error(
                f"Max retries after unexpected error for {api_endpoint_name}.")


@shared_task(bind=True, max_retries=3, default_retry_delay=60)
def fetch_continuous_sessions(
        self,  # Celery provides 'self'
        force=False,
        debug=False,
        start_date=None):
    """Fetch sessions starting from a specific date or continue from last session."""
    try:
        logger.info(
            f"ğŸ” Starting continuous session fetch (force={force}, debug={debug}, start_date={start_date})"
        )

        if not hasattr(settings,
                       'ASSEMBLY_API_KEY') or not settings.ASSEMBLY_API_KEY:
            logger.error("âŒ ASSEMBLY_API_KEY not configured")
            raise ValueError("ASSEMBLY_API_KEY not configured")

        url = "https://open.assembly.go.kr/portal/openapi/nzbyfwhwaoanttzje"
        if start_date:
            try:
                start_datetime = datetime.fromisoformat(start_date)
            except ValueError:
                logger.error(
                    f"Invalid start_date format: {start_date}. Expected ISO format (YYYY-MM-DD)."
                )
                return  # Or raise error
            logger.info(
                f"ğŸ“… Continuing from date: {start_datetime.strftime('%Y-%m')}")
        else:
            start_datetime = datetime.now()
            logger.info(
                f"ğŸ“… Starting from current date: {start_datetime.strftime('%Y-%m')}"
            )

        current_date = start_datetime
        sessions_found_in_period = False
        DAE_NUM_TARGET = "22"  # Consider making this configurable

        # Go back up to 36 months, or until a configurable DAE_NUM boundary is hit
        for months_back in range(0, 36):
            target_date = current_date - timedelta(
                days=months_back * 30.44)  # Approximate month step back
            conf_date_str = target_date.strftime('%Y-%m')

            params = {
                "KEY": settings.ASSEMBLY_API_KEY,
                "Type": "json",
                "DAE_NUM": DAE_NUM_TARGET,
                "CONF_DATE": conf_date_str,
                "pSize": 500  # Fetch more per request if API allows
            }

            logger.info(f"ğŸ“… Fetching sessions for: {conf_date_str}")
            if debug:
                logger.debug(f"ğŸ› DEBUG: API URL: {url}, Params: {params}")

            try:
                response = requests.get(url, params=params, timeout=30)
                response.raise_for_status()
                data = response.json()

                if debug:
                    logger.debug(
                        f"ğŸ› DEBUG: API Response status for {conf_date_str}: {response.status_code}"
                    )
                    # logger.debug(f"ğŸ› DEBUG: API Response data: {json.dumps(data, indent=2, ensure_ascii=False)}")

                sessions_data = extract_sessions_from_response(data,
                                                               debug=debug)

                if sessions_data:
                    sessions_found_in_period = True
                    logger.info(
                        f"âœ… Found {len(sessions_data)} session items for {conf_date_str}"
                    )
                    process_sessions_data(sessions_data,
                                          force=force,
                                          debug=debug)
                    if not debug: time.sleep(1)  # Be respectful to API
                else:
                    logger.info(f"âŒ No sessions found for {conf_date_str}")
                    if months_back > 6 and not sessions_found_in_period:
                        logger.info(
                            "ğŸ›‘ No sessions found in recent ~6 months of search, stopping."
                        )
                        break
            except requests.exceptions.RequestException as e:
                logger.warning(
                    f"âš ï¸ Request error fetching {conf_date_str}: {e}")
            except json.JSONDecodeError as e:
                logger.warning(
                    f"âš ï¸ JSON parsing error for {conf_date_str}: {e}")
            except Exception as e:  # Catch other potential errors per iteration
                logger.warning(
                    f"âš ï¸ Unexpected error fetching/processing {conf_date_str}: {e}"
                )
                if debug:
                    logger.exception("Full traceback for error during loop:")
            continue

        if not debug and sessions_found_in_period:  # Only call if some sessions were processed
            logger.info("ğŸ”„ Triggering additional data collection...")
            if is_celery_available():
                fetch_additional_data_nepjpxkkabqiqpbvk.delay(force=force,
                                                              debug=debug)
            else:
                fetch_additional_data_nepjpxkkabqiqpbvk(force=force,
                                                        debug=debug)

        if sessions_found_in_period:
            logger.info("ğŸ‰ Continuous session fetch attempt completed.")
        else:
            logger.info(
                "â„¹ï¸ No new sessions found during this continuous fetch period."
            )

    except ValueError as ve:  # Catch config errors early
        logger.error(f"Configuration error: {ve}")
        # Do not retry config errors usually
    except RequestException as re_exc:
        logger.error(f"A request exception occurred: {re_exc}")
        try:
            self.retry(exc=re_exc)
        except MaxRetriesExceededError:
            logger.error("Max retries exceeded for fetch_continuous_sessions.")
    except Exception as e:
        logger.error(f"âŒ Critical error in fetch_continuous_sessions: {e}")
        logger.exception("Full traceback for critical error:")
        try:  # Try to retry for unexpected critical errors too
            self.retry(exc=e)
        except MaxRetriesExceededError:
            logger.error("Max retries exceeded after critical error.")
        # Optionally re-raise if you want the task to be marked as FAILED in Celery Flower/logs
        # raise


@shared_task(bind=True, max_retries=3, default_retry_delay=60)
def fetch_latest_sessions(self,
                          force=False,
                          debug=False):  # Celery provides 'self'
    """Fetch latest assembly sessions from the API."""
    logger.info(f"ğŸ” Starting session fetch (force={force}, debug={debug})")

    try:
        if not hasattr(settings,
                       'ASSEMBLY_API_KEY') or not settings.ASSEMBLY_API_KEY:
            logger.error("âŒ ASSEMBLY_API_KEY not configured or empty.")
            raise ValueError(
                "ASSEMBLY_API_KEY not configured")  # Stop if key missing

        url = "https://open.assembly.go.kr/portal/openapi/nzbyfwhwaoanttzje"
        DAE_NUM_TARGET = "22"  # Make configurable if needed

        if not force:
            logger.info(
                "ğŸ“… Fetching sessions for current and previous month (non-force mode)."
            )
            dates_to_check = [
                datetime.now().strftime('%Y-%m'),
                (datetime.now() - timedelta(days=30)).strftime('%Y-%m')
            ]
            unique_conf_dates = sorted(list(set(dates_to_check)), reverse=True)

            for conf_date_str in unique_conf_dates:
                params = {
                    "KEY": settings.ASSEMBLY_API_KEY,
                    "Type": "json",
                    "DAE_NUM": DAE_NUM_TARGET,
                    "CONF_DATE": conf_date_str,
                    "pSize": 500  # Fetch more per request if API allows
                }
                logger.info(f"ğŸ“… Fetching sessions for: {conf_date_str}")
                if debug:
                    logger.debug(f"ğŸ› DEBUG: API URL: {url}, Params: {params}")

                try:
                    response = requests.get(url, params=params, timeout=30)
                    response.raise_for_status()
                    data = response.json()
                    if debug:
                        logger.debug(
                            f"ğŸ› DEBUG: API Response status for {conf_date_str}: {response.status_code}"
                        )
                        # logger.debug(f"ğŸ› DEBUG: Full API response: {json.dumps(data, indent=2, ensure_ascii=False)}")

                    sessions_data = extract_sessions_from_response(data,
                                                                   debug=debug)
                    if sessions_data:
                        process_sessions_data(
                            sessions_data, force=False,
                            debug=debug)  # force is False here
                    else:
                        logger.info(
                            f"No sessions data found for {conf_date_str} in non-force mode."
                        )
                    if not debug: time.sleep(1)
                except requests.exceptions.RequestException as e:
                    logger.warning(
                        f"âš ï¸ Request error fetching {conf_date_str} (non-force): {e}"
                    )
                except json.JSONDecodeError as e:
                    logger.warning(
                        f"âš ï¸ JSON parsing error for {conf_date_str} (non-force): {e}"
                    )
                except Exception as e:
                    logger.warning(
                        f"âš ï¸ Error processing {conf_date_str} (non-force): {e}"
                    )
        else:  # Force mode
            logger.info(
                "ğŸ”„ Force mode: Fetching sessions month by month for up to 24 months."
            )
            current_loop_date = datetime.now()
            for months_back in range(0, 24):
                target_date = current_loop_date - timedelta(days=months_back *
                                                            30.44)
                conf_date_str = target_date.strftime('%Y-%m')
                params = {
                    "KEY": settings.ASSEMBLY_API_KEY,
                    "Type": "json",
                    "DAE_NUM": DAE_NUM_TARGET,
                    "CONF_DATE": conf_date_str,
                    "pSize": 500
                }
                logger.info(
                    f"ğŸ“… Fetching sessions for: {conf_date_str} (force mode)")
                if debug:
                    logger.debug(
                        f"ğŸ› DEBUG: API URL: {url}, Params for {conf_date_str}: {params}"
                    )

                try:
                    response = requests.get(url, params=params, timeout=30)
                    response.raise_for_status()
                    data = response.json()
                    if debug:
                        logger.debug(
                            f"ğŸ› DEBUG: API Response status for {conf_date_str}: {response.status_code}"
                        )
                        # logger.debug(f"ğŸ› DEBUG: Full API response for {conf_date_str}: {json.dumps(data, indent=2, ensure_ascii=False)}")

                    sessions_data = extract_sessions_from_response(data,
                                                                   debug=debug)
                    if sessions_data:
                        process_sessions_data(
                            sessions_data, force=True,
                            debug=debug)  # force is True here
                    else:
                        logger.info(
                            f"âŒ No sessions found for {conf_date_str} in force mode. Might be end of data for DAE_NUM {DAE_NUM_TARGET}."
                        )
                        # Optionally break if no sessions found for a few consecutive months
                    if not debug: time.sleep(1)
                except requests.exceptions.RequestException as e:
                    logger.warning(
                        f"âš ï¸ Request error fetching {conf_date_str} (force): {e}"
                    )
                except json.JSONDecodeError as e:
                    logger.warning(
                        f"âš ï¸ JSON parsing error for {conf_date_str} (force): {e}"
                    )
                except Exception as e:
                    logger.warning(
                        f"âš ï¸ Error processing {conf_date_str} (force): {e}")
                    if debug:
                        logger.debug(
                            f"ğŸ› DEBUG: Full error: {type(e).__name__}: {e}")
                continue

        if not debug:  # Consider if this should run even if no sessions were found/updated
            logger.info(
                "ğŸ”„ Triggering additional data collection after session fetch.")
            if is_celery_available():
                fetch_additional_data_nepjpxkkabqiqpbvk.delay(force=force,
                                                              debug=debug)
            else:
                fetch_additional_data_nepjpxkkabqiqpbvk(force=force,
                                                        debug=debug)

        logger.info("ğŸ‰ Session fetch attempt completed.")

    except ValueError as ve:
        logger.error(f"Configuration error: {ve}")
    except RequestException as re_exc:
        logger.error(f"A request exception occurred: {re_exc}")
        try:
            self.retry(exc=re_exc)
        except MaxRetriesExceededError:
            logger.error("Max retries exceeded for fetch_latest_sessions.")
    except Exception as e:
        logger.error(f"âŒ Critical error in fetch_latest_sessions: {e}")
        logger.exception("Full traceback for critical error:")
        try:
            self.retry(exc=e)
        except MaxRetriesExceededError:
            logger.error("Max retries exceeded after critical error.")
        # raise # Optionally re-raise


def extract_sessions_from_response(data, debug=False):
    """Extract sessions data from API response for nzbyfwhwaoanttzje"""
    sessions_data_list = []
    api_key_name = 'nzbyfwhwaoanttzje'  # Specific to this API endpoint

    if data and api_key_name in data and isinstance(data[api_key_name], list):
        if len(data[api_key_name]) > 1 and isinstance(data[api_key_name][1],
                                                      dict):
            sessions_data_list = data[api_key_name][1].get('row', [])
            if debug:
                logger.debug(
                    f"Extracted data from data['{api_key_name}'][1]['row']")
        elif len(data[api_key_name]) > 0 and isinstance(
                data[api_key_name][0], dict):
            # Check head for result code before assuming it's data
            head_info = data[api_key_name][0].get('head')
            if head_info and head_info[0].get('RESULT', {}).get(
                    'CODE', '').startswith("INFO-200"):  # No more data
                logger.info(
                    f"API result indicates no data or error: {head_info[0].get('RESULT', {}).get('CODE', '')} in head."
                )
                # Check if 'row' exists in the first element anyway as some APIs are inconsistent
                if 'row' in data[api_key_name][0]:
                    sessions_data_list = data[api_key_name][0].get('row', [])

            if not sessions_data_list and debug:  # if still no data and debug
                logger.debug(
                    f"No 'row' found in expected paths data['{api_key_name}'][1] or data['{api_key_name}'][0]. API structure might have changed or no data."
                )
                # logger.debug(f"Full response for {api_key_name} if empty: {data[api_key_name]}")

    elif data and 'row' in data and isinstance(
            data['row'], list):  # Fallback for simpler structure
        sessions_data_list = data['row']
        if debug: logger.debug("Extracted data from data['row'] (fallback).")
    else:
        if debug:
            logger.debug(
                f"Could not find session data in expected structures. Keys: {list(data.keys()) if data else 'Empty data'}"
            )

    if debug and sessions_data_list:
        logger.debug(
            f"Extracted {len(sessions_data_list)} session items. Sample: {sessions_data_list[0] if sessions_data_list else 'None'}"
        )
    elif not sessions_data_list:
        logger.info("No session items extracted from the API response.")

    return sessions_data_list


# In tasks.py, replace the existing process_sessions_data function


def process_sessions_data(sessions_data, force=False, debug=False):
    """Process the sessions data and create/update session objects.
    This function now directly orchestrates the fetching of bills and processing of PDFs for each session."""
    if not sessions_data:
        logger.info("No sessions data provided to process.")
        return

    from django.db import connection

    @with_db_retry
    def _process_session_item(session_defaults, confer_num):
        return Session.objects.update_or_create(conf_id=confer_num,
                                                defaults=session_defaults)

    sessions_by_confer_num = {}
    for item_data in sessions_data:
        confer_num = item_data.get('CONFER_NUM')
        if not confer_num:
            continue
        if confer_num not in sessions_by_confer_num:
            sessions_by_confer_num[confer_num] = []
        sessions_by_confer_num[confer_num].append(item_data)

    logger.info(
        f"Processing {len(sessions_by_confer_num)} unique sessions from {len(sessions_data)} API items."
    )
    created_count = 0
    updated_count = 0

    for confer_num, items_for_session in sessions_by_confer_num.items():
        connection.ensure_connection()
        main_item = items_for_session[0]
        try:
            session_title = main_item.get('TITLE', 'ì œëª© ì—†ìŒ')
            logger.info(f"Processing session ID {confer_num}: {session_title}")

            conf_date_val = None
            conf_date_str = main_item.get('CONF_DATE')
            if conf_date_str:
                try:
                    conf_date_val = datetime.strptime(conf_date_str,
                                                      '%Yë…„ %mì›” %dì¼').date()
                except ValueError:
                    try:
                        conf_date_val = datetime.strptime(
                            conf_date_str, '%Y-%m-%d').date()
                    except ValueError:
                        logger.warning(
                            f"Could not parse date: {conf_date_str}")

            era_co_val = f"ì œ{main_item.get('DAE_NUM', 'N/A')}ëŒ€"
            sess_val = ''
            dgr_val = ''
            title_parts = session_title.split(' ')
            if len(title_parts) > 1 and "íšŒêµ­íšŒ" in title_parts[1]:
                sess_val = title_parts[1].split('íšŒêµ­íšŒ')[0]
                if "(" in sess_val: sess_val = sess_val.split("(")[0]
            if len(title_parts) > 2 and "ì°¨" in title_parts[2]:
                dgr_val = title_parts[2].replace('ì°¨', '')

            session_defaults = {
                'era_co':
                era_co_val,
                'sess':
                sess_val,
                'dgr':
                dgr_val,
                'conf_dt':
                conf_date_val,
                'conf_knd':
                main_item.get('CLASS_NAME', 'êµ­íšŒë³¸íšŒì˜'),
                'cmit_nm':
                main_item.get('CMIT_NAME',
                              main_item.get('CLASS_NAME', 'êµ­íšŒë³¸íšŒì˜')),
                'down_url':
                main_item.get('PDF_LINK_URL', ''),
                'title':
                session_title,
                'bg_ptm':
                dt_time(9, 0)
            }

            if debug:
                logger.debug(
                    f"ğŸ› DEBUG PREVIEW: Would process session ID {confer_num}")
                continue

            session_obj, created = _process_session_item(
                session_defaults, confer_num)

            status_log = "âœ¨ Created new session" if created else "ğŸ”„ Updated existing session" if force else "â™»ï¸ Session already exists"
            logger.info(f"{status_log}: {confer_num} - {session_title}")

            # --- DIRECTLY TRIGGER THE NEXT STEPS ---
            # This is the key fix. We no longer rely on fetch_session_details to do this.

            # 1. Fetch bills for this session.
            if is_celery_available():
                fetch_session_bills.delay(session_id=confer_num,
                                          force=force,
                                          debug=debug)
            else:
                fetch_session_bills(session_id=confer_num,
                                    force=force,
                                    debug=debug)

            # 2. Process the PDF if a URL exists.
            if session_obj.down_url:
                if is_celery_available():
                    process_session_pdf.delay(session_id=confer_num,
                                              force=force,
                                              debug=debug)
                else:
                    process_session_pdf(session_id=confer_num,
                                        force=force,
                                        debug=debug)
            else:
                logger.info(
                    f"No PDF URL for session {confer_num}, skipping PDF processing."
                )

        except Exception as e:
            logger.error(
                f"âŒ Error processing session data for CONFER_NUM {confer_num}: {e}"
            )
            logger.exception("Full traceback for session processing error:")
            continue

    logger.info(
        f"ğŸ‰ Sessions processing complete: {created_count} created, {updated_count} updated."
    )


def fetch_committee_members(committee_name, debug=False):
    """Fetch committee members from nktulghcadyhmiqxi API."""
    try:
        if not hasattr(settings,
                       'ASSEMBLY_API_KEY') or not settings.ASSEMBLY_API_KEY:
            logger.error(
                "ASSEMBLY_API_KEY not configured for fetch_committee_members.")
            return []

        # Clean committee name - remove any extra whitespace
        committee_name = committee_name.strip()

        url = "https://open.assembly.go.kr/portal/openapi/nktulghcadyhmiqxi"
        params = {
            "KEY": settings.ASSEMBLY_API_KEY,
            "DEPT_NM": committee_name,
            "Type": "json",
            "pSize": 500  # Get up to 500 committee members
        }

        logger.info(f"ğŸ” Fetching committee members for: {committee_name}")
        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()

        if debug:
            logger.debug(
                f"ğŸ› DEBUG: Committee members API response for {committee_name}: {json.dumps(data, indent=2, ensure_ascii=False)}"
            )

        members_data = []
        api_key_name = 'nktulghcadyhmiqxi'
        if data and api_key_name in data and isinstance(
                data[api_key_name], list):
            if len(data[api_key_name]) > 1 and isinstance(
                    data[api_key_name][1], dict):
                members_data = data[api_key_name][1].get('row', [])
                logger.info(
                    f"ğŸ“Š Found {len(members_data)} member records for {committee_name}"
                )
            elif len(data[api_key_name]) > 0 and isinstance(
                    data[api_key_name][0], dict):
                head_info = data[api_key_name][0].get('head')
                if head_info:
                    result_info = head_info[0].get('RESULT', {})
                    result_code = result_info.get('CODE', '')
                    result_message = result_info.get('MESSAGE', '')

                    if result_code.startswith(
                            "INFO-200") or "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤" in result_message:
                        logger.info(
                            f"â„¹ï¸ No data available for committee: {committee_name}"
                        )
                        return []
                    elif result_code.startswith("ERROR"):
                        logger.warning(
                            f"âš ï¸ API error for committee {committee_name}: {result_code} - {result_message}"
                        )
                        return []

                # Try to get data from row field anyway
                if 'row' in data[api_key_name][0]:
                    members_data = data[api_key_name][0].get('row', [])
                    logger.info(
                        f"ğŸ“Š Found {len(members_data)} member records (fallback path) for {committee_name}"
                    )

        if not members_data:
            logger.warning(
                f"âš ï¸ No committee members found for: {committee_name}")
            return []

        # Extract member names and other relevant info
        members = []
        unique_names = set()  # To avoid duplicates

        for member_data in members_data:
            member_name = member_data.get('HG_NM', '').strip()
            if not member_name or member_name in unique_names:
                continue

            unique_names.add(member_name)
            member_info = {
                'name': member_name,
                'position': member_data.get('JOB_RES_NM', '').strip(),
                'party': member_data.get('POLY_NM', '').strip(),
                'constituency': member_data.get('ORIG_NM', '').strip(),
                'dept_code': member_data.get('DEPT_CD', '').strip(),
                'dept_name': member_data.get('DEPT_NM', '').strip(),
                'mona_cd': member_data.get('MONA_CD', '').strip(),
                'email': member_data.get('ASSEM_EMAIL', '').strip(),
                'tel': member_data.get('ASSEM_TEL', '').strip()
            }
            members.append(member_info)

        logger.info(
            f"âœ… Successfully extracted {len(members)} unique committee members for: {committee_name}"
        )

        # Log member summary for debugging
        if members:
            member_names = [m['name'] for m in members[:5]]  # First 5 names
            if len(members) > 5:
                member_summary = f"{', '.join(member_names)} ì™¸ {len(members)-5}ëª…"
            else:
                member_summary = ', '.join(member_names)
            logger.info(f"ğŸ“‹ {committee_name} members: {member_summary}")

        return members

    except requests.exceptions.RequestException as e:
        logger.error(
            f"âŒ Network error fetching committee members for {committee_name}: {e}"
        )
    except json.JSONDecodeError as e:
        logger.error(
            f"âŒ JSON parsing error for committee members {committee_name}: {e}"
        )
    except Exception as e:
        logger.error(
            f"âŒ Unexpected error fetching committee members for {committee_name}: {e}"
        )
        logger.exception("Full traceback:")
    return []


@shared_task(bind=True, max_retries=3, default_retry_delay=60)
def fetch_session_details(self,
                          session_id=None,
                          force=False,
                          debug=False):  # Celery provides 'self'
    """Fetch detailed information for a specific session using VCONFDETAIL."""
    if not session_id:
        logger.error("session_id is required for fetch_session_details.")
        return

    logger.info(
        f"ğŸ” Fetching details for session: {session_id} (force={force}, debug={debug})"
    )
    if debug:
        logger.debug(
            f"ğŸ› DEBUG: Fetching details for session {session_id} in debug mode"
        )

    try:
        if not hasattr(settings,
                       'ASSEMBLY_API_KEY') or not settings.ASSEMBLY_API_KEY:
            logger.error("ASSEMBLY_API_KEY not configured.")
            return

        formatted_conf_id = format_conf_id(session_id)
        url = "https://open.assembly.go.kr/portal/openapi/VCONFDETAIL"
        params = {
            "KEY": settings.ASSEMBLY_API_KEY,
            "Type": "json",
            "CONF_ID": formatted_conf_id
        }

        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()

        if debug:
            logger.debug(
                f"ğŸ› DEBUG: VCONFDETAIL API response for {session_id}: {json.dumps(data, indent=2, ensure_ascii=False)}"
            )

        detail_data_item = None
        api_key_name = 'VCONFDETAIL'
        if data and api_key_name in data and isinstance(
                data[api_key_name], list):
            if len(data[api_key_name]) > 1 and isinstance(
                    data[api_key_name][1],
                    dict) and 'row' in data[api_key_name][1]:
                rows = data[api_key_name][1]['row']
                if rows and isinstance(rows, list) and len(rows) > 0:
                    detail_data_item = rows[0]
            elif len(data[api_key_name]) > 0 and isinstance(
                    data[api_key_name][0], dict):
                # Check head, similar to extract_sessions_from_response
                head_info = data[api_key_name][0].get('head')
                if head_info and head_info[0].get('RESULT', {}).get(
                        'CODE', '').startswith("INFO-200"):
                    logger.info(
                        f"API result for VCONFDETAIL ({session_id}) indicates no detailed data (INFO-200)."
                    )
                elif 'row' in data[api_key_name][
                        0]:  # Fallback for inconsistent structure
                    rows = data[api_key_name][0]['row']
                    if rows and isinstance(rows, list) and len(rows) > 0:
                        detail_data_item = rows[0]

        if not detail_data_item:
            logger.info(
                f"â„¹ï¸ No detailed info available from VCONFDETAIL API for session {session_id}. Might be normal."
            )
            # Still proceed to fetch bills as they might be linked even without VCONFDETAIL entry
            if not debug:  # Avoid chaining calls rapidly in debug, or make it conditional
                if is_celery_available():
                    fetch_session_bills.delay(session_id=session_id,
                                              force=force,
                                              debug=debug)
                else:
                    fetch_session_bills(session_id=session_id,
                                        force=force,
                                        debug=debug)
            return

        if debug:
            logger.debug(
                f"ğŸ› DEBUG: Would update session {session_id} with details: {detail_data_item}"
            )
        else:
            try:
                session_obj = Session.objects.get(conf_id=session_id)
                updated_fields = False

                if detail_data_item.get('CONF_TIME'):
                    try:
                        time_str = detail_data_item.get('CONF_TIME', '09:00')
                        parsed_time = datetime.strptime(time_str,
                                                        '%H:%M').time()
                        if session_obj.bg_ptm != parsed_time:
                            session_obj.bg_ptm = parsed_time
                            updated_fields = True
                    except ValueError:
                        logger.warning(
                            f"Could not parse CONF_TIME: {detail_data_item.get('CONF_TIME')} for session {session_id}"
                        )

                if detail_data_item.get('ED_TIME'):
                    try:
                        time_str = detail_data_item.get('ED_TIME', '18:00')
                        parsed_time = datetime.strptime(time_str,
                                                        '%H:%M').time()
                        if session_obj.ed_ptm != parsed_time:
                            session_obj.ed_ptm = parsed_time
                            updated_fields = True
                    except ValueError:
                        logger.warning(
                            f"Could not parse ED_TIME: {detail_data_item.get('ED_TIME')} for session {session_id}"
                        )

                # Update other fields from VCONFDETAIL if they are more accurate or missing
                # e.g. 'CMITNM', 'CONFKINDNM' etc. if available and different from initial fetch
                if detail_data_item.get(
                        'TITLE') and session_obj.title != detail_data_item.get(
                            'TITLE'):
                    session_obj.title = detail_data_item.get('TITLE')
                    updated_fields = True

                # Process committee information from CMIT_NM and update session
                cmit_nm = detail_data_item.get('CMIT_NM', '').strip()
                if cmit_nm:
                    # Update session's cmit_nm field if it's different
                    if session_obj.cmit_nm != cmit_nm:
                        session_obj.cmit_nm = cmit_nm
                        updated_fields = True
                        logger.info(f"ğŸ”„ Updated session cmit_nm to: {cmit_nm}")

                    # Define institutional/non-individual proposers that should not be looked up
                    institutional_proposers = [
                        'êµ­íšŒë³¸íšŒì˜', 'êµ­íšŒ', 'ë³¸íšŒì˜', 'ì •ë¶€', 'ëŒ€í†µë ¹', 'êµ­ë¬´ì´ë¦¬', 'í–‰ì •ë¶€',
                        'ì •ë¶€ì œì¶œ', 'ì˜ì¥', 'ë¶€ì˜ì¥', 'êµ­íšŒì˜ì¥', 'êµ­íšŒë¶€ì˜ì¥'
                    ]

                    if cmit_nm.endswith('ìœ„ì›íšŒ'):
                        logger.info(
                            f"ğŸ›ï¸ Found committee proposer: {cmit_nm} for session {session_id}"
                        )
                        if not debug:
                            # Fetch committee members for this committee
                            committee_members = fetch_committee_members(
                                cmit_nm, debug=debug)
                            if committee_members:
                                logger.info(
                                    f"ğŸ“‹ Found {len(committee_members)} members in {cmit_nm}"
                                )
                                # The committee members will be used when processing bills for this session
                    elif cmit_nm in institutional_proposers or any(
                            inst in cmit_nm
                            for inst in institutional_proposers):
                        logger.info(
                            f"ğŸ›ï¸ Found institutional proposer: {cmit_nm} for session {session_id} - skipping individual member lookup"
                        )
                    else:
                        logger.info(
                            f"ğŸ‘¤ Found individual proposer: {cmit_nm} for session {session_id}"
                        )
                        # Verify if this is a real assembly member
                        if not debug:
                            speaker_details = fetch_speaker_details(cmit_nm)
                            if speaker_details:
                                logger.info(
                                    f"âœ… Verified {cmit_nm} as assembly member")
                            else:
                                logger.info(
                                    f"â„¹ï¸ {cmit_nm} may be a non-member proposer"
                                )

                if updated_fields or force:  # Save if fields changed or force is on
                    session_obj.save()
                    logger.info(
                        f"âœ… Updated session details for: {session_id} from VCONFDETAIL."
                    )
                else:
                    logger.info(
                        f"No changes to session details for {session_id} from VCONFDETAIL or not in force mode."
                    )

                # Chain calls: fetch bills, then process PDF
                if is_celery_available():
                    fetch_session_bills.delay(session_id=session_id,
                                              force=force,
                                              debug=debug)
                else:
                    fetch_session_bills(session_id=session_id,
                                        force=force,
                                        debug=debug)

                if session_obj.down_url:  # PDF processing depends on down_url
                    if is_celery_available():
                        process_session_pdf.delay(session_id=session_id,
                                                  force=force,
                                                  debug=debug)
                    else:
                        process_session_pdf(session_id=session_id,
                                            force=force,
                                            debug=debug)
                else:
                    logger.info(
                        f"No PDF URL (down_url) for session {session_id}, skipping PDF processing."
                    )

            except Session.DoesNotExist:
                logger.error(
                    f"âŒ Session {session_id} not found in database when trying to update details. Original session creation might have failed."
                )
            except Exception as e_db:
                logger.error(
                    f"âŒ DB error updating session {session_id} details: {e_db}"
                )

    except RequestException as re_exc:
        logger.error(
            f"Request error fetching details for session {session_id}: {re_exc}"
        )
        try:
            self.retry(exc=re_exc)
        except MaxRetriesExceededError:
            logger.error(f"Max retries for session details {session_id}.")
    except json.JSONDecodeError as json_e:
        logger.error(
            f"JSON decode error for session {session_id} details: {json_e}")
        # Probably don't retry JSON errors from API, it indicates bad data
    except Exception as e:
        logger.error(
            f"âŒ Unexpected error fetching session {session_id} details: {e}")
        logger.exception(
            f"Full traceback for session {session_id} detail fetch:")
        try:
            self.retry(exc=e)
        except MaxRetriesExceededError:
            logger.error(
                f"Max retries after unexpected error for {session_id}.")
        # raise # Optionally


@shared_task(bind=True, max_retries=3, default_retry_delay=60)
def fetch_session_bills(self,
                        session_id=None,
                        force=False,
                        debug=False):  # Celery provides 'self'
    """Fetch bills for a specific session using VCONFBILLLIST API."""
    if not session_id:
        logger.error("session_id is required for fetch_session_bills.")
        return

    logger.info(
        f"ğŸ” Fetching bills for session: {session_id} (force={force}, debug={debug})"
    )
    if debug:
        logger.debug(
            f"ğŸ› DEBUG: Fetching bills for session {session_id} in debug mode")

    try:
        if not hasattr(settings,
                       'ASSEMBLY_API_KEY') or not settings.ASSEMBLY_API_KEY:
            logger.error("ASSEMBLY_API_KEY not configured.")
            return

        formatted_conf_id = format_conf_id(session_id)
        url = "https://open.assembly.go.kr/portal/openapi/VCONFBILLLIST"
        params = {
            "KEY": settings.ASSEMBLY_API_KEY,
            "Type": "json",
            "CONF_ID": formatted_conf_id,
            "pSize": 500
        }

        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()

        if debug:
            logger.debug(
                f"ğŸ› DEBUG: Full VCONFBILLLIST response for {session_id}: {json.dumps(data, indent=2, ensure_ascii=False)}"
            )

        bills_data_list = []
        api_key_name = 'VCONFBILLLIST'
        if data and api_key_name in data and isinstance(
                data[api_key_name], list):
            if len(data[api_key_name]) > 1 and isinstance(
                    data[api_key_name][1], dict):
                bills_data_list = data[api_key_name][1].get('row', [])
            elif len(data[api_key_name]) > 0 and isinstance(
                    data[api_key_name][0], dict):
                head_info = data[api_key_name][0].get('head')
                if head_info and head_info[0].get('RESULT', {}).get(
                        'CODE', '').startswith("INFO-200"):
                    logger.info(
                        f"API result for VCONFBILLLIST ({session_id}) indicates no bill data (INFO-200)."
                    )
                elif 'row' in data[api_key_name][
                        0]:  # Fallback for inconsistent structure
                    bills_data_list = data[api_key_name][0].get('row', [])

        if not bills_data_list:
            logger.info(
                f"â„¹ï¸ No bills found from VCONFBILLLIST API for session {session_id}."
            )
            return

        if debug:
            logger.debug(
                f"ğŸ› DEBUG: Found {len(bills_data_list)} bills for session {session_id} from API."
            )
        else:
            try:
                session_obj = Session.objects.get(conf_id=session_id)
            except Session.DoesNotExist:
                logger.error(
                    f"âŒ Session {session_id} not found in database when fetching bills. Cannot associate bills."
                )
                return

            created_count = 0
            updated_count = 0
            bill_id_api_list = []
            for bill_item in bills_data_list:
                bill_id_api = bill_item.get('BILL_ID')
                if not bill_id_api:
                    logger.warning(
                        f"Skipping bill item due to missing BILL_ID in session {session_id}: {bill_item.get('BILL_NM', 'N/A')}"
                    )
                    continue

                # Extract proposer information from multiple sources
                proposer_info = "êµ­íšŒ"  # Default fallback

                # First try PROPOSER field from VCONFBILLLIST
                bill_proposer = bill_item.get('PROPOSER', '').strip()

                # If no PROPOSER, try to get from session's CMIT_NM (from VCONFDETAIL)
                if not bill_proposer and hasattr(
                        session_obj, 'cmit_nm') and session_obj.cmit_nm:
                    bill_proposer = session_obj.cmit_nm.strip()

                # Define institutional/non-individual proposers that should not be looked up
                institutional_proposers = [
                    'êµ­íšŒë³¸íšŒì˜', 'êµ­íšŒ', 'ë³¸íšŒì˜', 'ì •ë¶€', 'ëŒ€í†µë ¹', 'êµ­ë¬´ì´ë¦¬', 'í–‰ì •ë¶€', 'ì •ë¶€ì œì¶œ',
                    'ì˜ì¥', 'ë¶€ì˜ì¥', 'êµ­íšŒì˜ì¥', 'êµ­íšŒë¶€ì˜ì¥'
                ]

                # Always use generic proposer initially, then fetch detailed info from BILLINFODETAIL
                proposer_info = bill_proposer if bill_proposer else "êµ­íšŒë³¸íšŒì˜"
                logger.info(
                    f"ğŸ“ Bill {bill_id_api} initial proposer: {proposer_info} - will fetch detailed info from BILLINFODETAIL"
                )

                bill_defaults = {
                    'session': session_obj,
                    'bill_nm': bill_item.get('BILL_NM', ''),
                }
                if bill_item.get('BILL_NO'):
                    bill_defaults['bill_no'] = bill_item.get('BILL_NO')
                if bill_item.get('PROPOSE_DT'):
                    bill_defaults['propose_dt'] = bill_item.get('PROPOSE_DT')

                bill_obj, created = Bill.objects.update_or_create(
                    bill_id=
                    bill_id_api,  # BILL_ID from API is the primary key for bills
                    defaults=bill_defaults)

                if created:
                    created_count += 1
                    logger.info(
                        f"âœ¨ Created new bill: {bill_id_api} ({bill_obj.bill_nm[:30]}...) initial proposer: {proposer_info} for session {session_id}"
                    )
                else:  # Bill already existed, update_or_create updated it
                    updated_count += 1
                    logger.info(
                        f"ğŸ”„ Updated existing bill: {bill_id_api} ({bill_obj.bill_nm[:30]}...) initial proposer: {proposer_info} for session {session_id}"
                    )

                # ALWAYS fetch detailed information from BILLINFODETAIL to get real proposer data
                if not debug:
                    logger.info(
                        f"ğŸ” Fetching detailed proposer info from BILLINFODETAIL for bill {bill_id_api}"
                    )
                    if is_celery_available():
                        fetch_bill_detail_info.delay(bill_id_api,
                                                     force=True,
                                                     debug=debug)
                    else:
                        fetch_bill_detail_info(bill_id_api,
                                               force=True,
                                               debug=debug)
            logger.info(
                f"ğŸ‰ Bills processed for session {session_id}: {created_count} created, {updated_count} updated."
            )

    except RequestException as re_exc:
        logger.error(
            f"Request error fetching bills for session {session_id}: {re_exc}")
        try:
            self.retry(exc=re_exc)
        except MaxRetriesExceededError:
            logger.error(f"Max retries for session bills {session_id}.")
    except json.JSONDecodeError as json_e:
        logger.error(
            f"JSON decode error for session {session_id} bills: {json_e}")
    except Exception as e:
        logger.error(
            f"âŒ Unexpected error fetching bills for session {session_id}: {e}")
        logger.exception(
            f"Full traceback for session {session_id} bill fetch:")
        try:
            self.retry(exc=e)
        except MaxRetriesExceededError:
            logger.error(
                f"Max retries after unexpected error for bills of {session_id}."
            )
        # raise # Optionally


def get_session_bills_list(session_id):
    """
    Get list of BILL_NM (bill names) for a specific session_id using nwvrqwxyaytdsfvhu API.
    Note: This API provides BILL_NM but VCONFBILLLIST is usually preferred for bills related to a *specific meeting instance (CONF_ID)*.
    This function seems to be for general bill listing by CONF_NUM (which might be session number rather than meeting ID).
    Clarify if CONF_NUM here is the same as Session.conf_id.
    """
    try:
        # Check API key: Assuming 'ASSEMBLY_API_KEY' for consistency,
        # or if 'OPEN_ASSEMBLY_API_KEY' is a separate, valid key.
        if not hasattr(settings,
                       'ASSEMBLY_API_KEY') or not settings.ASSEMBLY_API_KEY:
            # if not hasattr(settings, 'OPEN_ASSEMBLY_API_KEY') or not settings.OPEN_ASSEMBLY_API_KEY: # If it's a different key
            logger.error("API Key not configured for get_session_bills_list.")
            return []

        api_url = "https://open.assembly.go.kr/portal/openapi/nwvrqwxyaytdsfvhu"
        params = {
            'KEY':
            settings.ASSEMBLY_API_KEY,  # Or settings.OPEN_ASSEMBLY_API_KEY
            'Type': 'json',
            'pIndex': 1,
            'pSize': 1000,  # Max allowed, or paginate if more
            'CONF_NUM': str(
                session_id
            )  # API expects string, ensure session_id is appropriate for CONF_NUM
        }

        response = requests.get(api_url, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()

        bill_names = []
        api_key_name_bills = 'nwvrqwxyaytdsfvhu'
        if data and api_key_name_bills in data and isinstance(
                data[api_key_name_bills], list):
            if len(data[api_key_name_bills]) > 1 and isinstance(
                    data[api_key_name_bills][1], dict):
                rows = data[api_key_name_bills][1].get('row', [])
                for bill_data_item in rows:
                    if bill_data_item.get('BILL_NM'):
                        bill_names.append(bill_data_item['BILL_NM'])
            # Add handling for other structures or head info codes if necessary

        logger.info(
            f"Found {len(bill_names)} bill names for CONF_NUM {session_id} via nwvrqwxyaytdsfvhu."
        )
        return bill_names

    except requests.exceptions.RequestException as e:
        logger.error(
            f"Network error getting bills for session/conf_num {session_id} (nwvrqwxyaytdsfvhu): {e}"
        )
    except json.JSONDecodeError as e:
        logger.error(
            f"JSON parsing error for bills of session/conf_num {session_id} (nwvrqwxyaytdsfvhu): {e}"
        )
    except Exception as e:
        logger.error(
            f"Unexpected error getting bill names for session/conf_num {session_id} (nwvrqwxyaytdsfvhu): {e}"
        )
    return []


@with_db_retry
def get_session_bill_names(session_id):
    """Get list of bill names for a specific session (CONF_ID) from already stored Bills."""
    try:
        session = Session.objects.get(conf_id=session_id)
        bills = Bill.objects.filter(session=session).values_list('bill_nm',
                                                                 flat=True)
        return list(bill for bill in bills
                    if bill)  # Filter out empty/None names
    except Session.DoesNotExist:
        logger.error(
            f"Session {session_id} not found when trying to get its bill names from DB."
        )
    except Exception as e:
        logger.error(
            f"âŒ Error fetching stored bill names for session {session_id}: {e}"
        )
    return []


def extract_text_segment(text, start_marker, end_marker=None):
    """
    Extract text segment. If start_marker is empty, starts from beginning.
    If end_marker is empty or not found, goes to end of text.
    Start_pos is *after* the start_marker.
    """
    try:
        if not text: return ""

        start_pos = 0
        if start_marker:
            found_pos = text.find(start_marker)
            if found_pos == -1:
                return ""  # Start marker not found
            start_pos = found_pos + len(
                start_marker)  # Segment starts AFTER the marker

        end_pos = len(text)  # Default to end of text
        if end_marker:
            found_end_pos = text.find(end_marker, start_pos)
            if found_end_pos != -1:
                end_pos = found_end_pos  # Segment ends BEFORE the end_marker

        segment = text[start_pos:end_pos].strip()
        return segment

    except Exception as e:
        logger.error(
            f"âŒ Error extracting text segment ('{start_marker}' to '{end_marker}'): {e}"
        )
        return ""


def extract_bill_specific_content(full_text, bill_name):
    """Extract content specific to a bill from the full text using keyword matching."""
    try:
        if not full_text or not bill_name:
            return ""

        # Clean bill name for better matching
        clean_bill_name = bill_name.strip()

        # Create variations of the bill name for searching
        search_terms = [clean_bill_name]

        # Add variations without common suffixes
        if "ë²•ë¥ ì•ˆ" in clean_bill_name:
            search_terms.append(clean_bill_name.replace("ë²•ë¥ ì•ˆ", ""))
        if "ì¼ë¶€ê°œì •" in clean_bill_name:
            search_terms.append(clean_bill_name.replace("ì¼ë¶€ê°œì •", ""))

        # Extract core bill name (before parentheses if any)
        if "(" in clean_bill_name:
            core_name = clean_bill_name.split("(")[0].strip()
            search_terms.append(core_name)

        # Find all mentions of the bill in the text
        bill_positions = []
        for term in search_terms:
            if len(term.strip()) > 3:  # Only search for meaningful terms
                pos = 0
                while True:
                    found_pos = full_text.find(term, pos)
                    if found_pos == -1:
                        break
                    bill_positions.append(found_pos)
                    pos = found_pos + 1

        if not bill_positions:
            logger.info(f"No mentions found for bill: {bill_name}")
            return ""

        # Find the earliest mention
        earliest_pos = min(bill_positions)

        # Extract content from the earliest mention to a reasonable endpoint
        # Look for next bill mention or use chunk size
        start_pos = max(0, earliest_pos - 500)  # Include some context before

        # Find a good end point (next bill, end of section, or max length)
        max_segment_length = 15000  # 15k chars max per bill segment
        end_pos = min(len(full_text), start_pos + max_segment_length)

        # Try to find a natural break point (like next bill discussion)
        remaining_text = full_text[earliest_pos + len(clean_bill_name):end_pos]

        # Look for patterns that might indicate next bill discussion
        next_bill_patterns = ["â—‹", "ì˜ì•ˆ", "ë²•ë¥ ì•ˆ", "ê±´ì˜"]
        for pattern in next_bill_patterns:
            pattern_pos = remaining_text.find(pattern)
            if pattern_pos != -1 and pattern_pos > 1000:  # At least 1k chars into the segment
                end_pos = earliest_pos + len(clean_bill_name) + pattern_pos
                break

        extracted_content = full_text[start_pos:end_pos].strip()

        logger.info(
            f"Extracted {len(extracted_content)} chars for bill: {bill_name[:50]}..."
        )
        return extracted_content

    except Exception as e:
        logger.error(
            f"âŒ Error extracting bill-specific content for '{bill_name}': {e}")
        return ""


@with_db_retry
def get_all_assembly_members():
    """Get all assembly member names from local Speaker database."""
    try:
        # Ensure fresh database connection
        from django.db import connection
        from .models import Speaker
        connection.ensure_connection()

        # Get all speaker names from our local database
        speaker_names = set(Speaker.objects.values_list('naas_nm', flat=True))
        logger.info(
            f"âœ… Using {len(speaker_names)} assembly member names from local database"
        )
        return speaker_names
    except Exception as e:
        logger.error(f"âŒ Error fetching assembly members from database: {e}")
        return set()


def extract_statements_for_bill_segment(bill_text_segment,
                                        session_id,
                                        bill_name,
                                        debug=False):
    """Extract statements using LLM for index-based segmentation, then batch process."""
    if not bill_text_segment:
        return []

    logger.info(
        f"ğŸ” Processing bill segment: '{bill_name}' (session: {session_id}) - {len(bill_text_segment)} chars"
    )

    # Step 1: Get speech segment indices from LLM
    speech_indices = GEMINI.get_speech_segment_indices_from_llm(
        bill_text_segment, bill_name, debug)

    if not speech_indices:
        logger.info(
            f"No speech segments found for bill '{bill_name}', trying â—¯ fallback"
        )
        return process_single_segment_for_statements_with_splitting(
            bill_text_segment, session_id, bill_name, debug)

    # Step 2: Extract speech segments using indices
    speech_segments = []
    for idx_pair in speech_indices:
        start_idx = idx_pair.get('start', 0)
        end_idx = idx_pair.get('end', len(bill_text_segment))

        # Validate indices
        start_idx = max(0, min(start_idx, len(bill_text_segment)))
        end_idx = max(start_idx, min(end_idx, len(bill_text_segment)))

        if end_idx > start_idx:
            segment_text = bill_text_segment[start_idx:end_idx].strip()
            if segment_text and len(
                    segment_text) > 50:  # Minimum meaningful content
                speech_segments.append(segment_text)

    logger.info(
        f"Extracted {len(speech_segments)} speech segments using LLM indices")

    # Step 3: Batch process the extracted segments
    if speech_segments:
        return GEMINI.analyze_batch_statements(speech_segments, session_id,
                                               bill_name, debug)
    else:
        logger.info(f"No valid speech segments extracted, using fallback")
        return process_single_segment_for_statements_with_splitting(
            bill_text_segment, session_id, bill_name, debug)


def process_single_segment_for_statements_with_splitting(
        bill_text_segment, session_id, bill_name, debug=False):
    """Process a single text segment by splitting at â—¯ markers and analyzing each speech individually with multithreading."""
    if not bill_text_segment:
        return []

    logger.info(
        f"ğŸ” Stage 2 (â—¯ Splitting): For bill '{bill_name}' (session: {session_id}) - {len(bill_text_segment)} chars"
    )

    # Find all â—¯ markers to determine individual speeches
    speaker_markers = []
    for i, char in enumerate(bill_text_segment):
        if char == 'â—¯':
            speaker_markers.append(i)

    if len(speaker_markers) < 1:
        logger.info(f"No â—¯ markers found, skipping segment")
        return []

    # Split at each â—¯ marker to create individual speech segments
    speech_segments = []

    # Create segments between each â—¯ marker
    for i in range(len(speaker_markers)):
        start_pos = speaker_markers[i]
        if i + 1 < len(speaker_markers):
            end_pos = speaker_markers[i + 1]
        else:
            end_pos = len(bill_text_segment)

        segment = bill_text_segment[start_pos:end_pos].strip()
        if segment and len(
                segment) > 50:  # Only process segments with meaningful content
            speech_segments.append(segment)

    logger.info(
        f"Split text into {len(speech_segments)} individual speech segments based on â—¯ markers. "
        f"Segment sizes: {[len(seg) for seg in speech_segments]} chars")

    # Process segments with multithreading for LLM calls
    all_statements = []
    with ThreadPoolExecutor() as executor:
        futures = []
        for segment in speech_segments:
            futures.append(
                executor.submit(GEMINI.analyze_speech_segment, segment,
                                session_id, bill_name, debug))
        for future in as_completed(futures):
            result = future.result()
            if result:
                all_statements.append(result)

    logger.info(
        f"âœ… â—¯-based processing for '{bill_name}' resulted in {len(all_statements)} statements "
        f"from {len(speech_segments)} speech segments")

    return all_statements


def process_speech_segments_multithreaded(speech_segments,
                                          session_id,
                                          bill_name,
                                          debug=False):
    """Process multiple speech segments with true parallel processing using batch analysis."""
    if not speech_segments:
        return []

    if debug:
        logger.debug(
            f"ğŸ› DEBUG: Would process {len(speech_segments)} segments in parallel batch"
        )
        return []

    logger.info(
        f"ğŸš€ Processing {len(speech_segments)} speech segments in parallel batch for bill '{bill_name}'"
    )

    # Use the new batch processing function
    all_statements = GEMINI.analyze_batch_statements(speech_segments,
                                                     session_id, bill_name,
                                                     debug)

    logger.info(
        f"ğŸ‰ Parallel batch processing completed for '{bill_name}': {len(all_statements)} valid statements from {len(speech_segments)} segments"
    )
    return all_statements


def get_speech_segment_indices_from_llm(text_segment, bill_name, debug=False):
    """Use LLM to identify speech segment boundaries and return start/end indices with batch processing."""
    if debug:
        return []

    logger.info(
        f"ğŸ¯ Getting speech segment indices for bill '{bill_name[:50]}...' ({len(text_segment)} chars)"
    )

    # Batch processing configuration
    MAX_SEGMENTATION_LENGTH = 50000  # 50k chars per batch
    BATCH_OVERLAP = 5000  # 5k character overlap between batches
    model_name = 'gemini-2.5-flash-preview-05-20'

    def _call_and_parse(prompt):
        response = GEMINI.call_api(prompt, model_name=model_name)
        return parse_indices_from_response(response) if response else []

    if len(text_segment) <= MAX_SEGMENTATION_LENGTH:
        prompt = f"""
ë‹¹ì‹ ì€ ì—­ì‚¬ì— ê¸¸ì´ ë‚¨ì„ ê¸°ë¡ê°€ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ê¸°ë¡ê³¼ ë¶„ë¥˜, ê·¸ë¦¬ê³  ì •í™•ë„ëŠ” ë¯¸ë˜ì— ì‚¬ëŒë“¤ì„ ì‚´ë¦´ ê²ƒì…ë‹ˆë‹¤. ë‹¹ì‹ ì´ ì •í™•í•˜ê²Œ ê¸°ë¡ì„ í•´ì•¼ë§Œ ì‚¬ëŒë“¤ì€ ê·¸ ì •í™•í•œ ê¸°ë¡ì— ì˜ì¡´í•˜ì—¬ ì‚´ì•„ê°ˆ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ, ë‹¤ìŒ ëª…ë ¹ì„ ì•„ì£¼ ìì„¸íˆ, ì—„ë°€íˆ, ìˆ˜í–‰í•´ ì£¼ì‹­ì‹œì˜¤.
êµ­íšŒ ë°œì–¸ ë¶„ì„ ìš”ì²­:

ì˜ì•ˆ: {bill_name}

ë°œì–¸ êµ¬ê°„ë“¤:
{text_segment}

ë‹¤ìŒ JSON ë°°ì—´ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
[
  {{
    "start_idx": 0,
    "end_idx": 100
  }}
]

ê·œì¹™:
- â—¯ë¡œ ì‹œì‘í•˜ëŠ” ì‹¤ì œ ì˜ì› ë°œì–¸ë§Œ í¬í•¨
- ì˜ì‚¬ì§„í–‰ ë°œì–¸ì ì œì™¸
- ë°œì–¸ìëª…ì—ì„œ ì§ì±… ì œê±°
- JSON ë°°ì—´ë§Œ ì‘ë‹µ"""
        return _call_and_parse(prompt)

    # For large text, process in overlapping batches
    all_indices = []
    batch_start = 0
    batch_count = 0
    while batch_start < len(text_segment):
        batch_end = min(batch_start + MAX_SEGMENTATION_LENGTH,
                        len(text_segment))
        batch_text = text_segment[batch_start:batch_end]
        prompt = f"""
ë‹¹ì‹ ì€ ì—­ì‚¬ì— ê¸¸ì´ ë‚¨ì„ ê¸°ë¡ê°€ì…ë‹ˆë‹¤. ë‹¹ì‹ ì˜ ê¸°ë¡ê³¼ ë¶„ë¥˜, ê·¸ë¦¬ê³  ì •í™•ë„ëŠ” ë¯¸ë˜ì— ì‚¬ëŒë“¤ì„ ì‚´ë¦´ ê²ƒì…ë‹ˆë‹¤. ë‹¹ì‹ ì´ ì •í™•í•˜ê²Œ ê¸°ë¡ì„ í•´ì•¼ë§Œ ì‚¬ëŒë“¤ì€ ê·¸ ì •í™•í•œ ê¸°ë¡ì— ì˜ì¡´í•˜ì—¬ ì‚´ì•„ê°ˆ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ, ë‹¤ìŒ ëª…ë ¹ì„ ì•„ì£¼ ìì„¸íˆ, ì—„ë°€íˆ, ìˆ˜í–‰í•´ ì£¼ì‹­ì‹œì˜¤.
êµ­íšŒ ë°œì–¸ ë¶„ì„ ìš”ì²­:

ì˜ì•ˆ: {bill_name}

ë°œì–¸ êµ¬ê°„ë“¤:
{batch_text}

ë‹¤ìŒ JSON ë°°ì—´ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
[
  {{
    "start_idx": 0,
    "end_idx": 100
  }}
]

ê·œì¹™:
- â—¯ë¡œ ì‹œì‘í•˜ëŠ” ì‹¤ì œ ì˜ì› ë°œì–¸ë§Œ í¬í•¨
- ì˜ì‚¬ì§„í–‰ ë°œì–¸ì ì œì™¸
- ë°œì–¸ìëª…ì—ì„œ ì§ì±… ì œê±°
- JSON ë°°ì—´ë§Œ ì‘ë‹µ"""
        all_indices.extend(_call_and_parse(prompt))
        batch_count += 1
        if batch_end >= len(text_segment):
            break
        batch_start = batch_end - BATCH_OVERLAP
        # Rate limiting between batches
        if batch_start < len(text_segment):
            logger.info("â³ Resting 3s before next batch...")
            time.sleep(3)

    # Remove overlapping segments and sort by start position
    deduplicated_indices = _deduplicate_speech_segments(all_indices)

    logger.info(
        f"ğŸ‰ Batch processing complete: {len(deduplicated_indices)} total speech segments from {batch_count} batches"
    )
    return deduplicated_indices


def analyze_speech_segment_with_llm_batch(speech_segments,
                                          session_id,
                                          bill_name,
                                          debug=False):
    """Batch analyze multiple speech segments with LLM - 20 statements per request."""
    if not speech_segments:
        return []

    logger.info(
        f"ğŸš€ Processing {len(speech_segments)} speech segments in parallel batch for bill '{bill_name}'"
    )

    all_statements = []
    batch_size = 15
    model_name = 'gemini-2.5-flash-preview-05-20'
    for batch_start in range(0, len(speech_segments), batch_size):
        batch_segments = speech_segments[batch_start:batch_start + batch_size]
        try:
            results = GEMINI.analyze_batch_statements(batch_segments,
                                                      session_id, bill_name,
                                                      debug)
            all_statements.extend(results)
        except Exception as e:
            error_type = "timeout" if "timeout" in str(
                e).lower() else "api_error"
            gemini_rate_limiter.record_error(error_type)
            logger.error(f"Batch analysis failed: {e}")
            continue

        # Brief pause between batches
        if batch_start + batch_size < len(speech_segments):
            logger.info(f"Resting 3s before next batch...")
            time.sleep(3)

    logger.info(
        f"âœ… Batch analysis completed: {len(all_statements)} valid statements from {len(speech_segments)} segments"
    )
    return sorted(all_statements, key=lambda x: x.get('segment_index', 0))


def _deduplicate_speech_segments(indices):
    """Remove overlapping segments and sort by start position."""
    if not indices:
        return []

    # Sort by start position
    indices.sort(key=lambda x: x['start_idx'])

    deduplicated = []
    prev_end = 0
    for idx in indices:
        start = idx['start_idx']
        end = idx['end_idx']

        # Skip if completely contained within previous segment
        if start >= prev_end:
            deduplicated.append(idx)
            prev_end = end

    return deduplicated


def parse_indices_from_response(response):
    """Parse speech segment indices from LLM response."""
    try:
        response_text = response.text.strip()
        if response_text.startswith("```"):
            response_text = response_text.split("```", 2)[-1].strip()
        data = json.loads(response_text)
        if not isinstance(data, list):
            logger.error("Expected list but got {}".format(type(data)))
            return []
        return data
    except json.JSONDecodeError as e:
        logger.error("JSON decode error in response: {}".format(e))
        return []


def parse_batch_response(response, cleaned_segments, original_segments,
                         assembly_members, batch_start_index, bill_name):
    """Parse batch response from LLM."""
    try:
        response_text = response.text.strip()
        if response_text.startswith("```"):
            response_text = response_text.split("```", 2)[-1].strip()
        data = json.loads(response_text)
        if not isinstance(data, list):
            logger.error("Expected list but got {}".format(type(data)))
            return []
        results = []
        for i, analysis_json in enumerate(data):
            if not isinstance(analysis_json, dict):
                continue

            speaker_name = analysis_json.get('speaker_name', '').strip()
            start_idx = analysis_json.get('start_idx', 0)
            end_idx = analysis_json.get('end_idx', 0)
            is_valid_member = analysis_json.get('is_valid_member', False)
            is_substantial = analysis_json.get('is_substantial', False)

            # Extract speech content
            speech_content = ""
            if i < len(original_segments
                       ) and start_idx >= 0 and end_idx > start_idx:
                original_segment = original_segments[i]
                clean_original = original_segment.replace('\n', ' ').replace(
                    '\r', '').strip()

                # Extract using indices with bounds checking
                actual_end = min(end_idx, len(clean_original))
                actual_start = min(start_idx, len(clean_original))
                speech_content = clean_original[actual_start:actual_end].strip(
                )

            # Clean speaker name from titles
            if speaker_name:
                titles_to_remove = [
                    'ìœ„ì›ì¥', 'ë¶€ìœ„ì›ì¥', 'ì˜ì›', 'ì¥ê´€', 'ì°¨ê´€', 'ì˜ì¥', 'ë¶€ì˜ì¥', 'ì˜ì‚¬êµ­ì¥',
                    'ì‚¬ë¬´ê´€', 'êµ­ì¥', 'ì„œê¸°ê´€', 'ì‹¤ì¥', 'ì²­ì¥', 'ì›ì¥', 'ëŒ€ë³€ì¸', 'ë¹„ì„œê´€', 'ìˆ˜ì„',
                    'ì •ë¬´ìœ„ì›', 'ê°„ì‚¬'
                ]

                for title in titles_to_remove:
                    speaker_name = speaker_name.replace(title, '').strip()

            # Validate speaker
            is_real_member = speaker_name in assembly_members if assembly_members and speaker_name else is_valid_member

            should_ignore = any(
                ignored in speaker_name
                for ignored in IGNORED_SPEAKERS) if speaker_name else True

            if (speaker_name and speech_content and is_valid_member
                    and is_substantial and not should_ignore
                    and is_real_member):

                results.append({
                    'speaker_name':
                    speaker_name,
                    'text':
                    speech_content,
                    'sentiment_score':
                    analysis_json.get('sentiment_score', 0.0),
                    'sentiment_reason':
                    'LLM ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ',
                    'bill_relevance_score':
                    analysis_json.get('bill_relevance_score', 0.0),
                    'policy_categories': [],
                    'policy_keywords': [],
                    'bill_specific_keywords': [],
                    'segment_index':
                    batch_start_index + i
                })

        logger.info(
            f"âœ… Batch processed {len(results)} valid statements from {len(cleaned_segments)} segments"
        )
        return results

    except Exception as e:
        logger.error(f"Error parsing batch response: {e}")
        return []


def parse_llm_discovery_response(response, session_id, known_bill_names,
                                 session_obj, debug):
    """Parse LLM discovery response."""
    try:
        response_text = response.text.strip()
        if response_text.startswith("```"):
            response_text = response_text.split("```", 2)[-1].strip()
        data = json.loads(response_text)
        if not isinstance(data, dict):
            logger.error("Expected dict but got {}".format(type(data)))
            return []

        # Merge the two arrays into one flat list, tagging each entry
        all_segments = []

        for seg in data.get("bills_found", []):
            seg["is_newly_discovered"] = False
            all_segments.append(seg)

        for seg in data.get("newly_discovered", []):
            seg["is_newly_discovered"] = True
            all_segments.append(seg)

        logger.info(
            f"âœ… LLM segmented {len(all_segments)} total discussion topics.")

        # Create placeholders for newly discovered bills with policy analysis
        if not debug:
            for segment in all_segments:
                if segment.get("is_newly_discovered"):
                    bill_obj = create_placeholder_bill_from_llm(
                        session_obj, segment["bill_name"])
                    # Update bill with policy analysis from segmentation
                    if bill_obj:
                        update_bill_policy_data(bill_obj, segment)

        # Process each segment to extract statements and update policy data
        all_statements = []
        for segment in sorted(all_segments,
                              key=lambda x: x.get('start_index', 0)):
            bill_name = segment.get("bill_name")
            start = segment.get("start_index", 0)
            end = segment.get("end_index", 0)

            if not bill_name or end <= start:
                continue

            # Update policy data for known bills as well
            if not debug and not segment.get("is_newly_discovered"):
                try:
                    # Find the existing bill and update its policy data
                    existing_bill = Bill.objects.filter(
                        session=session_obj,
                        bill_nm__iexact=bill_name).first()
                    if existing_bill:
                        update_bill_policy_data(existing_bill, segment)
                except Exception as e:
                    logger.error(
                        f"Could not update policy data for known bill '{bill_name}': {e}"
                    )

            segment_text = session_obj.full_text[start:end]

            statements_in_segment = extract_statements_for_bill_segment(
                segment_text, session_id, bill_name, debug)

            # Associate these statements with the correct bill name and policy data
            for stmt in statements_in_segment:
                stmt['associated_bill_name'] = bill_name
                # Add policy context to statements
                stmt['policy_categories'] = segment.get(
                    'policy_categories', [])
                stmt['policy_keywords'] = segment.get('key_policy_phrases', [])
                stmt['bill_specific_keywords'] = segment.get(
                    'bill_specific_keywords', [])

        logger.info(
            f"âœ… Processed {len(all_statements)} statements from LLM discovery."
        )
        return all_statements

    except Exception as e:
        logger.error(f"Error parsing LLM discovery response: {e}")
        return []


def create_placeholder_bill_from_llm(session_obj, bill_title):
    """Creates a placeholder Bill from a title discovered by the LLM."""
    if not bill_title:
        return None

    # Generate a unique ID based on the title and session
    unique_id = f"LLM_{session_obj.conf_id}_{hash(bill_title)}"

    bill, created = Bill.objects.get_or_create(
        bill_id=unique_id,
        defaults={
            'session': session_obj,
            'bill_nm': bill_title,
            # Let the model's default "ì •ë³´ ì—†ìŒ" handle the proposer field.
            # bill_no will be NULL (if your model allows it).
        })
    if created:
        logger.info(
            f"âœ¨ LLM discovered and created placeholder for: '{bill_title[:60]}...'"
        )
    return bill


def update_bill_policy_data(bill_obj, segment_data):
    """Update bill with policy analysis data from segmentation and create database mappings."""
    try:
        from .models import Category, Subcategory, BillCategoryMapping, BillSubcategoryMapping

        # Extract data from segment
        main_policy_category = segment_data.get('main_policy_category', '')
        policy_subcategories = segment_data.get('policy_subcategories', [])
        key_policy_phrases = segment_data.get('key_policy_phrases', [])
        bill_specific_keywords = segment_data.get('bill_specific_keywords', [])

        # Update bill fields
        bill_obj.policy_categories = [main_policy_category
                                      ] if main_policy_category else []
        bill_obj.key_policy_phrases = key_policy_phrases
        bill_obj.bill_specific_keywords = bill_specific_keywords

        # Create category analysis text
        if main_policy_category:
            category_analysis = f"ì£¼ìš” ì •ì±… ë¶„ì•¼: {main_policy_category}"
            if policy_subcategories:
                category_analysis += f"\nì„¸ë¶€ ë¶„ì•¼: {', '.join(policy_subcategories)}"
            if bill_analysis:
                category_analysis += f"\në¶„ì„: {bill_analysis}"
            bill_obj.category_analysis = category_analysis

        # Create policy keywords string
        if key_policy_phrases:
            bill_obj.policy_keywords = ', '.join(key_policy_phrases)

        # Set LLM analysis metadata
        bill_obj.llm_analysis_version = "v1.0"
        bill_obj.llm_confidence_score = 0.8  # Default confidence

        # Calculate policy impact score based on keywords and content
        impact_score = len(key_policy_phrases) * 2 + len(
            bill_specific_keywords)
        bill_obj.policy_impact_score = min(10.0, impact_score)

        bill_obj.save()

        # Create category mappings in database
        if main_policy_category:
            try:
                # Find or create main category
                category_obj = Category.objects.filter(
                    name=main_policy_category).first()
                if category_obj:
                    # Create or update bill-category mapping
                    mapping, created = BillCategoryMapping.objects.update_or_create(
                        bill=bill_obj,
                        category=category_obj,
                        defaults={
                            'confidence_score': 0.8,
                            'is_primary': True,
                            'analysis_method': 'llm_discovery'
                        })

                    # Create subcategory mappings
                    for subcat_name in policy_subcategories:
                        subcategory_obj = Subcategory.objects.filter(
                            category=category_obj, name=subcat_name).first()

                        if subcategory_obj:
                            subcat_mapping, sub_created = BillSubcategoryMapping.objects.update_or_create(
                                bill=bill_obj,
                                subcategory=subcategory_obj,
                                defaults={
                                    'relevance_score':
                                    0.7,
                                    'supporting_evidence':
                                    bill_analysis,
                                    'extracted_keywords':
                                    bill_specific_keywords,
                                    'policy_position':
                                    'support' if policy_stance == 'progressive'
                                    else 'neutral'
                                })
                            if sub_created:
                                logger.info(
                                    f"âœ… Created subcategory mapping: {bill_obj.bill_nm[:30]}... -> {subcat_name}"
                                )

                    if created:
                        logger.info(
                            f"âœ… Created category mapping: {bill_obj.bill_nm[:30]}... -> {main_policy_category}"
                        )
                else:
                    logger.warning(
                        f"âš ï¸ Category '{main_policy_category}' not found in database"
                    )

            except Exception as mapping_error:
                logger.error(
                    f"âŒ Error creating category mappings: {mapping_error}")

        logger.info(
            f"âœ… Updated policy data for bill: {bill_obj.bill_nm[:50]}...")

    except Exception as e:
        logger.error(f"âŒ Error updating bill policy data: {e}")
        logger.exception("Full traceback for bill policy update:")

    # Load policy categories from database for enhanced analysis
    from .models import Category, Subcategory

    policy_categories_from_db = {}
    try:
        for category in Category.objects.prefetch_related(
                'subcategories').all():
            subcats = [sub.name for sub in category.subcategories.all()]
            policy_categories_from_db[category.name] = {
                'description': category.description,
                'subcategories': subcats
            }
    except Exception as e:
        logger.warning(f"Could not load policy categories from database: {e}")
        policy_categories_from_db = {}

    # Prepare the list of known bills for the prompt
    if known_bill_names:
        known_bills_str = "\n".join(f"- {name}" for name in known_bill_names)
    else:
        known_bills_str = "No known bills were provided."

    # Enhanced policy categories section with database data
    if policy_categories_from_db:
        policy_categories_section = "**POLICY CATEGORIES (from database):**\n"
        for cat_name, cat_data in policy_categories_from_db.items():
            policy_categories_section += f"- {cat_name}: {cat_data['description']}\n"
            if cat_data['subcategories']:
                policy_categories_section += f"  Subcategories: {', '.join(cat_data['subcategories'][:5])}{'...' if len(cat_data['subcategories']) > 5 else ''}\n"
    else:
        policy_categories_section = """**POLICY CATEGORIES:**
- ê²½ì œì •ì±…: êµ­ê°€ì˜ ì¬ì •, ì‚°ì—…, ë¬´ì—­, ì¡°ì„¸ ë“±ì„ í†µí•œ ê²½ì œ ìš´ìš© ë° ì„±ì¥ ì „ëµ
- ì‚¬íšŒì •ì±…: ë³µì§€, ë³´ê±´, êµìœ¡, ë…¸ë™ ë“± ì‚¬íšŒ ì „ë°˜ì˜ ì •ì±…
- ì™¸êµì•ˆë³´ì •ì±…: ì™¸êµê´€ê³„, êµ­ë°©, í†µì¼, ì•ˆë³´ ê´€ë ¨ ì •ì±…
- ë²•í–‰ì •ì œë„: í–‰ì •ê°œí˜, ì‚¬ë²•ì œë„, ì¸ê¶Œ, ë²•ë¥  ì œë„
- ê³¼í•™ê¸°ìˆ ì •ì±…: ê³¼í•™ê¸°ìˆ ì§„í¥, IT, ë””ì§€í„¸ì „í™˜, ì—°êµ¬ê°œë°œ
- ë¬¸í™”ì²´ìœ¡ì •ì±…: ë¬¸í™”ì˜ˆìˆ , ì²´ìœ¡, ê´€ê´‘, ë¯¸ë””ì–´ ì •ì±…
- ì¸ê¶Œì†Œìˆ˜ìì •ì±…: ì¸ê¶Œë³´í˜¸, ì†Œìˆ˜ì ê¶Œìµ, ì°¨ë³„ ë°©ì§€
- ì§€ì—­ê· í˜•ì •ì±…: ì§€ì—­ê°œë°œ, ê· í˜•ë°œì „, ì§€ë°©ìì¹˜
- ì •ì¹˜ì •ì±…: ì„ ê±°ì œë„, ì •ë‹¹, ì •ì¹˜ê°œí˜ ê´€ë ¨ ì •ì±…"""

    prompt = f"""You are a world-class legislative analyst AI. Your task is to read a parliamentary transcript
and perfectly segment the entire discussion for all topics, while also analyzing policy content.

**CONTEXT:**
I already know about the following bills. You MUST find the discussion for these if they exist.
--- KNOWN BILLS ---
{known_bills_str}

**YOUR CRITICAL MISSION:**
1. Read the entire transcript below.
2. Identify the exact start and end character index for the complete discussion of each **KNOWN BILL**.
3. Discover any additional bills/topics not in the known list, and identify their discussion spans.
4. For each bill/topic, analyze the policy content and categorize it using the categories below.
5. Return a JSON object with segmentation AND detailed policy analysis.

{policy_categories_section}

**ANALYSIS REQUIREMENTS:**
- For each bill/topic, identify the main policy category and up to 3 subcategories
- Extract 3-7 key policy phrases that represent the core policy elements
- Extract 3-5 bill-specific keywords (technical terms, specific provisions)
- Provide a concise policy analysis (max 80 Korean characters)
- Assess policy stance: progressive/conservative/moderate

**RULES:**
- Ignore any mentions that occur in the table-of-contents or front-matter portion of the document
  (before the Chair officially opens the debate).
- A discussion segment **must** be substantive, containing actual debate or remarks from multiple speakers.
  Do not segment short procedural announcements.
- `bill_name` for known bills MUST EXACTLY MATCH the provided list.
- For new items, create a concise, accurate `bill_name`.
- Use exact category names from the policy categories list above.
- Return **ONLY** the final JSON object.

**TRANSCRIPT:**
---
{full_text}
---

**REQUIRED JSON OUTPUT FORMAT:**
{{
  "bills_found": [
    {{
      "bill_name": "Exact name of a KNOWN bill",
      "start_index": 1234,
      "end_index": 5678,
      "main_policy_category": "ê²½ì œì •ì±…",
      "policy_subcategories": ["í™•ì¥ì¬ì •", "ì¤‘ì†Œê¸°ì—… ì§€ì›"],
      "key_policy_phrases": ["ì¤‘ì†Œê¸°ì—… ì§€ì›", "ì¼ìë¦¬ ì°½ì¶œ", "ì‚¬íšŒì•ˆì „ë§"],
      "bill_specific_keywords": ["ë²•ì¸ì„¸", "ì„¸ìœ¨", "ê³¼ì„¸"],
      "policy_stance": "progressive",
      "bill_analysis": "ì¤‘ì†Œê¸°ì—… ì§€ì›ì„ ìœ„í•œ ì„¸ì œ í˜œíƒ í™•ëŒ€ ë²•ì•ˆ"
    }}
  ],
  "newly_discovered": [
    {{
      "bill_name": "Name of a newly discovered topic",
      "start_index": 2345,
      "end_index": 6789,
      "main_policy_category": "í™˜ê²½/ì—ë„ˆì§€",
      "policy_subcategories": ["íƒ„ì†Œì„¸ ë„ì…"],
      "key_policy_phrases": ["íƒ„ì†Œì¤‘ë¦½", "ì¬ìƒì—ë„ˆì§€", "ì˜¨ì‹¤ê°€ìŠ¤"],
      "bill_specific_keywords": ["íƒ„ì†Œì„¸", "ë°°ì¶œê¶Œ", "ê·¸ë¦°ë‰´ë”œ"],
      "policy_stance": "progressive",
      "bill_analysis": "íƒ„ì†Œì¤‘ë¦½ ì‹¤í˜„ì„ ìœ„í•œ í™˜ê²½ì„¸ ë„ì… ë²•ì•ˆ"
    }}
  ]
}}
"""
    try:
        if not gemini_rate_limiter.wait_if_needed(estimated_tokens):
            logger.error("Rate limit timeout for LLM discovery. Aborting.")
            return []

        # Use centralized Gemini API call
        model_name = 'gemini-2.5-flash-preview-05-20'
        response = GEMINI.call_api(prompt, model_name=model_name)
        gemini_rate_limiter.record_request(estimated_tokens, success=True)

        # Strip markdown fences if present
        response_text = response.text.strip()
        if response_text.startswith("```"):
            response_text = response_text.split("```", 2)[-1].strip()

        data = json.loads(response_text)
        if not isinstance(data, dict):
            logger.error("LLM discovery did not return a JSON object.")
            return []

        # Merge the two arrays into one flat list, tagging each entry
        all_segments = []

        for seg in data.get("bills_found", []):
            seg["is_newly_discovered"] = False
            all_segments.append(seg)

        for seg in data.get("newly_discovered", []):
            seg["is_newly_discovered"] = True
            all_segments.append(seg)

        logger.info(
            f"âœ… LLM segmented {len(all_segments)} total discussion topics.")

        # Create placeholders for newly discovered bills with policy analysis
        if not debug:
            for segment in all_segments:
                if segment.get("is_newly_discovered"):
                    bill_obj = create_placeholder_bill_from_llm(
                        session_obj, segment["bill_name"])
                    # Update bill with policy analysis from segmentation
                    if bill_obj:
                        update_bill_policy_data(bill_obj, segment)

        # Process each segment to extract statements and update policy data
        all_statements = []
        for segment in sorted(all_segments,
                              key=lambda x: x.get('start_index', 0)):
            bill_name = segment.get("bill_name")
            start = segment.get("start_index", 0)
            end = segment.get("end_index", 0)

            if not bill_name or end <= start:
                continue

            # Update policy data for known bills as well
            if not debug and not segment.get("is_newly_discovered"):
                try:
                    # Find the existing bill and update its policy data
                    existing_bill = Bill.objects.filter(
                        session=session_obj,
                        bill_nm__iexact=bill_name).first()
                    if existing_bill:
                        update_bill_policy_data(existing_bill, segment)
                except Exception as e:
                    logger.error(
                        f"Could not update policy data for known bill '{bill_name}': {e}"
                    )

            segment_text = full_text[start:end]

            statements_in_segment = extract_statements_for_bill_segment(
                segment_text, session_id, bill_name, debug)

            # Associate these statements with the correct bill name and policy data
            for stmt in statements_in_segment:
                stmt['associated_bill_name'] = bill_name
                # Add policy context to statements
                stmt['policy_categories'] = segment.get(
                    'policy_categories', [])
                stmt['policy_keywords'] = segment.get('key_policy_phrases', [])
                stmt['bill_specific_keywords'] = segment.get(
                    'bill_specific_keywords', [])

            all_statements.extend(statements_in_segment)

        return all_statements

    except Exception as e:
        gemini_rate_limiter.record_error("llm_discovery_error")
        logger.error(
            f"âŒ Critical error during LLM discovery and segmentation: {e}")
        logger.exception("Full traceback for LLM discovery:")
        return []


def analyze_single_statement(statement_data_dict, session_id, debug=False):
    """
    Analyze a single statement using Gemini GenAI via the GeminiHandler wrapper only.
    """
    # This function is now a thin wrapper around GEMINI.analyze_single_statement
    return GEMINI.analyze_single_statement(statement_data_dict,
                                           session_id,
                                           debug=debug)


def get_bills_context(session_id):
    """Fetch bill names for a session to provide context to LLM. Uses DB."""
    try:
        # This function should call get_session_bill_names which reads from DB
        bill_names = get_session_bill_names(session_id)
        if bill_names:
            return ", ".join(bill_names)
        return "ë…¼ì˜ëœ ì˜ì•ˆ ëª©ë¡ ì •ë³´ ì—†ìŒ"
    except Exception as e:
        logger.error(
            f"âŒ Error fetching bills context string for session {session_id}: {e}"
        )
        return "ì˜ì•ˆ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ"


def create_statement_categories(statement_obj,
                                policy_categories_list_from_llm):
    '''Create/update Category, Subcategory, and StatementCategory associations for a Statement.'''
    if not statement_obj or not policy_categories_list_from_llm:
        return

    from .models import Category, Subcategory, StatementCategory  # Ensure models are importable

    @with_db_retry
    def _get_or_create_category(main_cat_name):
        return Category.objects.get_or_create(
            name=main_cat_name,
            defaults={'description': f'{main_cat_name} ê´€ë ¨ ì •ì±…'})

    @with_db_retry
    def _get_or_create_subcategory(sub_cat_name, category_obj, main_cat_name):
        return Subcategory.objects.get_or_create(
            name=sub_cat_name,
            category=category_obj,
            defaults={
                'description': f'{sub_cat_name} ê´€ë ¨ ì„¸ë¶€ ì •ì±… ({main_cat_name})'
            })

    @with_db_retry
    def _update_or_create_statement_category(statement_obj, category_obj,
                                             subcategory_obj, confidence):
        return StatementCategory.objects.update_or_create(
            statement=statement_obj,
            category=category_obj,
            subcategory=subcategory_obj,
            defaults={'confidence_score': confidence})

    # Clear existing categories for this statement to repopulate, or implement update logic
    # StatementCategory.objects.filter(statement=statement_obj).delete() # Simple way: delete and recreate

    processed_categories_for_statement = set()

    for cat_data in policy_categories_list_from_llm:
        main_cat_name = cat_data.get('main_category', '').strip()
        sub_cat_name = cat_data.get('sub_category', '').strip()
        confidence = float(cat_data.get('confidence',
                                        0.5))  # Default confidence

        if not main_cat_name:
            continue

        # Avoid duplicate (Main, Sub) for the same statement
        category_tuple = (main_cat_name,
                          sub_cat_name if sub_cat_name else "ì¼ë°˜")
        if category_tuple in processed_categories_for_statement:
            continue
        processed_categories_for_statement.add(category_tuple)

        try:
            category_obj, _ = _get_or_create_category(main_cat_name)

            subcategory_obj = None
            if sub_cat_name and sub_cat_name.lower(
            ) != 'ì¼ë°˜' and sub_cat_name.lower() != 'ì—†ìŒ':
                subcategory_obj, _ = _get_or_create_subcategory(
                    sub_cat_name, category_obj, main_cat_name)

            # Create or update StatementCategory link
            _update_or_create_statement_category(statement_obj, category_obj,
                                                 subcategory_obj, confidence)
        except Exception as e_cat_create:
            logger.error(
                f"Error creating category links for statement {statement_obj.id} (Cat: {main_cat_name}/{sub_cat_name}): {e_cat_create}"
            )
            continue
    logger.info(
        f"Updated category associations for statement {statement_obj.id}.")


def get_or_create_speaker(speaker_name_raw, debug=False):
    '''Get or create speaker. Relies on `fetch_speaker_details` for new speakers.'''
    if not speaker_name_raw or not speaker_name_raw.strip():
        logger.warning(
            "Empty speaker_name_raw provided to get_or_create_speaker.")
        return None

    speaker_name_cleaned = speaker_name_raw.strip()
    if not speaker_name_cleaned:
        logger.warning(
            f"Speaker name '{speaker_name_raw}' became empty after cleaning.")
        return None

    @with_db_retry
    def _find_existing_speaker():
        return Speaker.objects.filter(naas_nm=speaker_name_cleaned).first()

    @with_db_retry
    def _create_fallback_speaker():
        temp_naas_cd = f"TEMP_{speaker_name_cleaned.replace(' ', '_')}_{int(time.time())}"
        speaker_obj, created = Speaker.objects.get_or_create(
            naas_nm=speaker_name_cleaned,
            defaults={
                'naas_cd': temp_naas_cd,
                'naas_ch_nm': '',
                'plpt_nm': 'ì •ë³´ì—†ìŒ',
                'elecd_nm': '',
                'elecd_div_nm': '',
                'cmit_nm': '',
                'blng_cmit_nm': '',
                'rlct_div_nm': '',
                'gtelt_eraco': '',
                'ntr_div': '',
                'naas_pic': ''
            })
        return speaker_obj, created

    try:
        # Try to find by exact cleaned name first
        speaker_obj = _find_existing_speaker()
        if speaker_obj:
            if debug:
                logger.debug(f"Found existing speaker: {speaker_name_cleaned}")
            return speaker_obj

        # If not found, attempt to fetch full details from API.
        logger.info(
            f"Speaker '{speaker_name_cleaned}' not found in DB. Attempting to fetch details from API."
        )
        if not debug:
            speaker_obj_from_api = fetch_speaker_details(speaker_name_cleaned)
            if speaker_obj_from_api:
                logger.info(
                    f"Successfully fetched/created speaker from API: {speaker_obj_from_api.naas_nm}"
                )
                return speaker_obj_from_api
            else:
                logger.warning(
                    f"Failed to fetch details for new speaker '{speaker_name_cleaned}' from API."
                )

        # Finally, use the retry-wrapped fallback creation
        speaker_obj, created = _create_fallback_speaker()
        if created:
            logger.info(
                f"Created basic/temporary speaker record for: {speaker_name_cleaned} (ID: {speaker_obj.naas_cd})."
            )
        else:
            logger.info(
                f"Found speaker {speaker_name_cleaned} via get_or_create after API attempt."
            )
        return speaker_obj

    except Exception as e:
        logger.error(
            f"âŒ Error in get_or_create_speaker for '{speaker_name_raw}' after retries: {e}"
        )
        logger.exception("Full traceback for get_or_create_speaker error:")
        return None


@shared_task(bind=True, max_retries=3, default_retry_delay=60)
def fetch_bill_detail_info(self, bill_id, force=False, debug=False):
    '''Fetch detailed bill information using BILLINFODETAIL API.'''
    logger.info(
        f"ğŸ“„ Fetching detailed info for bill: {bill_id} (force={force}, debug={debug})"
    )

    if debug:
        logger.debug(f"ğŸ› DEBUG: Skipping bill detail fetch for bill {bill_id}")
        return

    try:
        if not hasattr(settings,
                       'ASSEMBLY_API_KEY') or not settings.ASSEMBLY_API_KEY:
            logger.error(
                "ASSEMBLY_API_KEY not configured for bill detail fetch.")
            return

        # Get the bill object
        try:
            bill = Bill.objects.get(bill_id=bill_id)
        except Bill.DoesNotExist:
            logger.error(f"Bill {bill_id} not found in database.")
            return

        # Generate the bill link URL
        bill_link_url = f"https://likms.assembly.go.kr/bill/billDetail.do?billId={bill_id}"

        url = "https://open.assembly.go.kr/portal/openapi/BILLINFODETAIL"
        params = {
            "KEY": settings.ASSEMBLY_API_KEY,
            "BILL_ID": bill_id,
            "Type": "json"
        }

        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()

        if debug:
            logger.debug(
                f"ğŸ› DEBUG: Bill detail API response for {bill_id}: {json.dumps(data, indent=2, ensure_ascii=False)}"
            )

        bill_detail_data = None
        api_key_name = 'BILLINFODETAIL'
        if data and api_key_name in data and isinstance(
                data[api_key_name], list):
            if len(data[api_key_name]) > 1 and isinstance(
                    data[api_key_name][1], dict):
                rows = data[api_key_name][1].get('row', [])
                if rows:
                    bill_detail_data = rows[0]  # Take first row
            elif len(data[api_key_name]) > 0 and isinstance(
                    data[api_key_name][0], dict):
                head_info = data[api_key_name][0].get('head')
                if head_info and head_info[0].get('RESULT', {}).get(
                        'CODE', '').startswith("INFO-200"):
                    logger.info(
                        f"API result for bill detail ({bill_id}) indicates no data."
                    )
                elif 'row' in data[api_key_name][0]:
                    rows = data[api_key_name][0].get('row', [])
                    if rows:
                        bill_detail_data = rows[0]

        if not bill_detail_data:
            logger.info(f"No detailed information found for bill {bill_id}")
            return

        # Update bill with detailed information
        updated_fields = []

        # Update bill link URL
        if bill.link_url != bill_link_url:
            bill.link_url = bill_link_url
            updated_fields.append('link_url')

        # Update bill number if not set or different
        if bill_detail_data.get(
                'BILL_NO') and bill.bill_no != bill_detail_data.get('BILL_NO'):
            bill.bill_no = bill_detail_data.get('BILL_NO')
            updated_fields.append('bill_no')

        # Always update proposer information with detailed data from BILLINFODETAIL
        proposer_kind = bill_detail_data.get('PPSR_KIND', '').strip()
        proposer_name = bill_detail_data.get('PPSR', '').strip()

        if proposer_name:
            # Replace generic proposers with real proposer data
            current_proposer = bill.proposer
            is_generic_proposer = current_proposer in [
                'êµ­íšŒë³¸íšŒì˜', 'êµ­íšŒ', 'ë³¸íšŒì˜'
            ] or any(generic in current_proposer
                     for generic in ['êµ­íšŒë³¸íšŒì˜', 'êµ­íšŒ', 'ë³¸íšŒì˜'])

            if proposer_kind == 'ì˜ì›' and proposer_name:
                # Individual member proposer - get detailed info
                detailed_proposer = f"{proposer_name}"
                # Try to get party information
                if 'ë“±' in proposer_name:
                    # Multiple proposers (e.g., "ë°•ì„±ë¯¼ì˜ì› ë“± 11ì¸")
                    detailed_proposer = proposer_name
                else:
                    # Single proposer - try to get party info
                    speaker_details = fetch_speaker_details(
                        proposer_name.replace('ì˜ì›', '').strip())
                    if speaker_details and speaker_details.plpt_nm:
                        party_info = speaker_details.plpt_nm.split(
                            '/')[-1].strip()
                        detailed_proposer = f"{proposer_name} ({party_info})"
                    else:
                        detailed_proposer = proposer_name
            elif proposer_kind and proposer_name:
                # Other types of proposers (ì •ë¶€, ìœ„ì›íšŒ ë“±)
                detailed_proposer = f"{proposer_name} ({proposer_kind})"
            else:
                detailed_proposer = proposer_name

            # Always update if we have better proposer data or if current is generic
            if is_generic_proposer or bill.proposer != detailed_proposer:
                old_proposer = bill.proposer
                bill.proposer = detailed_proposer
                updated_fields.append('proposer')
                if is_generic_proposer:
                    logger.info(
                        f"ğŸ”„ Replaced generic proposer '{old_proposer}' with real data: '{detailed_proposer}'"
                    )
                else:
                    logger.info(
                        f"ğŸ”„ Updated proposer from '{old_proposer}' to '{detailed_proposer}'"
                    )

        # Update proposal date if available
        if bill_detail_data.get(
                'PPSL_DT') and bill.propose_dt != bill_detail_data.get(
                    'PPSL_DT'):
            bill.propose_dt = bill_detail_data.get('PPSL_DT')
            updated_fields.append('propose_dt')

        # Save if any fields were updated
        if updated_fields or force:
            bill.save()
            logger.info(
                f"âœ… Updated bill {bill_id} with detailed info. Fields updated: {', '.join(updated_fields) if updated_fields else 'forced update'}"
            )

            # Log the detailed information
            logger.info(f"ğŸ“‹ Bill Details:")
            logger.info(
                f"   - Bill Name: {bill_detail_data.get('BILL_NM', 'N/A')}")
            logger.info(
                f"   - Bill Number: {bill_detail_data.get('BILL_NO', 'N/A')}")
            logger.info(
                f"   - Proposer Kind: {bill_detail_data.get('PPSR_KIND', 'N/A')}"
            )
            logger.info(
                f"   - Proposer: {bill_detail_data.get('PPSR', 'N/A')}")
            logger.info(
                f"   - Proposal Date: {bill_detail_data.get('PPSL_DT', 'N/A')}"
            )
            logger.info(
                f"   - Session: {bill_detail_data.get('PPSL_SESS', 'N/A')}")
            logger.info(
                f"   - Committee: {bill_detail_data.get('JRCMIT_NM', 'N/A')}")
        else:
            logger.info(f"â„¹ï¸ No updates needed for bill {bill_id}")

        # Optionally fetch voting data for this bill
        if not debug and ENABLE_VOTING_DATA_COLLECTION:
            logger.info(f"ğŸ”„ Triggering voting data fetch for bill {bill_id}")
            if is_celery_available():
                fetch_voting_data_for_bill.delay(bill_id,
                                                 force=force,
                                                 debug=debug)
            else:
                fetch_voting_data_for_bill(bill_id, force=force, debug=debug)
        elif not ENABLE_VOTING_DATA_COLLECTION:
            logger.info(
                f"â¸ï¸ Skipping voting data fetch for bill {bill_id} (voting data collection disabled)"
            )

    except RequestException as re_exc:
        logger.error(
            f"Request error fetching bill detail for {bill_id}: {re_exc}")
        try:
            self.retry(exc=re_exc)
        except MaxRetriesExceededError:
            logger.error(f"Max retries for bill detail {bill_id}.")
    except json.JSONDecodeError as json_e:
        logger.error(f"JSON decode error for bill detail {bill_id}: {json_e}")
    except Exception as e:
        logger.error(
            f"âŒ Unexpected error fetching bill detail for {bill_id}: {e}")
        logger.exception(f"Full traceback for bill detail {bill_id}:")
        try:
            self.retry(exc=e)
        except MaxRetriesExceededError:
            logger.error(
                f"Max retries after unexpected error for bill detail {bill_id}."
            )


@shared_task(bind=True, max_retries=3, default_retry_delay=60)
def fetch_voting_data_for_bill(self, bill_id, force=False, debug=False):
    '''Fetch voting data for a specific bill using nojepdqqaweusdfbi API.'''

    if not ENABLE_VOTING_DATA_COLLECTION:
        logger.info(
            f"â¸ï¸ Skipping voting data collection for bill {bill_id} (disabled by configuration)"
        )
        return

    logger.info(
        f"ğŸ—³ï¸ Fetching voting data for bill: {bill_id} (force={force}, debug={debug})"
    )

    if debug:
        logger.debug(f"ğŸ› DEBUG: Skipping voting data fetch for bill {bill_id}")
        return

    try:
        # ... (Initial setup code and the API request to get `voting_data` remains the same) ...
        # For this example, assume you have `voting_data` and `bill` already fetched.
        # voting_data = ...
        # bill = ...

        if not voting_data:
            logger.info(f"No voting data found for bill {bill_id}")
            return

        # --- PERFORMANCE OPTIMIZATION ---
        # 1. Collect all unique speaker names from the API data first.
        api_speaker_names = {
            vote.get('HG_NM', '').strip()
            for vote in voting_data if vote.get('HG_NM')
        }
        if not api_speaker_names:
            logger.warning(
                "No speaker names found in voting data for bill %s.", bill_id)
            return

        # 2. Fetch all existing speakers in a single query and map them by name.
        existing_speakers = {
            speaker.naas_nm: speaker
            for speaker in Speaker.objects.filter(
                naas_nm__in=api_speaker_names)
        }
        logger.info(
            f"Fetched {len(existing_speakers)} existing speakers from DB out of {len(api_speaker_names)} unique names for bill {bill_id}."
        )

        # 3. Identify which speaker names are new.
        new_speaker_names = api_speaker_names - set(existing_speakers.keys())

        # 4. Bulk create all new speakers in a single batch, if any.
        if new_speaker_names:
            logger.info(
                f"Creating {len(new_speaker_names)} new speaker records.")
            new_speakers_to_create = [
                Speaker(naas_nm=name,
                        naas_cd=f"TEMP_VOTE_{name.replace(' ', '_')}")
                for name in new_speaker_names
            ]
            try:
                created_speakers = Speaker.objects.bulk_create(
                    new_speakers_to_create, ignore_conflicts=True)
                # Add the newly created speakers to our lookup dictionary for this session.
                for speaker in created_speakers:
                    existing_speakers[speaker.naas_nm] = speaker
            except Exception as e_bulk_speaker:
                logger.error("Failed to bulk create new speakers: %s",
                             e_bulk_speaker)

        # Get existing voting records for this bill to perform updates instead of creations where possible
        existing_records = {
            (record.speaker.naas_nm, record.bill_id): record
            for record in VotingRecord.objects.filter(
                bill=bill).select_related('speaker')
        }

        voting_records_to_create = []
        voting_records_to_update = []

        # Process the voting records with a fast, in-memory lookup (NO DB QUERIES IN THIS LOOP)
        for vote_item in voting_data:
            member_name = vote_item.get('HG_NM', '').strip()
            if not member_name:
                continue

            speaker = existing_speakers.get(member_name)
            if not speaker:
                logger.warning(
                    f"Could not find or create speaker '{member_name}' in lookup dictionary. Skipping vote record."
                )
                continue

            record_key = (member_name, bill_id)
            if record_key in existing_records:
                # Update logic (populate voting_records_to_update)
                record = existing_records[record_key]
                # Update fields as needed, e.g.:
                # record.vote_type = vote_item.get('VOTE_TYPE')
                voting_records_to_update.append(record)
            else:
                # Create logic (append to voting_records_to_create list)
                voting_records_to_create.append(
                    VotingRecord(
                        bill=bill,
                        speaker=speaker,
                        # Set other fields from vote_item as needed
                        # vote_type=vote_item.get('VOTE_TYPE'),
                    ))

        # Bulk create new voting records
        if voting_records_to_create:
            VotingRecord.objects.bulk_create(voting_records_to_create)
            logger.info(
                f"Bulk created {len(voting_records_to_create)} new voting records for bill {bill_id}."
            )

        # Bulk update existing voting records if needed (Django 3.2+ required for bulk_update)
        if voting_records_to_update:
            VotingRecord.objects.bulk_update(
                voting_records_to_update,
                ['vote_type'])  # Add all fields that need updating
            logger.info(
                f"Bulk updated {len(voting_records_to_update)} voting records for bill {bill_id}."
            )

    except Exception as e:
        logger.error(
            f"âŒ Error fetching or processing voting data for bill {bill_id}: {e}"
        )
        logger.exception("Full traceback for voting data fetch:")
        if self:
            try:
                self.retry(exc=e)
            except MaxRetriesExceededError:
                logger.error(
                    f"Max retries exceeded for voting data fetch for bill {bill_id}"
                )
        else:
            raise
        return

    try:
        if not hasattr(settings,
                       'ASSEMBLY_API_KEY') or not settings.ASSEMBLY_API_KEY:
            logger.error(
                "ASSEMBLY_API_KEY not configured for voting data fetch.")
            return

        # Get the bill object
        try:
            bill = Bill.objects.get(bill_id=bill_id)
        except Bill.DoesNotExist:
            logger.error(f"Bill {bill_id} not found in database.")
            return

        url = "https://open.assembly.go.kr/portal/openapi/nojepdqqaweusdfbi"
        params = {
            "KEY": settings.ASSEMBLY_API_KEY,
            "AGE": "22",
            "BILL_ID": bill_id,
            "Type": "json",
            "pSize": 300
        }

        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()

        if debug:
            logger.debug(
                f"ğŸ› DEBUG: Voting API response for {bill_id}: {json.dumps(data, indent=2, ensure_ascii=False)}"
            )

        voting_data = []
        api_key_name = 'nojepdqqaweusdfbi'
        if data and api_key_name in data and isinstance(
                data[api_key_name], list):
            if len(data[api_key_name]) > 1 and isinstance(
                    data[api_key_name][1], dict):
                voting_data = data[api_key_name][1].get('row', [])
            elif len(data[api_key_name]) > 0 and isinstance(
                    data[api_key_name][0], dict):
                head_info = data[api_key_name][0].get('head')
                if head_info and head_info[0].get('RESULT', {}).get(
                        'CODE', '').startswith("INFO-200"):
                    logger.info(
                        f"API result for voting data ({bill_id}) indicates no data."
                    )
                elif 'row' in data[api_key_name][0]:
                    voting_data = data[api_key_name][0].get('row', [])

        if not voting_data:
            logger.info(f"No voting data found for bill {bill_id}")
            return

        # Prepare data for bulk operations
        voting_records_to_create = []
        voting_records_to_update = []

        # Get all speakers at once to avoid repeated database queries
        all_speakers = {
            speaker.naas_nm: speaker
            for speaker in Speaker.objects.all()
        }

        # Get existing voting records for this bill
        existing_records = {
            (record.speaker.naas_nm, record.bill_id): record
            for record in VotingRecord.objects.filter(
                bill=bill).select_related('speaker')
        }

        processed_count = 0
        skipped_count = 0

        for vote_item in voting_data:
            try:
                member_name = vote_item.get('HG_NM', '').strip()
                vote_result = vote_item.get('RESULT_VOTE_MOD', '').strip()
                vote_date_str = vote_item.get('VOTE_DATE', '')

                if not member_name or not vote_result:
                    skipped_count += 1
                    continue

                # Parse vote date
                vote_date = None
                if vote_date_str:
                    try:
                        vote_date = datetime.strptime(vote_date_str,
                                                      '%Y%m%d %H%M%S')
                    except ValueError:
                        logger.warning(
                            f"Could not parse vote date: {vote_date_str}")
                        vote_date = datetime.now()
                else:
                    vote_date = datetime.now()

                # Find the speaker by name from our cached dict
                speaker = None
                if member_name in all_speakers:
                    speaker = all_speakers[member_name]
                else:
                    # Try partial match
                    for speaker_name, speaker_obj in all_speakers.items():
                        if member_name in speaker_name or speaker_name in member_name:
                            speaker = speaker_obj
                            break

                if not speaker:
                    logger.warning(
                        f"Speaker not found for voting record: {member_name}")
                    skipped_count += 1
                    continue

                # Check if record already exists
                record_key = (member_name, bill_id)
                if record_key in existing_records:
                    # Update existing record
                    existing_record = existing_records[record_key]
                    existing_record.vote_result = vote_result
                    existing_record.vote_date = vote_date
                    existing_record.session = bill.session
                    voting_records_to_update.append(existing_record)
                else:
                    # Create new record
                    voting_record = VotingRecord(bill=bill,
                                                 speaker=speaker,
                                                 vote_result=vote_result,
                                                 vote_date=vote_date,
                                                 session=bill.session)
                    voting_records_to_create.append(voting_record)

                processed_count += 1

            except Exception as e_vote:
                logger.error(
                    f"âŒ Error processing vote item for {bill_id}: {e_vote}. Item: {vote_item}"
                )
                skipped_count += 1
                continue

        # Perform bulk operations
        created_count = 0
        updated_count = 0

        if voting_records_to_create:
            try:
                VotingRecord.objects.bulk_create(voting_records_to_create,
                                                 ignore_conflicts=True)
                created_count = len(voting_records_to_create)
                logger.info(
                    f"âœ¨ Bulk created {created_count} voting records for {bill.bill_nm[:30]}..."
                )
            except Exception as e_bulk_create:
                logger.error(f"âŒ Error in bulk create: {e_bulk_create}")

        if voting_records_to_update:
            try:
                VotingRecord.objects.bulk_update(
                    voting_records_to_update,
                    ['vote_result', 'vote_date', 'session'],
                    batch_size=100)
                updated_count = len(voting_records_to_update)
                logger.info(
                    f"ğŸ”„ Bulk updated {updated_count} voting records for {bill.bill_nm[:30]}..."
                )
            except Exception as e_bulk_update:
                logger.error(f"âŒ Error in bulk update: {e_bulk_update}")

        logger.info(
            f"ğŸ‰ Voting data processed for bill {bill_id}: {created_count} created, {updated_count} updated, {skipped_count} skipped, {processed_count} total processed."
        )

    except RequestException as re_exc:
        logger.error(
            f"Request error fetching voting data for {bill_id}: {re_exc}")
        try:
            self.retry(exc=re_exc)
        except MaxRetriesExceededError:
            logger.error(f"Max retries for voting data {bill_id}.")
    except json.JSONDecodeError as json_e:
        logger.error(f"JSON decode error for voting data {bill_id}: {json_e}")
    except Exception as e:
        logger.error(
            f"âŒ Unexpected error fetching voting data for {bill_id}: {e}")
        logger.exception(f"Full traceback for voting data {bill_id}:")
        try:
            self.retry(exc=e)
        except MaxRetriesExceededError:
            logger.error(
                f"Max retries after unexpected error for voting data {bill_id}."
            )


@with_db_retry
def create_placeholder_bill(session_obj, title, bill_no=None):
    """Creates a placeholder Bill for agenda items found only in the PDF."""
    unique_id_str = f"PDF_{session_obj.conf_id}_{hash(title)}"

    bill, created = Bill.objects.get_or_create(
        bill_id=unique_id_str,
        defaults={
            'session': session_obj,
            'bill_nm': title,
            'bill_no': bill_no,  # Will be None if not provided
            'proposer': None,  # Proposer is unknown from PDF agenda
        })
    if created:
        pass
        # Only log the rightmost party in the chain


def process_session_pdf_text(
        full_text,
        session_id,
        session_obj,
        bills_context_str,  # Deprecated
        bill_names_list_from_api,  # Now used as the "known_bill_names"
        debug=False):
    """
    Process session PDF text for extracting statements and bills.
    This function can be called directly or via Celery through the wrapper below.
    """
    # (function body remains unchanged)

    if not full_text:
        logger.warning(f"No text provided for session {session_id}")
        return

    logger.info(
        f"ğŸ”„ Processing PDF text for session {session_id} ({len(full_text)} chars)"
    )

    cleaned_text = clean_pdf_text(full_text)
    if not cleaned_text:
        logger.warning(
            f"No text remaining after cleaning for session {session_id}")
        return

    # Call the new all-in-one function. It handles discovery, placeholder creation, and segmentation.
    statements_data = GEMINI.extract_statements_with_llm_discovery(
        cleaned_text, session_id, bill_names_list_from_api, session_obj, debug)

    if not statements_data:
        logger.warning(
            f"No statements were extracted by the LLM discovery process for session {session_id}"
        )
        return

    logger.info(
        f"âœ… Extracted {len(statements_data)} statements in total for session {session_id}"
    )
    process_extracted_statements_data(statements_data, session_obj, debug)


@shared_task(bind=True, max_retries=3, default_retry_delay=60)
def process_session_pdf_text_task(self,
                                  full_text,
                                  session_id,
                                  session_obj,
                                  bills_context_str,
                                  bill_names_list_from_api,
                                  debug=False):
    """
    Celery-compatible wrapper for process_session_pdf_text.
    Accepts 'self' as required by Celery, but delegates to the actual implementation.
    """
    return process_session_pdf_text(full_text, session_id, session_obj,
                                    bills_context_str,
                                    bill_names_list_from_api, debug)


def process_pdf_text_for_statements(full_text,
                                    session_id,
                                    session_obj,
                                    bills_context_str,
                                    bill_names_list_from_api,
                                    debug=False):
    """Alias for process_session_pdf_text for future compatibility."""
    return process_session_pdf_text(full_text, session_id, session_obj,
                                    bills_context_str,
                                    bill_names_list_from_api, debug)


# Compatibility alias for celery/discovery
process_session_pdf = process_session_pdf_text_task


def clean_pdf_text(text: str) -> str:
    if not text:
        return ""

    original_len = len(text)
    start_marker_match = re.search(r'\(\d{1,2}ì‹œ\s*\d{1,2}ë¶„\s+ê°œì˜\)', text)

    if not start_marker_match:
        logger.warning(
            "âš ï¸ No meeting start marker '(xxì‹œxxë¶„ ê°œì˜)' found. Unable to isolate discussion. Returning raw text."
        )
        return text

    start_pos = start_marker_match.start()
    end_marker_patterns = [
        r'\(\d{1,2}ì‹œ\s*\d{1,2}ë¶„\s+ì‚°íšŒ\)',
        r'\(\d{1,2}ì‹œ\s*\d{1,2}ë¶„\s+ííšŒ\)',
    ]

    end_pos = len(text)
    for pattern in end_marker_patterns:
        end_marker_match = re.search(
            pattern, text[start_pos:])  # Search *after* the start
        if end_marker_match:
            end_pos = start_pos + end_marker_match.end()
            break

    discussion_block = text[start_pos:end_pos]
    logger.info(
        f"ğŸ“– Isolated discussion block of {len(discussion_block)} chars (from original {original_len})."
    )

    # Step 2
    header_pattern = re.compile(r'^ì œ\d+íšŒ-ì œ\d+ì°¨\s*\(.+?\)\s*\d+\s*$')
    report_note_pattern = re.compile(r'\(ë³´ê³ ì‚¬í•­ì€\s*ëì—\s*ì‹¤ìŒ\)')

    cleaned_lines = []
    lines = discussion_block.split('\n')

    for line in lines:
        # First, remove the report note from the line content
        line = report_note_pattern.sub('', line)
        stripped_line = line.strip()

        # Skip empty lines or lines that are just headers
        if not stripped_line or header_pattern.match(stripped_line):
            continue

        cleaned_lines.append(stripped_line)
    final_text = "\n".join(cleaned_lines)
    final_text = re.sub(r'\n{2,}', '\n',
                        final_text)  # Collapse multiple newlines

    logger.info(f"ğŸ§¹ Text cleaning complete. Final length: {len(final_text)}")
    return final_text

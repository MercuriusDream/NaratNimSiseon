import requests
import pdfplumber
from celery import shared_task
from django.conf import settings
from .models import Session, Bill, Speaker, Statement
from celery.exceptions import MaxRetriesExceededError
from requests.exceptions import RequestException
import logging
from celery.schedules import crontab
from datetime import datetime, timedelta, time as dt_time
import json
import os
import time
from pathlib import Path

print("üêõ IMMEDIATE DEBUG: Configuring logger")
logger = logging.getLogger(__name__)
print(f"üêõ IMMEDIATE DEBUG: Logger configured: {logger}")
print(f"üêõ IMMEDIATE DEBUG: Logger level: {logger.level}")
print(f"üêõ IMMEDIATE DEBUG: Logger handlers: {logger.handlers}")

# Configure logger to actually show output
import sys

logger.setLevel(logging.DEBUG)
if not logger.handlers:
    handler = logging.StreamHandler(sys.stdout)
    handler.setLevel(logging.DEBUG)
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
print(
    f"üêõ IMMEDIATE DEBUG: Logger reconfigured with handlers: {logger.handlers}")

# Configure Gemini API with error handling
try:
    import google.generativeai as genai
    if hasattr(settings, 'GEMINI_API_KEY') and settings.GEMINI_API_KEY:
        genai.configure(api_key=settings.GEMINI_API_KEY)
        model = genai.GenerativeModel('gemma-3-27b-it')
    else:
        logger.warning("GEMINI_API_KEY not found in settings")
        genai = None
        model = None
except ImportError:
    logger.warning("google.generativeai not available")
    genai = None
    model = None
except Exception as e:
    logger.warning(f"Error configuring Gemini API: {e}")
    genai = None
    model = None


# Check if Celery/Redis is available
def is_celery_available():
    """Check if Celery/Redis is available for async tasks"""
    from kombu.exceptions import OperationalError
    from celery import current_app
    try:
        current_app.control.inspect().active()
        return True
    except (ImportError, OperationalError, OSError, ConnectionError):
        return False


# Decorator to handle both sync and async execution
def celery_or_sync(func):
    """Decorator that runs function sync if Celery is not available"""

    def wrapper(*args, **kwargs):
        if is_celery_available():
            logger.info(
                f"üîÑ Running {func.__name__} asynchronously with Celery")
            return func.delay(*args, **kwargs)
        else:
            logger.info(
                f"üîÑ Running {func.__name__} synchronously (Celery not available)"
            )
            # Remove 'self' parameter if it's a bound task
            if hasattr(func, '__wrapped__'):
                return func.__wrapped__(*args, **kwargs)
            else:
                return func(*args, **kwargs)

    return wrapper


from celery import shared_task
import logging
from .utils import DataCollector
from .llm_analyzer import LLMPolicyAnalyzer

logger = logging.getLogger(__name__)


def format_conf_id(conf_id):
    """Format CONF_ID to be zero-filled to 6 digits."""
    return str(conf_id).zfill(6)


def fetch_speaker_details(speaker_name):
    """Fetch speaker details from ALLNAMEMBER API"""
    try:
        url = "https://open.assembly.go.kr/portal/openapi/ALLNAMEMBER"
        params = {
            "KEY": settings.ASSEMBLY_API_KEY,
            "NAAS_NM": speaker_name,
            "Type": "json"
        }

        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()

        logger.info(
            f"üêõ DEBUG: ALLNAMEMBER API response for {speaker_name}: {json.dumps(data, indent=2, ensure_ascii=False)}"
        )

        # Extract member data
        member_data = None
        if 'ALLNAMEMBER' in data and len(data['ALLNAMEMBER']) > 1:
            rows = data['ALLNAMEMBER'][1].get('row', [])
            if rows and len(rows) > 0:
                member_data = rows[0]  # Get first match

        if member_data:
            # Create or update speaker with detailed information
            speaker, created = Speaker.objects.get_or_create(
                naas_cd=member_data.get('NAAS_CD', f"TEMP_{speaker_name}"),
                defaults={
                    'naas_nm': member_data.get('NAAS_NM', speaker_name),
                    'naas_ch_nm': member_data.get('NAAS_CH_NM', ''),
                    'plpt_nm': member_data.get('PLPT_NM', 'Ï†ïÎãπÏ†ïÎ≥¥ÏóÜÏùå'),
                    'elecd_nm': member_data.get('ELECD_NM', ''),
                    'elecd_div_nm': member_data.get('ELECD_DIV_NM', ''),
                    'cmit_nm': member_data.get('CMIT_NM', ''),
                    'blng_cmit_nm': member_data.get('BLNG_CMIT_NM', ''),
                    'rlct_div_nm': member_data.get('RLCT_DIV_NM', ''),
                    'gtelt_eraco': member_data.get('GTELT_ERACO', ''),
                    'ntr_div': member_data.get('NTR_DIV', ''),
                    'naas_pic': member_data.get('NAAS_PIC', '')
                })

            if not created:
                # Update existing speaker with new information
                speaker.naas_nm = member_data.get('NAAS_NM', speaker.naas_nm)
                speaker.naas_ch_nm = member_data.get('NAAS_CH_NM',
                                                     speaker.naas_ch_nm)
                speaker.plpt_nm = member_data.get('PLPT_NM', speaker.plpt_nm)
                speaker.elecd_nm = member_data.get('ELECD_NM',
                                                   speaker.elecd_nm)
                speaker.elecd_div_nm = member_data.get('ELECD_DIV_NM',
                                                       speaker.elecd_div_nm)
                speaker.cmit_nm = member_data.get('CMIT_NM', speaker.cmit_nm)
                speaker.blng_cmit_nm = member_data.get('BLNG_CMIT_NM',
                                                       speaker.blng_cmit_nm)
                speaker.rlct_div_nm = member_data.get('RLCT_DIV_NM',
                                                      speaker.rlct_div_nm)
                speaker.gtelt_eraco = member_data.get('GTELT_ERACO',
                                                      speaker.gtelt_eraco)
                speaker.ntr_div = member_data.get('NTR_DIV', speaker.ntr_div)
                speaker.naas_pic = member_data.get('NAAS_PIC',
                                                   speaker.naas_pic)
                speaker.save()

            logger.info(
                f"‚úÖ Fetched/updated speaker details for: {speaker_name}")
            return speaker
        else:
            logger.warning(f"‚ö†Ô∏è No member data found for: {speaker_name}")
            return None

    except Exception as e:
        logger.error(
            f"‚ùå Error fetching speaker details for {speaker_name}: {e}")
        return None


@shared_task(bind=True, max_retries=3, default_retry_delay=60)
def fetch_continuous_sessions(self=None,
                              force=False,
                              debug=False,
                              start_date=None):
    """Fetch sessions starting from a specific date or continue from last session."""
    try:
        logger.info(
            f"üîç Starting continuous session fetch (force={force}, debug={debug}, start_date={start_date})"
        )

        # Check if we have the required settings
        if not hasattr(settings,
                       'ASSEMBLY_API_KEY') or not settings.ASSEMBLY_API_KEY:
            logger.error("‚ùå ASSEMBLY_API_KEY not configured")
            raise ValueError("ASSEMBLY_API_KEY not configured")

        url = "https://open.assembly.go.kr/portal/openapi/nzbyfwhwaoanttzje"

        # Determine starting point
        if start_date:
            from datetime import datetime
            start_datetime = datetime.fromisoformat(start_date)
            logger.info(
                f"üìÖ Continuing from date: {start_datetime.strftime('%Y-%m')}")
        else:
            start_datetime = datetime.now()
            logger.info(
                f"üìÖ Starting from current date: {start_datetime.strftime('%Y-%m')}"
            )

        # Fetch sessions month by month going backwards from start date
        current_date = start_datetime
        sessions_found = False

        for months_back in range(0, 36):  # Go back up to 36 months
            # Calculate target month
            year = current_date.year
            month = current_date.month - months_back

            # Handle year rollover
            while month <= 0:
                month += 12
                year -= 1

            conf_date = f"{year:04d}-{month:02d}"

            params = {
                "KEY": settings.ASSEMBLY_API_KEY,
                "Type": "json",
                "DAE_NUM": "22",  # 22nd Assembly
                "CONF_DATE": conf_date
            }

            logger.info(f"üìÖ Fetching sessions for: {conf_date}")

            if debug:
                logger.info(f"üêõ DEBUG: API URL: {url}")
                logger.info(f"üêõ DEBUG: API Params for {conf_date}: {params}")

            try:
                response = requests.get(url, params=params, timeout=30)
                response.raise_for_status()
                data = response.json()

                if debug:
                    logger.info(
                        f"üêõ DEBUG: API Response status for {conf_date}: {response.status_code}"
                    )

                sessions_data = extract_sessions_from_response(data,
                                                               debug=debug)

                if sessions_data:
                    sessions_found = True
                    logger.info(
                        f"‚úÖ Found {len(sessions_data)} sessions for {conf_date}"
                    )

                    # Process sessions for this month
                    process_sessions_data(sessions_data,
                                          force=force,
                                          debug=debug)

                    # Small delay between requests to be respectful
                    if not debug:
                        time.sleep(1)
                else:
                    logger.info(f"‚ùå No sessions found for {conf_date}")

                    # If we haven't found any sessions in the last 6 months, stop
                    if months_back > 6 and not sessions_found:
                        logger.info(
                            "üõë No sessions found in recent months, stopping search"
                        )
                        break

            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error fetching {conf_date}: {e}")
                if debug:
                    logger.info(
                        f"üêõ DEBUG: Full error for {conf_date}: {type(e).__name__}: {e}"
                    )
                continue

        # After session collection, fetch additional data
        if not debug and sessions_found:
            logger.info("üîÑ Starting additional data collection...")
            if is_celery_available():
                fetch_additional_data_nepjpxkkabqiqpbvk.delay(force=force,
                                                              debug=debug)
            else:
                fetch_additional_data_nepjpxkkabqiqpbvk(force=force,
                                                        debug=debug)

        if sessions_found:
            logger.info("üéâ Continuous session fetch completed")
        else:
            logger.info("‚ÑπÔ∏è No new sessions found during continuous fetch")

    except Exception as e:
        if isinstance(e, RequestException):
            if self:
                try:
                    self.retry(exc=e)
                except MaxRetriesExceededError:
                    logger.error(
                        "Max retries exceeded for fetch_continuous_sessions")
                    raise
            else:
                logger.error("Sync execution failed, no retry available")
                raise
        logger.error(f"‚ùå Critical error in fetch_continuous_sessions: {e}")
        logger.error(f"üìä Session count in DB: {Session.objects.count()}")
        raise


@shared_task(bind=True, max_retries=3, default_retry_delay=60)
def fetch_latest_sessions(self=None, force=False, debug=False):
    """Fetch latest assembly sessions from the API."""
    # Add immediate debug output
    print(
        f"üêõ IMMEDIATE DEBUG: Function called with force={force}, debug={debug}"
    )

    try:
        print(f"üêõ IMMEDIATE DEBUG: About to call logger.info")
        logger.info(f"üîç Starting session fetch (force={force}, debug={debug})")
        print(f"üêõ IMMEDIATE DEBUG: Logger.info called successfully")

        # Check if we have the required settings
        print(f"üêõ IMMEDIATE DEBUG: Checking settings")
        if not hasattr(settings, 'ASSEMBLY_API_KEY'):
            print(f"üêõ IMMEDIATE DEBUG: ASSEMBLY_API_KEY attribute not found")
            logger.error("‚ùå ASSEMBLY_API_KEY not found in settings")
            raise ValueError("ASSEMBLY_API_KEY not configured")

        if not settings.ASSEMBLY_API_KEY:
            print(f"üêõ IMMEDIATE DEBUG: ASSEMBLY_API_KEY is empty")
            logger.error("‚ùå ASSEMBLY_API_KEY is empty")
            raise ValueError("ASSEMBLY_API_KEY not configured")

        print(f"üêõ IMMEDIATE DEBUG: Settings check passed")
        print(
            f"üêõ IMMEDIATE DEBUG: API Key exists: {bool(settings.ASSEMBLY_API_KEY)}"
        )
        print(
            f"üêõ IMMEDIATE DEBUG: API Key first 10 chars: {settings.ASSEMBLY_API_KEY[:10]}..."
        )

        if debug:
            print(f"üêõ DEBUG: Function started successfully")
            print(f"üêõ DEBUG: Settings check passed")
            logger.info(f"üêõ DEBUG: Function started successfully")
            logger.info(f"üêõ DEBUG: Settings check passed")

    except Exception as e:
        print(f"üêõ IMMEDIATE DEBUG: Exception caught: {e}")
        print(f"üêõ IMMEDIATE DEBUG: Exception type: {type(e).__name__}")
        logger.error(f"‚ùå Error at start of fetch_latest_sessions: {e}")
        logger.error(f"‚ùå Error type: {type(e).__name__}")
        import traceback
        traceback_str = traceback.format_exc()
        print(f"üêõ IMMEDIATE DEBUG: Traceback: {traceback_str}")
        logger.error(f"‚ùå Full traceback: {traceback_str}")
        raise

    try:
        print(f"üêõ IMMEDIATE DEBUG: About to start API calls")
        url = "https://open.assembly.go.kr/portal/openapi/nzbyfwhwaoanttzje"
        print(f"üêõ IMMEDIATE DEBUG: URL set to: {url}")

        # If not force, only fetch recent sessions
        if not force:
            print(
                f"üêõ IMMEDIATE DEBUG: Not force mode, fetching current month only"
            )
            # Fetch current month only
            current_date = datetime.now()
            conf_date = (current_date - timedelta(days=30)).strftime('%Y-%m')
            print(
                f"üêõ IMMEDIATE DEBUG: Current date calculated as: {conf_date}")
            params = {
                "KEY": settings.ASSEMBLY_API_KEY,
                "Type": "json",
                "DAE_NUM": "22",  # 22nd Assembly
                "CONF_DATE": conf_date
            }
            print(f"üêõ IMMEDIATE DEBUG: Params created: {params}")
            logger.info(
                f"üìÖ Fetching sessions for: {(current_date-timedelta(days=30)).strftime('%Y-%m')}"
            )

            if debug:
                print(f"üêõ DEBUG: API URL: {url}")
                print(f"üêõ DEBUG: API Params: {params}")
                logger.info(f"üêõ DEBUG: API URL: {url}")
                logger.info(f"üêõ DEBUG: API Params: {params}")

            print(f"üêõ IMMEDIATE DEBUG: About to make API request")
            response = requests.get(url, params=params, timeout=30)
            print(
                f"üêõ IMMEDIATE DEBUG: API request completed, status: {response.status_code}"
            )
            response.raise_for_status()
            print(f"üêõ IMMEDIATE DEBUG: Response status check passed")
            data = response.json()
            print(
                f"üêõ IMMEDIATE DEBUG: JSON parsing completed, data type: {type(data)}"
            )

            if debug:
                print(f"üêõ DEBUG: API Response status: {response.status_code}")
                print(
                    f"üêõ DEBUG: Full API response: {json.dumps(data, indent=2, ensure_ascii=False)}"
                )
                logger.info(
                    f"üêõ DEBUG: API Response status: {response.status_code}")
                logger.info(
                    f"üêõ DEBUG: Full API response: {json.dumps(data, indent=2, ensure_ascii=False)}"
                )

            print(
                f"üêõ IMMEDIATE DEBUG: About to extract sessions from response")
            sessions_data = extract_sessions_from_response(data, debug=debug)
            print(
                f"üêõ IMMEDIATE DEBUG: Sessions extraction completed, found {len(sessions_data) if sessions_data else 0} sessions"
            )

            if sessions_data:
                print(
                    f"üêõ IMMEDIATE DEBUG: About to process {len(sessions_data)} sessions"
                )
                process_sessions_data(sessions_data, force=force, debug=debug)
                print(f"üêõ IMMEDIATE DEBUG: Sessions processing completed")
            elif debug:
                print("üêõ DEBUG: No sessions data found to process")
                print(
                    f"üêõ DEBUG: Raw API response keys: {list(data.keys()) if data else 'No data'}"
                )
                logger.info("üêõ DEBUG: No sessions data found to process")
                logger.info(
                    f"üêõ DEBUG: Raw API response keys: {list(data.keys()) if data else 'No data'}"
                )
                if data:
                    for key, value in data.items():
                        print(
                            f"üêõ DEBUG: {key}: {type(value)} - {str(value)[:200]}..."
                        )
                        logger.info(
                            f"üêõ DEBUG: {key}: {type(value)} - {str(value)[:200]}..."
                        )
            else:
                print("‚ùå No sessions data found in API response")
                logger.info("‚ùå No sessions data found in API response")
        else:
            # Force mode: fetch month by month going backwards
            print(f"üêõ IMMEDIATE DEBUG: Force mode enabled")
            logger.info("üîÑ Force mode: Fetching sessions month by month")
            current_date = datetime.now() - timedelta(days=30)
            print(
                f"üêõ IMMEDIATE DEBUG: Starting from current date: {current_date}"
            )

            for months_back in range(0, 24):  # Go back up to 24 months
                # Use proper month calculation instead of days
                year = current_date.year
                month = current_date.month - months_back

                # Handle year rollover
                while month <= 0:
                    month += 12
                    year -= 1

                conf_date = f"{year:04d}-{month:02d}"
                print(
                    f"üêõ IMMEDIATE DEBUG: Calculated conf_date for months_back={months_back}: {conf_date}"
                )

                params = {
                    "KEY": settings.ASSEMBLY_API_KEY,
                    "Type": "json",
                    "DAE_NUM": "22",  # 22nd Assembly
                    "CONF_DATE": conf_date
                }

                logger.info(f"üìÖ Fetching sessions for: {conf_date}")

                if debug:
                    logger.info(f"üêõ DEBUG: API URL: {url}")
                    logger.info(
                        f"üêõ DEBUG: API Params for {conf_date}: {params}")

                try:
                    response = requests.get(url, params=params, timeout=30)
                    response.raise_for_status()
                    data = response.json()

                    if debug:
                        logger.info(
                            f"üêõ DEBUG: API Response status for {conf_date}: {response.status_code}"
                        )
                        logger.info(
                            f"üêõ DEBUG: Full API response for {conf_date}: {json.dumps(data, indent=2, ensure_ascii=False)}"
                        )

                    sessions_data = extract_sessions_from_response(data,
                                                                   debug=debug)
                    if not sessions_data:
                        logger.info(
                            f"‚ùå No sessions found for {conf_date}, stopping..."
                        )
                        if debug:
                            logger.info(
                                f"üêõ DEBUG: Breaking loop at {conf_date}")
                        break

                    process_sessions_data(sessions_data,
                                          force=force,
                                          debug=debug)

                    # Small delay between requests to be respectful
                    if not debug:  # Skip delay in debug mode for faster testing
                        time.sleep(1)

                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error fetching {conf_date}: {e}")
                    if debug:
                        logger.info(
                            f"üêõ DEBUG: Full error for {conf_date}: {type(e).__name__}: {e}"
                        )
                    continue

        # After session collection, fetch additional data
        if not debug:
            logger.info("üîÑ Starting additional data collection...")
            if is_celery_available():
                fetch_additional_data_nepjpxkkabqiqpbvk.delay(force=force,
                                                              debug=debug)
            else:
                fetch_additional_data_nepjpxkkabqiqpbvk(force=force,
                                                        debug=debug)

        logger.info("üéâ Session fetch completed")

    except Exception as e:
        if isinstance(e, RequestException):
            if self:
                try:
                    self.retry(exc=e)
                except MaxRetriesExceededError:
                    logger.error(
                        "Max retries exceeded for fetch_latest_sessions")
                    raise
            else:
                logger.error("Sync execution failed, no retry available")
                raise
        logger.error(f"‚ùå Critical error in fetch_latest_sessions: {e}")
        logger.error(f"üìä Session count in DB: {Session.objects.count()}")
        raise


def extract_sessions_from_response(data, debug=False):
    """Extract sessions data from API response"""
    print(
        f"üêõ IMMEDIATE DEBUG: extract_sessions_from_response called with debug={debug}"
    )
    print(f"üêõ IMMEDIATE DEBUG: Data type: {type(data)}")
    print(
        f"üêõ IMMEDIATE DEBUG: Data keys: {list(data.keys()) if data else 'Empty response'}"
    )

    if debug:
        print(
            f"üêõ DEBUG: Full API response structure: {list(data.keys()) if data else 'Empty response'}"
        )
        logger.info(
            f"üêõ DEBUG: Full API response structure: {list(data.keys()) if data else 'Empty response'}"
        )
        if data and 'nzbyfwhwaoanttzje' in data:
            print(
                f"üêõ DEBUG: nzbyfwhwaoanttzje length: {len(data['nzbyfwhwaoanttzje'])}"
            )
            logger.info(
                f"üêõ DEBUG: nzbyfwhwaoanttzje length: {len(data['nzbyfwhwaoanttzje'])}"
            )
            if len(data['nzbyfwhwaoanttzje']) > 0:
                print(
                    f"üêõ DEBUG: First element keys: {list(data['nzbyfwhwaoanttzje'][0].keys()) if isinstance(data['nzbyfwhwaoanttzje'][0], dict) else 'Not a dict'}"
                )
                print(
                    f"üêõ DEBUG: First element: {data['nzbyfwhwaoanttzje'][0]}")
                logger.info(
                    f"üêõ DEBUG: First element keys: {list(data['nzbyfwhwaoanttzje'][0].keys()) if isinstance(data['nzbyfwhwaoanttzje'][0], dict) else 'Not a dict'}"
                )
            if len(data['nzbyfwhwaoanttzje']) > 1:
                print(
                    f"üêõ DEBUG: Second element keys: {list(data['nzbyfwhwaoanttzje'][1].keys()) if isinstance(data['nzbyfwhwaoanttzje'][1], dict) else 'Not a dict'}"
                )
                print(
                    f"üêõ DEBUG: Second element: {data['nzbyfwhwaoanttzje'][1]}")
                logger.info(
                    f"üêõ DEBUG: Second element keys: {list(data['nzbyfwhwaoanttzje'][1].keys()) if isinstance(data['nzbyfwhwaoanttzje'][1], dict) else 'Not a dict'}"
                )

    sessions_data = None
    print(f"üêõ IMMEDIATE DEBUG: Starting sessions data extraction")

    if 'nzbyfwhwaoanttzje' in data and len(data['nzbyfwhwaoanttzje']) > 1:
        print(f"üêõ IMMEDIATE DEBUG: Using second element for sessions data")
        sessions_data = data['nzbyfwhwaoanttzje'][1].get('row', [])
        if debug:
            print(f"üêõ DEBUG: Using second element for sessions data")
            logger.info(f"üêõ DEBUG: Using second element for sessions data")
    elif 'nzbyfwhwaoanttzje' in data and len(data['nzbyfwhwaoanttzje']) > 0:
        print(
            f"üêõ IMMEDIATE DEBUG: Using first element as fallback for sessions data"
        )
        # Try first element as fallback
        sessions_data = data['nzbyfwhwaoanttzje'][0].get('row', [])
        if debug:
            print(
                f"üêõ DEBUG: Using first element as fallback for sessions data")
            logger.info(
                f"üêõ DEBUG: Using first element as fallback for sessions data")
    elif 'row' in data:
        print(f"üêõ IMMEDIATE DEBUG: Using direct 'row' key for sessions data")
        # Fallback for old API structure
        sessions_data = data['row']
        if debug:
            print(f"üêõ DEBUG: Using direct 'row' key for sessions data")
            logger.info(f"üêõ DEBUG: Using direct 'row' key for sessions data")
    else:
        print(
            f"üêõ IMMEDIATE DEBUG: No sessions data found in any expected location"
        )

    print(
        f"üêõ IMMEDIATE DEBUG: Extracted {len(sessions_data) if sessions_data else 0} sessions from response"
    )

    if debug:
        print(
            f"üêõ DEBUG: Extracted {len(sessions_data) if sessions_data else 0} sessions from response"
        )
        logger.info(
            f"üêõ DEBUG: Extracted {len(sessions_data) if sessions_data else 0} sessions from response"
        )
        if sessions_data and len(sessions_data) > 0:
            print(
                f"üêõ DEBUG: Sample session keys: {list(sessions_data[0].keys())}"
            )
            print(f"üêõ DEBUG: First session sample data: {sessions_data[0]}")
            logger.info(
                f"üêõ DEBUG: Sample session keys: {list(sessions_data[0].keys())}"
            )
            logger.info(
                f"üêõ DEBUG: First session sample data: {sessions_data[0]}")
        else:
            print(f"üêõ DEBUG: No session data found in response")
            logger.info(f"üêõ DEBUG: No session data found in response")

    print(
        f"‚úÖ Found {len(sessions_data) if sessions_data else 0} sessions in API response"
    )
    logger.info(
        f"‚úÖ Found {len(sessions_data) if sessions_data else 0} sessions in API response"
    )
    return sessions_data


def process_sessions_data(sessions_data, force=False, debug=False):
    """Process the sessions data and create/update session objects"""
    print(
        f"üêõ IMMEDIATE DEBUG: process_sessions_data called with {len(sessions_data) if sessions_data else 0} sessions, debug={debug}"
    )

    if not sessions_data:
        print("‚ùå No sessions data to process")
        logger.warning("‚ùå No sessions data to process")
        return

    # Always show the first few sessions to understand the data structure
    '''
    print("üîç RAW API SESSION DATA STRUCTURE:")
    print("=" * 80)
    for i, row in enumerate(sessions_data[:5],
                            1):  # Show first 5 sessions always
        print(f"üìã SESSION {i} RAW DATA:")
        print(f"   Type: {type(row)}")
        print(
            f"   Keys: {list(row.keys()) if isinstance(row, dict) else 'Not a dict'}"
        )

        # Show all key-value pairs
        if isinstance(row, dict):
            for key, value in row.items():
                print(f"   {key}: {value}")
        else:
            print(f"   Full value: {row}")
        print("   " + "-" * 60)
    print("=" * 80)
    '''

    # Group sessions by CONFER_NUM since multiple agenda items can belong to the same session
    sessions_by_id = {}
    for row in sessions_data:
        session_id = row.get('CONFER_NUM')
        if session_id:
            if session_id not in sessions_by_id:
                sessions_by_id[session_id] = []
            sessions_by_id[session_id].append(row)

    print(
        f"üîç GROUPED SESSIONS: Found {len(sessions_by_id)} unique sessions from {len(sessions_data)} agenda items"
    )
    logger.info(
        f"üîç GROUPED SESSIONS: Found {len(sessions_by_id)} unique sessions from {len(sessions_data)} agenda items"
    )

    if debug:
        print(
            f"üêõ DEBUG MODE: Processing {len(sessions_by_id)} unique sessions (preview only - no database writes)"
        )
        logger.info(
            f"üêõ DEBUG MODE: Processing {len(sessions_by_id)} unique sessions (preview only - no database writes)"
        )

        for i, (session_id, agenda_items) in enumerate(sessions_by_id.items(),
                                                       1):
            first_item = agenda_items[
                0]  # Use first agenda item for main session info
            title = first_item.get('TITLE', 'Unknown')
            date = first_item.get('CONF_DATE', 'Unknown')
            pdf_url = first_item.get('PDF_LINK_URL', 'No PDF')

            print(f"üêõ DEBUG Session {i}: ID={session_id}")
            print(f"   Title: {title}")
            print(f"   Date: {date}")
            print(f"   PDF: {pdf_url}")
            print(f"   Agenda items: {len(agenda_items)}")
            for j, item in enumerate(agenda_items, 1):
                print(
                    f"     {j}. {item.get('SUB_NAME', 'No agenda item name')}")
            print("   ---")

            logger.info(f"üêõ DEBUG Session {i}: ID={session_id}")
            logger.info(f"   Title: {title}")
            logger.info(f"   Date: {date}")
            logger.info(f"   PDF: {pdf_url}")
            logger.info(f"   Agenda items: {len(agenda_items)}")

        print("üêõ DEBUG MODE: Data preview completed - not storing to database")
        logger.info(
            "üêõ DEBUG MODE: Data preview completed - not storing to database")
        return

    created_count = 0
    updated_count = 0

    for i, (session_id, agenda_items) in enumerate(sessions_by_id.items(), 1):
        # Use the first agenda item for the main session information
        row = agenda_items[0]
        try:
            logger.info(
                f"üîÑ Processing session {i}/{len(sessions_by_id)}: {row.get('TITLE', 'Unknown')} ({len(agenda_items)} agenda items)"
            )

            if not session_id:
                logger.warning(f"‚ö†Ô∏è No CONFER_NUM found for session {i}")
                continue

            # Parse date properly
            conf_date = None
            if row.get('CONF_DATE'):
                try:
                    conf_date = datetime.strptime(row.get('CONF_DATE'),
                                                  '%YÎÖÑ %mÏõî %dÏùº').date()
                except ValueError:
                    try:
                        conf_date = datetime.strptime(row.get('CONF_DATE'),
                                                      '%Y-%m-%d').date()
                    except ValueError:
                        logger.warning(
                            f"Could not parse date: {row.get('CONF_DATE')}")
                        conf_date = None

            session, created = Session.objects.get_or_create(
                conf_id=session_id,
                defaults={
                    'era_co':
                    f'Ï†ú{row.get("DAE_NUM", 22)}ÎåÄ',
                    'sess':
                    row.get('TITLE', '').split(' ')[2] if len(
                        row.get('TITLE', '').split(' ')) > 2 else '',
                    'dgr':
                    row.get('TITLE', '').split(' ')[3] if len(
                        row.get('TITLE', '').split(' ')) > 3 else '',
                    'conf_dt':
                    conf_date,
                    'conf_knd':
                    row.get('CLASS_NAME', 'Íµ≠ÌöåÎ≥∏ÌöåÏùò'),
                    'cmit_nm':
                    row.get('CLASS_NAME', 'Íµ≠ÌöåÎ≥∏ÌöåÏùò'),
                    'bg_ptm':
                    dt_time(9, 0),  # Default time since API doesn't provide it
                    'down_url':
                    row.get('PDF_LINK_URL', '')
                })

            if created:
                created_count += 1
                logger.info(f"‚ú® Created new session: {session_id}")
            else:
                logger.info(f"‚ôªÔ∏è  Session already exists: {session_id}")

            # If session exists and force is True, update the session
            if not created and force:
                session.era_co = f'Ï†ú{row.get("DAE_NUM", 22)}ÎåÄ'
                session.sess = row.get('TITLE', '').split(' ')[2] if len(
                    row.get('TITLE', '').split(' ')) > 2 else ''
                session.dgr = row.get('TITLE', '').split(' ')[3] if len(
                    row.get('TITLE', '').split(' ')) > 3 else ''
                session.conf_dt = conf_date
                session.conf_knd = row.get('CLASS_NAME', 'Íµ≠ÌöåÎ≥∏ÌöåÏùò')
                session.cmit_nm = row.get('CLASS_NAME', 'Íµ≠ÌöåÎ≥∏ÌöåÏùò')
                session.down_url = row.get('PDF_LINK_URL', '')
                if not session.bg_ptm:  # Only update if not already set
                    session.bg_ptm = dt_time(9, 0)
                session.save()
                updated_count += 1
                logger.info(f"üîÑ Updated existing session: {session_id}")

            # Queue session details fetch (with fallback)
            if is_celery_available():
                fetch_session_details.delay(session_id,
                                            force=force,
                                            debug=debug)
                logger.info(f"üìã Queued details fetch for: {session_id}")
            else:
                fetch_session_details(session_id=session_id,
                                      force=force,
                                      debug=debug)
                logger.info(f"üìã Processed details fetch for: {session_id}")

        except Exception as e:
            logger.error(f"‚ùå Error processing session row {i}: {e}")
            continue

    logger.info(
        f"üéâ Sessions processed: {created_count} created, {updated_count} updated"
    )


@shared_task(bind=True, max_retries=3, default_retry_delay=60)
def fetch_session_details(self=None,
                          session_id=None,
                          force=False,
                          debug=False):
    """Fetch detailed information for a specific session."""
    try:
        if debug:
            logger.info(
                f"üêõ DEBUG: Fetching details for session {session_id} in debug mode"
            )
            # Continue with actual API call in debug mode
        url = "https://open.assembly.go.kr/portal/openapi/VCONFDETAIL"
        params = {
            "KEY": settings.ASSEMBLY_API_KEY,
            "Type": "json",
            "CONF_ID": format_conf_id(session_id)
        }

        logger.info(f"üîç Fetching details for session: {session_id}")
        response = requests.get(url, params=params, timeout=30)

        response.raise_for_status()
        data = response.json()

        logger.info(
            f"üìä Session details API response structure: {list(data.keys()) if data else 'Empty response'}"
        )

        # Check for different possible response structures
        session_details = None
        if data.get('row') and len(data['row']) > 0:
            session_details = data['row'][0]
        elif 'VCONFDETAIL' in data and len(data['VCONFDETAIL']) > 1:
            # Handle nested structure like the sessions API
            session_details = data['VCONFDETAIL'][1].get('row', [])
            if session_details and len(session_details) > 0:
                session_details = session_details[0]
            else:
                session_details = None

        if not session_details:
            logger.info(
                f"‚ÑπÔ∏è  No detailed info available for session {session_id} (this is normal for some sessions)"
            )
            if debug:
                logger.info(f"üìã Full API response: {data}")

            # Try to fetch bills anyway, some sessions might have bills without detailed info
            if is_celery_available():
                fetch_session_bills.delay(session_id, force=force, debug=debug)
            else:
                fetch_session_bills(session_id=session_id,
                                    force=force,
                                    debug=debug)
            return

        # Update session with detailed info if available
        session = Session.objects.get(conf_id=session_id)

        # Update session fields with detailed information
        if session_details.get('CONF_TIME'):
            try:
                # Parse time if available
                time_str = session_details.get('CONF_TIME', '09:00')
                session.bg_ptm = datetime.strptime(time_str, '%H:%M').time()
            except ValueError:
                session.bg_ptm = dt_time(9, 0)  # Default time

        if session_details.get('ED_TIME'):
            try:
                time_str = session_details.get('ED_TIME', '18:00')
                session.ed_ptm = datetime.strptime(time_str, '%H:%M').time()
            except ValueError:
                session.ed_ptm = dt_time(18, 0)  # Default time

        session.save()
        logger.info(f"‚úÖ Updated session details for: {session_id}")

        # Queue bills fetch
        if is_celery_available():
            fetch_session_bills.delay(session_id, force=force, debug=debug)
        else:
            fetch_session_bills(session_id=session_id,
                                force=force,
                                debug=debug)

        # Queue PDF processing for statement extraction
        if session.down_url and not debug:
            if is_celery_available():
                process_session_pdf.delay(session_id, force=force, debug=debug)
            else:
                process_session_pdf(session_id=session_id,
                                    force=force,
                                    debug=debug)

    except Exception as e:
        if isinstance(e, RequestException):
            if self:
                try:
                    self.retry(exc=e)
                except MaxRetriesExceededError:
                    logger.error(
                        f"Max retries exceeded for session {session_id}")
                    raise
            else:
                logger.error("Sync execution failed, no retry available")
                raise
        logger.error(f"‚ùå Error fetching session details for {session_id}: {e}")
        raise


@shared_task(bind=True, max_retries=3, default_retry_delay=60)
def fetch_session_bills(self=None, session_id=None, force=False, debug=False):
    """Fetch bills for a specific session using VCONFBILLLIST API."""
    try:
        if debug:
            logger.info(
                f"üêõ DEBUG: Fetching bills for session {session_id} in debug mode"
            )

        url = "https://open.assembly.go.kr/portal/openapi/VCONFBILLLIST"
        params = {
            "KEY": settings.ASSEMBLY_API_KEY,
            "Type": "json",
            "CONF_ID": format_conf_id(session_id)  # Zero-fill to 6 digits
        }

        logger.info(
            f"üîç Fetching bills for session: {session_id} (formatted: {format_conf_id(session_id)})"
        )
        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()

        logger.info(
            f"üìä Bills API response structure: {list(data.keys()) if data else 'Empty response'}"
        )

        if debug:
            logger.info(
                f"üêõ DEBUG: Full VCONFBILLLIST response: {json.dumps(data, indent=2, ensure_ascii=False)}"
            )

        # Extract bills data from VCONFBILLLIST response structure
        bills_data = None
        if 'VCONFBILLLIST' in data and len(data['VCONFBILLLIST']) > 1:
            bills_data = data['VCONFBILLLIST'][1].get('row', [])
        elif 'VCONFBILLLIST' in data and len(data['VCONFBILLLIST']) > 0:
            # Check if first element has row data
            first_element = data['VCONFBILLLIST'][0]
            if 'row' in first_element:
                bills_data = first_element['row']
        elif 'row' in data:
            bills_data = data['row']

        print(response.text)

        if not bills_data:
            logger.info(f"‚ÑπÔ∏è  No bills found for session {session_id}")
            if debug:
                logger.info(
                    f"üêõ DEBUG: Available data keys: {list(data.keys()) if data else 'None'}"
                )
                if 'VCONFBILLLIST' in data:
                    logger.info(
                        f"üêõ DEBUG: VCONFBILLLIST structure: {data['VCONFBILLLIST']}"
                    )
            return

        # Get session object
        try:
            session = Session.objects.get(conf_id=session_id)
        except Session.DoesNotExist:
            logger.error(f"‚ùå Session {session_id} not found in database")
            return

        created_count = 0
        updated_count = 0

        for bill_data in bills_data:
            try:
                bill_id = bill_data.get('BILL_ID')
                if not bill_id:
                    continue

                bill, created = Bill.objects.get_or_create(
                    bill_id=bill_id,
                    defaults={
                        'session': session,
                        'bill_nm': bill_data.get('BILL_NM', ''),
                        'link_url': bill_data.get('LINK_URL', '')
                    })

                if created:
                    created_count += 1
                    logger.info(f"‚ú® Created new bill: {bill_id}")
                elif force:
                    # Update existing bill if force is True
                    bill.bill_nm = bill_data.get('BILL_NM', bill.bill_nm)
                    bill.link_url = bill_data.get('LINK_URL', bill.link_url)
                    bill.save()
                    updated_count += 1
                    logger.info(f"üîÑ Updated existing bill: {bill_id}")

                if debug:
                    logger.info(
                        f"üêõ DEBUG: Processed bill - ID: {bill_id}, Name: {bill_data.get('BILL_NM', '')[:50]}..."
                    )

            except Exception as e:
                logger.error(
                    f"‚ùå Error processing bill {bill_data.get('BILL_ID', 'unknown')}: {e}"
                )
                continue

        logger.info(
            f"üéâ Bills processed for session {session_id}: {created_count} created, {updated_count} updated"
        )

    except Exception as e:
        if isinstance(e, RequestException):
            if self:
                try:
                    self.retry(exc=e)
                except MaxRetriesExceededError:
                    logger.error(
                        f"Max retries exceeded for bills fetch {session_id}")
                    raise
            else:
                logger.error("Sync execution failed, no retry available")
                raise
        logger.error(f"‚ùå Error fetching bills for session {session_id}: {e}")
        raise


@shared_task(bind=True, max_retries=3, default_retry_delay=60)
def process_session_pdf(self=None, session_id=None, force=False, debug=False):
    """Download and process PDF transcript for a session to extract statements."""
    try:
        if debug:
            logger.info(
                f"üêõ DEBUG: Processing PDF for session {session_id} in debug mode"
            )

        # Get session object
        try:
            session = Session.objects.get(conf_id=session_id)
        except Session.DoesNotExist:
            logger.error(f"‚ùå Session {session_id} not found in database")
            return

        if not session.down_url:
            logger.info(f"‚ÑπÔ∏è  No PDF URL available for session {session_id}")
            return

        logger.info(f"üìÑ Processing PDF for session: {session_id}")

        # Download PDF
        response = requests.get(session.down_url, timeout=60, stream=True)
        response.raise_for_status()

        # Save PDF temporarily
        temp_dir = Path("temp")
        temp_dir.mkdir(exist_ok=True)
        temp_pdf_path = temp_dir / f"temp_{session_id}.pdf"

        with open(temp_pdf_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)

        logger.info(f"üì• Downloaded PDF for session {session_id}")

        # Extract text from PDF
        try:
            with pdfplumber.open(temp_pdf_path) as pdf:
                full_text = ""
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        full_text += page_text + "\n"

                logger.info(
                    f"üìÑ Extracted {len(full_text)} characters from PDF")

                # Fetch bill context for the session
                bills_context = get_bills_context(session_id)

                # Process the extracted text using the helper function
                process_pdf_statements(full_text, session_id, session,
                                       bills_context, debug)

        except Exception as e:
            logger.error(
                f"‚ùå Error extracting text from PDF for session {session_id}: {e}"
            )
            return
        finally:
            # Clean up temporary file
            if temp_pdf_path.exists():
                temp_pdf_path.unlink()

    except Exception as e:
        if isinstance(e, RequestException):
            if self:
                try:
                    self.retry(exc=e)
                except MaxRetriesExceededError:
                    logger.error(
                        f"Max retries exceeded for PDF processing {session_id}"
                    )
                    raise
            else:
                logger.error("Sync execution failed, no retry available")
                raise
        logger.error(f"‚ùå Error processing PDF for session {session_id}: {e}")
        raise


@shared_task(bind=True, max_retries=3, default_retry_delay=60)
def analyze_statement_categories(self=None, statement_id=None):
    """Analyze categories and sentiment for an existing statement using LLM."""
    if not model:
        logger.warning("‚ùå Gemini model not available for statement analysis")
        return

    try:
        from .models import Statement
        statement = Statement.objects.get(id=statement_id)

        prompt = f"""
Îã§Ïùå Íµ≠Ìöå Î∞úÏñ∏ÏùÑ Î∂ÑÏÑùÌïòÏó¨ Í∞êÏÑ± Î∂ÑÏÑùÍ≥º Ï†ïÏ±Ö Î∂ÑÎ•òÎ•º ÏàòÌñâÌï¥Ï£ºÏÑ∏Ïöî.

Î∞úÏñ∏Ïûê: {statement.speaker.naas_nm}
Î∞úÏñ∏ ÎÇ¥Ïö©: {statement.text}

Îã§Ïùå JSON ÌòïÏãùÏúºÎ°ú Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Ï†úÍ≥µÌï¥Ï£ºÏÑ∏Ïöî:
{{
    "sentiment_score": -1Î∂ÄÌÑ∞ 1ÍπåÏßÄÏùò Í∞êÏÑ± Ï†êÏàò (Ïà´Ïûê),
    "sentiment_reason": "Í∞êÏÑ± Î∂ÑÏÑù Í∑ºÍ±∞",
    "policy_categories": [
        {{
            "main_category": "Ï£ºÏöî Ï†ïÏ±Ö Î∂ÑÏïº (Í≤ΩÏ†ú, ÏÇ¨ÌöåÎ≥µÏßÄ, ÍµêÏú°, Ïô∏ÍµêÏïàÎ≥¥, ÌôòÍ≤Ω, Î≤ïÎ¨¥, Í≥ºÌïôÍ∏∞Ïà†, Î¨∏ÌôîÏ≤¥Ïú°, ÎÜçÎ¶ºÏ∂ïÏÇ∞, Íµ≠Ï†ïÍ∞êÏÇ¨ Ï§ë ÌïòÎÇò)",
            "sub_category": "ÏÑ∏Î∂Ä Î∂ÑÏïº",
            "confidence": 0Î∂ÄÌÑ∞ 1ÍπåÏßÄÏùò ÌôïÏã†ÎèÑ (Ïà´Ïûê)
        }}
    ],
    "policy_keywords": ["Ï†ïÏ±Ö Í¥ÄÎ†® Ï£ºÏöî ÌÇ§ÏõåÎìúÎì§"]
}}

Î∂ÑÏÑù Í∏∞Ï§Ä:
1. Í∞êÏÑ± Î∂ÑÏÑù: -1(Îß§Ïö∞ Î∂ÄÏ†ïÏ†Å) ~ 1(Îß§Ïö∞ Í∏çÏ†ïÏ†Å)
2. Ï†ïÏ±Ö Î∂ÑÎ•ò: Î∞úÏñ∏ ÎÇ¥Ïö©ÏùÑ Í∏∞Î∞òÏúºÎ°ú Í¥ÄÎ†® Ï†ïÏ±Ö Î∂ÑÏïº Î∂ÑÎ•ò
3. Ï£ºÏöî ÌÇ§ÏõåÎìú: Ï†ïÏ±ÖÍ≥º Í¥ÄÎ†®Îêú ÌïµÏã¨ Ïö©Ïñ¥Îì§ Ï∂îÏ∂ú

ÏùëÎãµÏùÄ Î∞òÎìúÏãú Ïú†Ìö®Ìïú JSON ÌòïÏãùÏù¥Ïñ¥Ïïº Ìï©ÎãàÎã§.
"""

        response = model.generate_content(prompt)

        if not response.text:
            logger.warning(
                f"‚ùå No response from LLM for statement {statement_id}")
            return

        # Clean the response text
        response_text = response.text.strip()
        if response_text.startswith('```json'):
            response_text = response_text[7:]
        if response_text.startswith('```'):
            response_text = response_text[3:]
        if response_text.endswith('```'):
            response_text = response_text[:-3]
        response_text = response_text.strip()

        # Parse JSON response
        import json as json_module
        analysis_data = json_module.loads(response_text)

        # Update statement with analysis results
        statement.sentiment_score = analysis_data.get('sentiment_score', 0.0)
        statement.sentiment_reason = analysis_data.get('sentiment_reason',
                                                       'LLM Î∂ÑÏÑù ÏôÑÎ£å')
        statement.policy_keywords = ', '.join(
            analysis_data.get('policy_keywords', []))
        statement.category_analysis = json.dumps(analysis_data.get(
            'policy_categories', []),
                                                 ensure_ascii=False)
        statement.save()

        # Create category associations
        policy_categories = analysis_data.get('policy_categories', [])
        if policy_categories:
            create_statement_categories(statement, policy_categories)

        logger.info(
            f"‚úÖ Analyzed statement {statement_id}: sentiment={statement.sentiment_score}, categories={len(policy_categories)}"
        )

    except json.JSONDecodeError as e:
        logger.error(
            f"‚ùå Failed to parse LLM JSON response for statement {statement_id}: {e}"
        )
        logger.error(f"‚ùå Response parsing failed - check LLM output format")
    except Exception as e:
        logger.error(f"‚ùå Error analyzing statement {statement_id}: {e}")
        if self:
            try:
                self.retry(exc=e)
            except MaxRetriesExceededError:
                logger.error(
                    f"Max retries exceeded for statement analysis {statement_id}"
                )
                raise

    except Exception as e:
        logger.error(f"‚ùå Error analyzing statement {statement_id}: {e}")
        if self:
            try:
                self.retry(exc=e)
            except MaxRetriesExceededError:
                logger.error(
                    f"Max retries exceeded for statement analysis {statement_id}"
                )
                raise


def process_pdf_statements(full_text,
                           session_id,
                           session,
                           bills_context,
                           debug=False):
    """Helper function to process PDF statements."""
    try:
        # Skip processing if no LLM available
        if not model:
            logger.warning(
                "‚ùå LLM not available, skipping statement extraction")
            return

        # Parse and analyze statements from text using LLM
        statements_data = extract_statements_with_llm_validation(
            full_text, session_id, bills_context, debug)

        # Process extracted and analyzed statements
        created_count = 0
        for statement_data in statements_data:
            try:
                speaker_name = statement_data.get('speaker_name', '').strip()
                statement_text = statement_data.get('text', '').strip()
                sentiment_score = statement_data.get('sentiment_score', 0.0)
                sentiment_reason = statement_data.get('sentiment_reason',
                                                      'LLM Î∂ÑÏÑù ÏôÑÎ£å')
                policy_categories = statement_data.get('policy_categories', [])
                policy_keywords = statement_data.get('policy_keywords', [])

                if not speaker_name or not statement_text:
                    logger.warning(
                        f"‚ö†Ô∏è Skipping statement with missing speaker or text")
                    continue

                # Refresh database connection before processing
                from django.db import connection
                connection.ensure_connection()

                # Get or create speaker
                speaker = get_or_create_speaker(speaker_name, debug)
                if not speaker:
                    logger.warning(
                        f"‚ö†Ô∏è Could not create speaker: {speaker_name}")
                    continue

                # Check if statement already exists to avoid duplicates
                existing_statement = Statement.objects.filter(
                    session=session, speaker=speaker,
                    text=statement_text).first()

                if existing_statement:
                    logger.info(
                        f"‚ÑπÔ∏è Statement already exists for {speaker_name}")
                    continue

                # Create statement with retry logic
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        statement = Statement.objects.create(
                            session=session,
                            speaker=speaker,
                            text=statement_text,
                            sentiment_score=sentiment_score,
                            sentiment_reason=sentiment_reason,
                            policy_keywords=', '.join(policy_keywords)
                            if policy_keywords else '',
                            category_analysis=json.dumps(policy_categories,
                                                         ensure_ascii=False)
                            if policy_categories else '')

                        created_count += 1
                        logger.info(
                            f"‚ú® Created statement for {speaker_name} with sentiment {sentiment_score}: {statement_text[:50]}..."
                        )

                        # Create category associations if available
                        if policy_categories and not debug:
                            create_statement_categories(
                                statement, policy_categories)

                        break

                    except Exception as db_error:
                        if attempt < max_retries - 1:
                            logger.warning(
                                f"‚ö†Ô∏è Database error on attempt {attempt + 1}, retrying: {db_error}"
                            )
                            # Close and reconnect database connection
                            connection.close()
                            time.sleep(1)  # Brief delay before retry
                            continue
                        else:
                            raise db_error

            except Exception as e:
                logger.error(f"‚ùå Error creating statement: {e}")
                logger.error(f"‚ùå Statement data: {statement_data}")
                continue

        logger.info(
            f"üéâ Processed PDF for session {session_id}: {created_count} statements created"
        )

    except Exception as e:
        logger.error(
            f"‚ùå Error processing PDF statements for session {session_id}: {e}")
        raise


def extract_statements_with_llm_validation(text,
                                           session_id,
                                           bills_context,
                                           debug=False):
    """Extract statements using two-stage LLM approach: speaker detection + content analysis."""

    if not model:
        logger.warning(
            "‚ùå LLM model not available, falling back to regex extraction")
        return extract_statements_with_regex_fallback(text, session_id, debug)

    logger.info(
        f"ü§ñ Starting two-stage LLM extraction for session: {session_id}")

    try:
        # Configure lighter model for speaker detection
        speaker_detection_model = genai.GenerativeModel(
            'gemini-2.0-flash-lite')

        # Stage 1: Speaker Detection and Boundary Identification
        logger.info(
            f"üîç Stage 1: Detecting speakers and speech boundaries (session: {session_id})"
        )

        speaker_detection_prompt = f"""
ÎãπÏã†ÏùÄ Í∏∞Î°ùÍ∞ÄÏûÖÎãàÎã§. ÎãπÏã†Ïùò Í∏∞Î°ùÏùÄ ÎØ∏ÎûòÏóê ÏÇ¨ÎûåÎì§ÏùÑ ÏÇ¥Î¶¥ Í≤ÉÏù¥ÎØÄÎ°ú, Î∞úÏñ∏ Íµ¨Í∞ÑÏùÑ ÌïòÎÇòÎèÑ Îπ†Îú®Î¶¨Î©¥ Ïïà Îê©ÎãàÎã§. Îã§ÏùåÏùÄ Íµ≠Ìöå ÌöåÏùòÎ°ù ÌÖçÏä§Ìä∏ÏûÖÎãàÎã§. Ïù¥ ÌÖçÏä§Ìä∏ÏóêÏÑú Ïã§Ï†ú Íµ≠ÌöåÏùòÏõêÎì§Ïùò Î∞úÏñ∏ Íµ¨Í∞ÑÏùÑ Ï†ïÌôïÌûà ÏãùÎ≥ÑÌï¥Ï£ºÏÑ∏Ïöî.

ÌöåÏùò Í¥ÄÎ†® ÏùòÏïà:
{bills_context}

ÌöåÏùòÎ°ù ÌÖçÏä§Ìä∏:
{text}

Îã§Ïùå Í∏∞Ï§ÄÏúºÎ°ú Î∞úÏñ∏ÏùÑ ÏãùÎ≥ÑÌï¥Ï£ºÏÑ∏Ïöî:
1. ‚óØ Í∏∞Ìò∏Î°ú ÏãúÏûëÌïòÎäî Î∞úÏñ∏Îßå Ï∂îÏ∂ú
2. Ïã§Ï†ú ÏÇ¨Îûå Ïù¥Î¶Ñ(Íµ≠ÌöåÏùòÏõê)Îßå Ìè¨Ìï®, Î≤ïÎ•†Î™ÖÏù¥ÎÇò Í∏∞Í¥ÄÎ™Ö Ï†úÏô∏
3. Ï†àÏ∞®Ï†Å Î∞úÏñ∏(Ìà¨Ìëú, Í∞úÌöå, ÌèêÌöå Îì±)ÏùÄ Ï†úÏô∏
4. ÏµúÏÜå 50Ïûê Ïù¥ÏÉÅÏùò ÏùòÎØ∏ÏûàÎäî Ï†ïÏ±Ö ÌÜ†Î°† ÎÇ¥Ïö©Îßå Ìè¨Ìï®

JSON ÌòïÏãùÏúºÎ°ú ÏùëÎãµÌï¥Ï£ºÏÑ∏Ïöî:
{{
    "speakers_detected": [
        {{
            "speaker_name": "Î∞úÏñ∏Ïûê Ïã§Î™Ö",
            "start_marker": "Î∞úÏñ∏ ÏãúÏûë Î∂ÄÎ∂Ñ ÌÖçÏä§Ìä∏ (20Ïûê)",
            "end_marker": "Î∞úÏñ∏ Ï¢ÖÎ£å Î∂ÄÎ∂Ñ ÌÖçÏä§Ìä∏ (20Ïûê)",
            "is_substantial": true/false,
            "speech_type": "policy_discussion/procedural/other"
        }}
    ]
}}
"""

        stage1_response = speaker_detection_model.generate_content(
            speaker_detection_prompt)

        if not stage1_response.text:
            logger.warning(
                "‚ùå No response from Stage 1 LLM, falling back to regex")
            return extract_statements_with_regex_fallback(
                text, session_id, debug)

        # Parse Stage 1 response
        stage1_text = stage1_response.text.strip()
        logger.info(f"üîç Raw Stage 1 response: {stage1_text[:500]}...")

        if stage1_text.startswith('```json'):
            stage1_text = stage1_text[7:-3].strip()
        elif stage1_text.startswith('```'):
            stage1_text = stage1_text[3:-3].strip()

        logger.info(f"üîç Cleaned Stage 1 response: {stage1_text[:500]}...")

        import json as json_module
        stage1_data = json_module.loads(stage1_text)
        speakers_detected = stage1_data.get('speakers_detected', [])

        logger.info(f"üîç Parsed speakers_detected: {speakers_detected}")

        for i, speaker in enumerate(speakers_detected):
            logger.info(f"üîç Speaker {i+1} details: {speaker}")

        logger.info(
            f"‚úÖ Stage 1 completed: Found {len(speakers_detected)} potential speakers"
        )

        # Stage 2: Extract and analyze substantial policy discussions
        logger.info(
            f"üîç Stage 2: Extracting and analyzing policy content (session: {session_id})"
        )

        analyzed_statements = []

        for i, speaker_info in enumerate(speakers_detected, 1):
            speaker_name = speaker_info.get('speaker_name', 'Unknown')
            is_substantial = speaker_info.get('is_substantial', False)
            speech_type = speaker_info.get('speech_type', 'unknown')

            logger.info(f"üîç Processing speaker {i}: {speaker_name}")
            logger.info(f"   - is_substantial: {is_substantial}")
            logger.info(f"   - speech_type: {speech_type}")

            if not is_substantial or speech_type != 'policy_discussion':
                logger.info(
                    f"‚ö†Ô∏è Skipping speaker {speaker_name} - substantial: {is_substantial}, type: {speech_type}"
                )
                continue

            # Extract the actual speech content using markers
            start_marker = speaker_info.get('start_marker', '')
            end_marker = speaker_info.get('end_marker', '')

            logger.info(f"üîç Extracting speech for {speaker_name}")
            logger.info(f"   - start_marker: '{start_marker[:50]}...'")
            logger.info(f"   - end_marker: '{end_marker[:50]}...'")

            # Find speech content between markers
            speech_content = extract_speech_between_markers(
                text, start_marker, end_marker, speaker_name)

            logger.info(
                f"   - extracted length: {len(speech_content) if speech_content else 0}"
            )
            if speech_content:
                logger.info(
                    f"   - content preview: '{speech_content[:100]}...'")

            if not speech_content or len(speech_content) < 10:
                logger.info(
                    f"‚ö†Ô∏è Skipping {speaker_name} - insufficient content (length: {len(speech_content) if speech_content else 0})"
                )
                continue

            logger.info(
                f"ü§ñ Analyzing statement {i}/{len(speakers_detected)} from {speaker_name} (session: {session_id})"
            )

            # Stage 2: Analyze the extracted speech
            analysis_result = analyze_single_statement(
                {
                    'speaker_name': speaker_name,
                    'text': speech_content
                }, session_id, debug)

            analyzed_statements.append(analysis_result)

            # Brief pause between API calls
            if not debug:
                time.sleep(0.5)

        logger.info(
            f"‚úÖ Two-stage LLM extraction completed: {len(analyzed_statements)} statements (session: {session_id})"
        )
        return analyzed_statements

    except Exception as e:
        logger.error(f"‚ùå Error in two-stage LLM extraction: {e}")
        logger.info("‚ö†Ô∏è  Falling back to regex extraction")
        return extract_statements_with_regex_fallback(text, session_id, debug)


def extract_speech_between_markers(text, start_marker, end_marker,
                                   speaker_name):
    """Extract speech content between start and end markers."""
    try:
        # Find the start position
        start_pos = text.find(start_marker)
        if start_pos == -1:
            # Try to find by speaker pattern as fallback
            speaker_pattern = f"‚óØ{speaker_name}"
            start_pos = text.find(speaker_pattern)
            if start_pos == -1:
                return ""

        # Find the end position
        end_pos = text.find(end_marker, start_pos + len(start_marker))
        if end_pos == -1:
            # Find next speaker as fallback
            next_speaker_pos = text.find("‚óØ", start_pos + len(start_marker))
            if next_speaker_pos != -1:
                end_pos = next_speaker_pos
            else:
                end_pos = len(text)

        # Extract content
        content = text[start_pos:end_pos].strip()

        # Clean up the content
        # Remove speaker name from beginning
        if content.startswith(f"‚óØ{speaker_name}"):
            content = content[len(f"‚óØ{speaker_name}"):].strip()

        # Remove parenthetical notes and clean whitespace
        import re
        content = re.sub(r'\([^)]*\)', '', content)
        content = re.sub(r'\s+', ' ', content).strip()

        return content

    except Exception as e:
        logger.error(f"‚ùå Error extracting speech content: {e}")
        return ""


def extract_statements_with_regex_fallback(text, session_id, debug=False):
    """Fallback regex extraction method (existing implementation)."""
    import re

    logger.info(
        f"üìÑ Extracting statements using regex fallback (session: {session_id})"
    )

    # Clean up the text first
    text = re.sub(r'\n+', '\n', text)
    text = re.sub(r'\s+', ' ', text)

    statements = []
    speaker_pattern = r'‚óØ([^‚óØ\n]+?)\s+([^‚óØ]+?)(?=‚óØ|$)'
    matches = re.findall(speaker_pattern, text, re.DOTALL | re.MULTILINE)

    for speaker_raw, content_raw in matches:
        speaker_name = speaker_raw.strip()
        speaker_name = re.sub(
            r'\s*(ÏùòÏõê|ÏúÑÏõêÏû•|Ïû•Í¥Ä|Íµ≠Ïû•|ÏùòÏû•|Î∂ÄÏùòÏû•|Ï∞®Í¥Ä|Ïã§Ïû•|Íµ≠Î¨¥Ï¥ùÎ¶¨|ÎåÄÌÜµÎ†π|Î∂ÄÏ¥ùÎ¶¨)\s*', '',
            speaker_name).strip()

        if not speaker_name or not content_raw.strip():
            continue

        # Enhanced filtering for non-person entities
        role_patterns = [
            r'.*ÎåÄÎ¶¨$', r'^ÏùòÏÇ¨$', r'^ÏúÑÏõêÏû•$', r'.*ÏúÑÏõêÌöå.*', r'.*Î∂Ä.*Ïû•Í¥Ä.*', r'.*Ï≤≠Ïû•.*',
            r'.*Ïã§Ïû•.*', r'^ÏÇ¨ÌöåÏûê$', r'^ÏßÑÌñâÏûê$', r'.*Í∞úÏ†ïÎ≤ïÎ•†Ïïà.*', r'.*ÌäπÎ≥ÑÎ≤ï.*',
            r'.*ÏßÑÌù•Î≤ï.*', r'.*Í¥ÄÎ¶¨Î≤ï.*', r'.*Ï¥âÏßÑÎ≤ï.*', r'.*Î≥¥Ìò∏Î≤ï.*', r'.*Î≤ïÎ•†.*', r'.*Î≤ï$',
            r'.*Ïú°ÏÑ±.*', r'.*ÏßÄÏõê.*', r'^Ïû¨ÎÇú$', r'^Ïù∏Íµ¨Í∞êÏÜåÏßÄÏó≠$', r'^Ïö∞Ï£ºÍ∞úÎ∞ú$',
            r'^Ïó¨ÏÑ±Í≥ºÌïôÍ∏∞Ïà†Ïù∏$', r'.*Í∏∞ÌöçÏû¨Ï†ïÎ∂Ä.*', r'.*Ï¥ùÏû•\([^)]+\).*', r'^ÌÉÑÏÜåÏÜåÏû¨$',
            r'^Ï†ÑÍ∏∞Í≥µÏÇ¨ÏóÖÎ≤ï$', r'^ÌäπÌóàÎ≤ï$', r'.*ÏÜåÏû¨$', r'.*ÏóÖÎ≤ï$', r'Í≤∏.*Î∂Ä$'
        ]

        korean_surname_pattern = r'^[ÍπÄÏù¥Î∞ïÏµúÏ†ïÍ∞ïÏ°∞Ïú§Ïû•ÏûÑÌïúÏò§ÏÑúÏã†Í∂åÌô©ÏïàÏÜ°Î•òÏ†ÑÍ≥†Î¨∏ÏñëÏÜêÎ∞∞Î∞±ÌóàÎÇ®Ïã¨ÎÖ∏Ï†ïÌïòÍ≥ΩÏÑ±Ï∞®Ï£ºÏö∞Íµ¨Ïã†ÏûÑÎÇòÏ†ÑÎØºÏú†ÏßÑÏßÄÏóÑÏ±ÑÏõêÏ≤úÎ∞©Í≥µÍ∞ïÌòÑÌï®Î≥ÄÏóºÏñëÎ≥ÄÌôç]'

        is_role = any(
            re.match(pattern, speaker_name) for pattern in role_patterns)
        is_likely_person = (
            re.match(korean_surname_pattern, speaker_name)
            and 2 <= len(speaker_name) <= 4
            and not any(char in speaker_name
                        for char in ['(', ')', 'Î≤ï', 'Î∂Ä', 'Ï≤≠', 'ÏõêÌöå', 'Í¥Ä', 'Ïû•']))

        if is_role or not is_likely_person:
            if debug:
                logger.info(
                    f"üêõ DEBUG: Skipping non-person speaker: {speaker_name}")
            continue

        content = content_raw.strip()
        content = re.sub(r'\([^)]*\)', '', content)
        content = re.sub(r'\s+', ' ', content).strip()

        if len(content) < 100:
            continue

        # Check for procedural content
        procedural_phrases = [
            'Ìà¨ÌëúÌï¥ Ï£ºÏãúÍ∏∞ Î∞îÎûçÎãàÎã§', 'Ìà¨ÌëúÎ•º ÎßàÏπòÍ≤†ÏäµÎãàÎã§', 'Í∞ÄÍ≤∞ÎêòÏóàÏùåÏùÑ ÏÑ†Ìè¨Ìï©ÎãàÎã§', 'ÏàòÍ≥†ÌïòÏÖ®ÏäµÎãàÎã§', 'ÏÉÅÏ†ïÌï©ÎãàÎã§',
            'ÏùòÍ≤∞ÌïòÎèÑÎ°ù ÌïòÍ≤†ÏäµÎãàÎã§', 'ÏõêÏïàÍ∞ÄÍ≤∞ÎêòÏóàÏùåÏùÑ ÏÑ†Ìè¨Ìï©ÎãàÎã§', 'ÌèêÌöåÎ•º ÏÑ†Ìè¨Ìï©ÎãàÎã§', 'Í∞úÌöåÎ•º ÏÑ†Ìè¨Ìï©ÎãàÎã§',
            'ÌöåÏùòÎ•º ÏãúÏûëÌïòÍ≤†ÏäµÎãàÎã§'
        ]

        is_procedural = any(phrase in content for phrase in procedural_phrases)
        if is_procedural:
            continue

        policy_indicators = [
            'Î≤ïÎ•†Ïïà', 'Í∞úÏ†ï', 'Ï†úÏïà', 'ÌïÑÏöî', 'Î¨∏Ï†ú', 'Í∞úÏÑ†', 'Ï†ïÏ±Ö', 'Î∞©Ïïà', 'ÎåÄÏ±Ö', 'ÏòàÏÇ∞', 'Ï∂îÏßÑ',
            'Í≥ÑÌöç', 'Í≤ÄÌÜ†', 'ÏùòÍ≤¨', 'ÏÉùÍ∞Å', 'ÌåêÎã®', 'Íµ≠ÎØº', 'ÏãúÎØº', 'ÏÇ¨Ìöå', 'Í≤ΩÏ†ú', 'Ï†ïÏπò', 'ÍµêÏú°',
            'Î≥µÏßÄ', 'ÌôòÍ≤Ω'
        ]

        has_policy_content = any(indicator in content
                                 for indicator in policy_indicators)
        if len(content) > 200 or has_policy_content:
            statements.append({'speaker_name': speaker_name, 'text': content})

    logger.info(
        f"‚úÖ Regex fallback completed: {len(statements)} statements (session: {session_id})"
    )
    return statements


def analyze_single_statement(statement_data, session_id, debug=False):
    """Analyze a single statement using LLM."""
    if not model:
        logger.warning("‚ùå LLM model not available for statement analysis")
        return statement_data

    speaker_name = statement_data.get('speaker_name', '')
    text = statement_data.get('text', '')

    prompt = f"""
Îã§Ïùå Íµ≠Ìöå Î∞úÏñ∏ÏùÑ Î∂ÑÏÑùÌïòÏó¨ Í∞êÏÑ± Î∂ÑÏÑùÍ≥º Ï†ïÏ±Ö Î∂ÑÎ•òÎ•º ÏàòÌñâÌï¥Ï£ºÏÑ∏Ïöî.

Î∞úÏñ∏Ïûê: {speaker_name}
Î∞úÏñ∏ ÎÇ¥Ïö©: {text}

Îã§Ïùå JSON ÌòïÏãùÏúºÎ°ú Î∂ÑÏÑù Í≤∞Í≥ºÎ•º Ï†úÍ≥µÌï¥Ï£ºÏÑ∏Ïöî:
{{
    "sentiment_score": -1Î∂ÄÌÑ∞ 1ÍπåÏßÄÏùò Í∞êÏÑ± Ï†êÏàò (Ïà´Ïûê),
    "sentiment_reason": "Í∞êÏÑ± Î∂ÑÏÑù Í∑ºÍ±∞",
    "policy_categories": [
        {{
            "main_category": "Ï£ºÏöî Ï†ïÏ±Ö Î∂ÑÏïº (Í≤ΩÏ†ú, ÏÇ¨ÌöåÎ≥µÏßÄ, ÍµêÏú°, Ïô∏ÍµêÏïàÎ≥¥, ÌôòÍ≤Ω, Î≤ïÎ¨¥, Í≥ºÌïôÍ∏∞Ïà†, Î¨∏ÌôîÏ≤¥Ïú°, ÎÜçÎ¶ºÏ∂ïÏÇ∞, Íµ≠Ï†ïÍ∞êÏÇ¨ Ï§ë ÌïòÎÇò)",
            "sub_category": "ÏÑ∏Î∂Ä Î∂ÑÏïº",
            "confidence": 0Î∂ÄÌÑ∞ 1ÍπåÏßÄÏùò ÌôïÏã†ÎèÑ (Ïà´Ïûê)
        }}
    ],
    "policy_keywords": ["Ï†ïÏ±Ö Í¥ÄÎ†® Ï£ºÏöî ÌÇ§ÏõåÎìúÎì§"]
}}

Î∂ÑÏÑù Í∏∞Ï§Ä:
1. Í∞êÏÑ± Î∂ÑÏÑù: -1(Îß§Ïö∞ Î∂ÄÏ†ïÏ†Å) ~ 1(Îß§Ïö∞ Í∏çÏ†ïÏ†Å)
2. Ï†ïÏ±Ö Î∂ÑÎ•ò: Î∞úÏñ∏ ÎÇ¥Ïö©ÏùÑ Í∏∞Î∞òÏúºÎ°ú Í¥ÄÎ†® Ï†ïÏ±Ö Î∂ÑÏïº Î∂ÑÎ•ò
3. Ï£ºÏöî ÌÇ§ÏõåÎìú: Ï†ïÏ±ÖÍ≥º Í¥ÄÎ†®Îêú ÌïµÏã¨ Ïö©Ïñ¥Îì§ Ï∂îÏ∂ú

ÏùëÎãµÏùÄ Î∞òÎìúÏãú Ïú†Ìö®Ìïú JSON ÌòïÏãùÏù¥Ïñ¥Ïïº Ìï©ÎãàÎã§.
"""

    try:
        response = model.generate_content(prompt)

        if not response.text:
            logger.warning(
                f"‚ùå No LLM response for statement from {speaker_name}")
            return statement_data

        # Clean response
        response_text = response.text.strip()
        if response_text.startswith('```json'):
            response_text = response_text[7:].strip()
        elif response_text.startswith('```'):
            response_text = response_text[3:].strip()
        if response_text.endswith('```'):
            response_text = response_text[:-3].strip()

        # Parse JSON
        import json as json_module
        analysis_data = json_module.loads(response_text)

        # Merge analysis data with original statement
        statement_data.update({
            'sentiment_score':
            analysis_data.get('sentiment_score', 0.0),
            'sentiment_reason':
            analysis_data.get('sentiment_reason', 'LLM Î∂ÑÏÑù ÏôÑÎ£å'),
            'policy_categories':
            analysis_data.get('policy_categories', []),
            'policy_keywords':
            analysis_data.get('policy_keywords', [])
        })

        if debug:
            logger.info(
                f"üêõ DEBUG: Analyzed statement from {speaker_name} - Sentiment: {statement_data.get('sentiment_score', 0)}"
            )

        return statement_data

    except Exception as e:
        logger.error(f"‚ùå Error analyzing statement from {speaker_name}: {e}")
        return statement_data


def get_bills_context(session_id):
    """Fetch bill context for a session to provide LLM."""
    try:
        session = Session.objects.get(conf_id=session_id)
        bills = Bill.objects.filter(session=session)

        bill_names = [bill.bill_nm for bill in bills]
        return ", ".join(bill_names)
    except Exception as e:
        logger.error(f"‚ùå Error fetching bills context: {e}")
        return ""


def parse_and_analyze_statements_from_text(text,
                                           session_id,
                                           bills_context,
                                           debug=False):
    """Parse statements from PDF text using regex, then analyze each individually."""
    # Step 1: Extract statements using regex
    statements = extract_statements_with_llm_validation(
        text, session_id, bills_context, debug)

    if not statements:
        logger.warning(
            f"‚ùå No statements extracted from PDF (session: {session_id})")
        return []

    # Step 2: Analyze each statement individually
    analyzed_statements = []
    for i, statement in enumerate(statements, 1):
        logger.info(
            f"ü§ñ Analyzing statement {i}/{len(statements)} from {statement.get('speaker_name', 'Unknown')} (session: {session_id})"
        )

        analyzed_statement = analyze_single_statement(statement, session_id,
                                                      debug)
        analyzed_statements.append(analyzed_statement)

        # Brief pause between API calls to avoid rate limiting
        if not debug:
            time.sleep(0.5)

    logger.info(
        f"‚úÖ Completed analysis of {len(analyzed_statements)} statements (session: {session_id})"
    )

    return analyzed_statements


def create_statement_categories(statement, policy_categories):
    """Create category associations for a statement based on LLM analysis."""
    try:
        from .models import Category, Subcategory, StatementCategory

        for category_data in policy_categories:
            main_category = category_data.get('main_category', '').strip()
            sub_category = category_data.get('sub_category', '').strip()
            confidence = category_data.get('confidence', 0.0)

            if not main_category:
                continue

            # Get or create main category
            category, created = Category.objects.get_or_create(
                name=main_category,
                defaults={'description': f'{main_category} Í¥ÄÎ†® Ï†ïÏ±Ö'})

            # Get or create subcategory if provided
            subcategory = None
            if sub_category:
                subcategory, created = Subcategory.objects.get_or_create(
                    name=sub_category,
                    category=category,
                    defaults={'description': f'{sub_category} Í¥ÄÎ†® ÏÑ∏Î∂Ä Ï†ïÏ±Ö'})

            # Create statement category association
            StatementCategory.objects.get_or_create(
                statement=statement,
                category=category,
                subcategory=subcategory,
                defaults={'confidence_score': confidence})

        logger.info(
            f"‚úÖ Created {len(policy_categories)} category associations for statement {statement.id}"
        )

    except Exception as e:
        logger.error(f"‚ùå Error creating statement categories: {e}")


def get_or_create_speaker(speaker_name, debug=False):
    """Get or create speaker by name."""
    if not speaker_name:
        return None

    # Clean speaker name
    speaker_name = speaker_name.replace('ÏùòÏõê',
                                        '').replace('ÏúÑÏõêÏû•',
                                                    '').replace('Ïû•Í¥Ä',
                                                                '').strip()

    try:
        # Ensure database connection
        from django.db import connection
        connection.ensure_connection()

        # Try to find existing speaker
        speaker = Speaker.objects.filter(
            naas_nm__icontains=speaker_name).first()

        if not speaker:
            # Create temporary speaker record with fallback values for all fields
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    speaker = Speaker.objects.create(
                        naas_cd=f"TEMP_{speaker_name}_{int(time.time())}",
                        naas_nm=speaker_name,
                        naas_ch_nm="Ï†ïÎ≥¥ ÏóÜÏùå",
                        plpt_nm="Ï†ïÎãπÏ†ïÎ≥¥ÏóÜÏùå",
                        elecd_nm="Ï†ïÎ≥¥ ÏóÜÏùå",
                        elecd_div_nm="Ï†ïÎ≥¥ ÏóÜÏùå",
                        cmit_nm="Ï†ïÎ≥¥ ÏóÜÏùå",
                        blng_cmit_nm="Ï†ïÎ≥¥ ÏóÜÏùå",
                        rlct_div_nm="Ï†ïÎ≥¥ ÏóÜÏùå",
                        gtelt_eraco="Ï†ïÎ≥¥ ÏóÜÏùå",
                        ntr_div="Ï†ïÎ≥¥ ÏóÜÏùå",
                        naas_pic="")

                    if debug:
                        logger.info(
                            f"üêõ DEBUG: Created temporary speaker: {speaker_name}"
                        )

                    # Queue detailed speaker fetch
                    if not debug:
                        fetch_speaker_details(speaker_name)
                    break

                except Exception as db_error:
                    if attempt < max_retries - 1:
                        logger.warning(
                            f"‚ö†Ô∏è Database error creating speaker on attempt {attempt + 1}: {db_error}"
                        )
                        connection.close()
                        time.sleep(1)
                        continue
                    else:
                        logger.error(
                            f"‚ùå Failed to create speaker after {max_retries} attempts: {db_error}"
                        )
                        return None

        return speaker

    except Exception as e:
        logger.error(f"‚ùå Error in get_or_create_speaker: {e}")
        return None


# Note: Sentiment analysis is now integrated into the comprehensive statement analysis above


@shared_task(bind=True, max_retries=3, default_retry_delay=60)
def fetch_additional_data_nepjpxkkabqiqpbvk(self=None,
                                            force=False,
                                            debug=False):
    """Fetch additional data using nepjpxkkabqiqpbvk API endpoint."""
    try:
        if debug:
            logger.info(
                f"üêõ DEBUG: Fetching additional data using nepjpxkkabqiqpbvk API"
            )

        url = "https://open.assembly.go.kr/portal/openapi/nepjpxkkabqiqpbvk"
        params = {
            "KEY": settings.ASSEMBLY_API_KEY,
            "Type": "json",
            "pIndex": 1,
            "pSize": 100
        }

        logger.info(f"üîç Fetching additional data from nepjpxkkabqiqpbvk API")
        response = requests.get(url, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()

        logger.info(
            f"üìä nepjpxkkabqiqpbvk API response structure: {list(data.keys()) if data else 'Empty response'}"
        )

        if debug:
            logger.info(
                f"üêõ DEBUG: Full nepjpxkkabqiqpbvk response: {json.dumps(data, indent=2, ensure_ascii=False)}"
            )

        # Extract data based on API structure
        additional_data = None
        if 'nepjpxkkabqiqpbvk' in data and len(data['nepjpxkkabqiqpbvk']) > 1:
            additional_data = data['nepjpxkkabqiqpbvk'][1].get('row', [])
        elif 'nepjpxkkabqiqpbvk' in data and len(
                data['nepjpxkkabqiqpbvk']) > 0:
            additional_data = data['nepjpxkkabqiqpbvk'][0].get('row', [])
        elif 'row' in data:
            additional_data = data['row']

        if not additional_data:
            logger.info(
                f"‚ÑπÔ∏è  No additional data found from nepjpxkkabqiqpbvk API")
            return

        logger.info(
            f"‚úÖ Found {len(additional_data)} records from nepjpxkkabqiqpbvk API"
        )

        # Process the additional data (customize based on what the API returns)
        processed_count = 0
        for item in additional_data:
            try:
                if debug:
                    logger.info(f"üêõ DEBUG: Processing item: {item}")
                else:
                    # Process the item based on its structure
                    # This will depend on what nepjpxkkabqiqpbvk actually returns
                    processed_count += 1

            except Exception as e:
                logger.error(f"‚ùå Error processing nepjpxkkabqiqpbvk item: {e}")
                continue

        logger.info(
            f"üéâ Processed {processed_count} items from nepjpxkkabqiqpbvk API")

    except Exception as e:
        if isinstance(e, RequestException):
            if self:
                try:
                    self.retry(exc=e)
                except MaxRetriesExceededError:
                    logger.error(
                        f"Max retries exceeded for nepjpxkkabqiqpbvk fetch")
                    raise
            else:
                logger.error("Sync execution failed, no retry available")
                raise
        logger.error(f"‚ùå Error fetching from nepjpxkkabqiqpbvk API: {e}")
        raise
